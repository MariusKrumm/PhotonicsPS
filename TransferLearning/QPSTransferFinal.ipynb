{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63920e21",
   "metadata": {},
   "source": [
    "# Transfer learning with photonic quantum PS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb98d481-241c-4093-8526-1bbd9e228073",
   "metadata": {},
   "source": [
    "This Jupyter notebook investigates a photonic quantum PS agent in a specific transfer learning scenario, see: \n",
    "\n",
    "Fulvio Flamini, Marius Krumm, Lukas J. Fiderer, Thomas Mueller, Hans J. Briegel, <em>Reinforcement learning via single-photon quantum walks</em>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67533f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf3f56",
   "metadata": {},
   "source": [
    "We pick fixed seeds for the random number generators of Pytorch and Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bb4922b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixRNG = True\n",
    "\n",
    "if fixRNG:\n",
    "    torch.manual_seed(3133742)\n",
    "    np.random.seed(3133742)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983dd27",
   "metadata": {
    "tags": []
   },
   "source": [
    "Folders to export numerical results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92485d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for saving figures and the data used to draw them\n",
    "dirNameFigures = \"TransferFigures\"\n",
    "if not os.path.exists(dirNameFigures):\n",
    "    os.mkdir(dirNameFigures)\n",
    "  \n",
    "# This is for saving PyTorch models\n",
    "dirNameModels = \"TransferModels\"\n",
    "if not os.path.exists(dirNameModels):\n",
    "    os.mkdir(dirNameModels)\n",
    "    \n",
    "# This is for saving PyTorch parameters\n",
    "dirNameParameters = \"TransferParameters\"\n",
    "if not os.path.exists(dirNameParameters):\n",
    "    os.mkdir(dirNameParameters)\n",
    "    \n",
    "dirNameUnitaries = \"TransferUnitaries\"\n",
    "if not os.path.exists(dirNameUnitaries):\n",
    "    os.mkdir(dirNameUnitaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0119581d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Numerical simulation of interferometer setups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339473ef",
   "metadata": {},
   "source": [
    "For our photonics Quantum PS agent, we will numerically simulate interferometer setups using Mach-Zehnder interferometers (MZI) as their basic building blocks. One of these setups is the Clements layout which allows to implement arbitrary unitaries for single photons, see:\n",
    "William R. Clements et al., Optica Vol. 3, Issue 12, pp. 1460-1465 (2016).\n",
    "\n",
    "Another setup which we consider is a tree-shaped layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7663e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MZI(phases):\n",
    "    \"\"\"\n",
    "    Defines a universal 2x2 unitary which implements a tunable\n",
    "    Mach-Zender interferometer characterized by two phase-parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    M = torch.tensor([[1.0+1.0j , 1.0+1.0j],[1.0+1.0j , 1.0+1.0j]])\n",
    "    M[0,0] = torch.exp(1j*phases[0])*torch.cos(phases[1])\n",
    "    M[0,1] = -torch.sin(phases[1]) \n",
    "    M[1,0] = torch.exp(1j*phases[0])*torch.sin(phases[1])\n",
    "    M[1,1] = torch.cos(phases[1])\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e3b2645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ClementsLayer(dim, withSkip , phases):\n",
    "    \"\"\"\n",
    "    The Clements layout uses layers of parallel Mach-Zender interferometers, \n",
    "    sometimes with a parallel wire (depends on number of modes and whether layer-index \n",
    "    is odd or even).\n",
    "    \"\"\"\n",
    "    if withSkip == 0:\n",
    "        if dim % 2 == 0:\n",
    "            for k in range(dim//2):\n",
    "                if k == 0:\n",
    "                    M = MZI(phases[0:2])\n",
    "                else:\n",
    "                    M = torch.block_diag(M, MZI(phases[2*k : (2*k)+2]))\n",
    "        if dim % 2 == 1:\n",
    "            for k in range((dim-1)//2):\n",
    "                if k == 0:\n",
    "                    M = MZI(phases[0:2])\n",
    "                else:\n",
    "                    M = torch.block_diag(M, MZI(phases[2*k : (2*k)+2]))\n",
    "            M = torch.block_diag(M, torch.ones(size = [1,1]))\n",
    "    if withSkip == 1:\n",
    "        if dim % 2 == 0:\n",
    "            M = torch.ones(size = [1,1])\n",
    "            for k in range((dim-2)//2):\n",
    "                M = torch.block_diag(M, MZI(phases[2*k : (2*k)+2]))\n",
    "            M = torch.block_diag(M, torch.ones(size = [1,1]))\n",
    "        if dim % 2 == 1:\n",
    "            M=torch.ones(size=[1,1])\n",
    "            for k in range((dim-1)//2):\n",
    "                M = torch.block_diag(M , MZI(phases[2*k : (2*k)+2]))\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1a22e7",
   "metadata": {},
   "source": [
    "The full Clements layout is now simply given by multiplying the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "264537fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Clements(dim, ClementsPhases, outputPhases):\n",
    "    M = torch.eye(dim , dtype = torch.cfloat)\n",
    "    for k in range(dim):\n",
    "        M = torch.matmul(ClementsLayer(dim, withSkip = k % 2, phases = ClementsPhases[k,:]), M)\n",
    "    M = torch.matmul(torch.diag(torch.exp(1j*outputPhases)) ,M)\n",
    "    return M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283fd2be",
   "metadata": {},
   "source": [
    "An alternative layout that we will use arranges the MZI into a tree-shaped pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a1f4b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TreeLayer( InputState, Phases, num_MZI):\n",
    "    \"\"\"\n",
    "    A layer for the interometer tree. For each component of the input state, there are \n",
    "    two components in the layer output. They are obtained by feeding the corresponding\n",
    "    layer input mode into a MZI, leaving the other MZI input mode dark.\n",
    "    \"\"\"\n",
    "    \n",
    "    for m in range(num_MZI):\n",
    "        CurrentState = torch.zeros(size = [2,1] , dtype = torch.cfloat)\n",
    "        CurrentState[0,0] = InputState[m,0]\n",
    "        CurrentState = torch.matmul(MZI(Phases[2*m : 2*m+2]) , CurrentState)\n",
    "        if m == 0:\n",
    "            StateSoFar = CurrentState\n",
    "        else:\n",
    "            StateSoFar = torch.cat([StateSoFar, CurrentState])\n",
    "    return StateSoFar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38f13d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tree(dim, TreePhases, OutputPhases):\n",
    "    \"\"\"\n",
    "    The full tree, implemented by multiplying the layers. For simplicity, we assume\n",
    "    that the phases are provided in a rectangular arrays, with the first index labeling\n",
    "    the layer and the second the position in the layer. The drawback is that many of\n",
    "    the phases in this array do not actually get used.\n",
    "    \"\"\"\n",
    "    num_layers = int(np.ceil(np.log2(dim)))\n",
    "    LayerOut = torch.tensor([[1.0 +0.00j]])\n",
    "    for layer in range(num_layers):\n",
    "        if layer < num_layers - 1:\n",
    "            LayerOut = TreeLayer(LayerOut, Phases = TreePhases[layer, :] , num_MZI = LayerOut.size(0))\n",
    "        elif layer == num_layers - 1:\n",
    "            #If the number of output modes is not a power of 2, then the last layer \n",
    "            #will have fewer MZIs, and instead there will be some parallel waveguides.\n",
    "            LastNumMZI = (dim - (2**(num_layers - 1))) \n",
    "            WorkState = LayerOut[0:LastNumMZI, :]      #Input to last layer\n",
    "            Remaining = LayerOut[LastNumMZI : , :]     #Parallel waveguides\n",
    "            WorkState = TreeLayer(WorkState, TreePhases[layer , :], LastNumMZI)\n",
    "            LayerOut = torch.cat([WorkState, Remaining], )\n",
    "            LayerOut = torch.matmul(torch.diag(torch.exp(1.0j*OutputPhases)) , LayerOut)\n",
    "    return LayerOut\n",
    "            \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebd33dc",
   "metadata": {},
   "source": [
    "## Toy model for transfer learning: The scenario of Eva et al."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7eef28f",
   "metadata": {},
   "source": [
    "In this Jupyter notebook, we will work with the toy model of \n",
    "\n",
    "B. Eva, K. Ried, T. Müller, and H. J. Briegel, “How a minimal learning agent can infer the existence of unobserved variables in a complex environment,” Minds & Machines, pp. 1–35, 2022, Online first, DOI = https://doi.org/10.1007/s11023-022-09619-5 , arXiv:1910.06985.\n",
    "\n",
    "This toy model considers an idealized experimental setting: An agent is given a physical object, characterized by three observables, each of which can take 3 values. The agent can perform one of several experiments, each of them being determined by exactly one observable. In Ried et al., the goal was that the agent learns an explicit representation of the underlying observables and their values.\n",
    "\n",
    "In our case, we consider the same toy model, but our focus is on particular aspects of transfer learning, rather than the discovery of observables. We consider a 3-layer quantum PS. For now, we will perform a layer-wise training. The second layer is directly trained to learn the values of the observables. This is a crucial difference to Ried et al, who wanted the agent to passively learn a representation in which each middle layer vertex corresponds to one value of one observable.\n",
    "\n",
    "Our interest is in the third layer. Different from the classical PS agent, we want to demonstrate that the Quantum PS agent can perform well on experiments requiring the knowledge of two observables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877f4394",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_observables = 3\n",
    "num_values = 3 #number of values each observable can take"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4487a62b",
   "metadata": {},
   "source": [
    "Now, we create a list of the single, two and three observable experiments. Each experiment asks whether certain observables have certain values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be5e20d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "One_Obs_Exps = []\n",
    "experiment_index = 0\n",
    "\n",
    "for obs in range(num_observables):\n",
    "    for value in range(num_values):\n",
    "        exp_title = 'Experiment '+str(experiment_index)+' checks whether observable '\n",
    "        exp_title += str(obs)+' has value '+str(value)\n",
    "        One_Obs_Exps.append( [exp_title , obs, value])\n",
    "        experiment_index += 1\n",
    "        \n",
    "Two_Obs_Exps = []\n",
    "experiment_index = 0\n",
    "\n",
    "for obs1 in range(0, num_observables-1, 1):\n",
    "    #We don't want several copies of the same experiment. Therefore, we make the convention that obs2 is always larger than obs1.\n",
    "    for obs2 in range(obs1+1, num_observables, 1):\n",
    "        for value1 in range(num_values):\n",
    "            for value2 in range(num_values):\n",
    "                exp_title = 'Experiment '+str(experiment_index)+' checks whether observable '\n",
    "                exp_title += str(obs1)+' has value '+str(value1) +' and whether observable '\n",
    "                exp_title += str(obs2)+' has value '+str(value2)\n",
    "                Two_Obs_Exps.append( [exp_title , obs1, value1, obs2, value2])\n",
    "                experiment_index += 1\n",
    "                \n",
    "Three_Obs_Exps = []\n",
    "experiment_index = 0\n",
    "\n",
    "for obs1 in range(0, num_observables-2, 1):\n",
    "    for obs2 in range(obs1+1, num_observables-1, 1):\n",
    "        for obs3 in range(obs2+1, num_observables, 1):\n",
    "            for value1 in range(num_values):\n",
    "                for value2 in range(num_values):\n",
    "                    for value3 in range(num_values):\n",
    "                        exp_title = 'Experiment '+str(experiment_index)+' checks whether observable '\n",
    "                        exp_title += str(obs1)+' has value '+str(value1) +' and whether observable '\n",
    "                        exp_title += str(obs2)+' has value '+str(value2) + ' and whether observable '\n",
    "                        exp_title += str(obs3)+' has value ' + str(value3)\n",
    "                        Three_Obs_Exps.append( [exp_title , obs1, value1, obs2, value2, obs3, value3])\n",
    "                        experiment_index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e9628",
   "metadata": {},
   "source": [
    "Next, we can define our reward function for the middle layer. It is +1 or -1 depending on whether the agent identifies the value of the chosen observable correctly or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82e0195d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardLayer2(percept, predicted_answer):\n",
    "    chosenObs = predicted_answer // num_values\n",
    "    chosenValue = predicted_answer % num_values\n",
    "\n",
    "    if percept[chosenObs] == chosenValue:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4912f2",
   "metadata": {},
   "source": [
    "Now, we define the reward function for the third layer. For a given experiment, it checks whether the agent's answers are correct. If they are, the reward is +1, otherwise it is -1. \n",
    "\n",
    "We also output whether the right answer is yes or no: A no-answer is much more likely, so we will later give a larger weight to a correct yes-answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8707d339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewardLayer3(percept, experiment_index, predicted_answer , experiment_list = One_Obs_Exps):\n",
    "    rightAnswer = 1\n",
    "    for targetObsInd, targetValueInd in zip(range(1, len(experiment_list[experiment_index]), 2) , range(2, len(experiment_list[experiment_index]), 2)):\n",
    "        #Read observable and the value asked for by the experiment\n",
    "        obs = experiment_list[experiment_index][targetObsInd]\n",
    "        suggested_value = experiment_list[experiment_index][targetValueInd]\n",
    "        actual_value = percept[obs]\n",
    "        if suggested_value != actual_value:\n",
    "            #rightAnswer is 1 if the observable value asked for by the experiment is the actual value of the percept, otherwise 0.\n",
    "            rightAnswer = 0\n",
    "\n",
    "    if rightAnswer == predicted_answer:\n",
    "        return 1, rightAnswer\n",
    "    else:\n",
    "        return -1, rightAnswer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c62045",
   "metadata": {},
   "source": [
    "Here, you can double check that the reward function works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ea18196",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Experiment 9 checks whether observable 0 has value 0 and whether observable 2 has value 0', 0, 0, 2, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1, 0)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(Two_Obs_Exps[9])\n",
    "rewardLayer3([1,0,0], 9 , 1, Two_Obs_Exps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c704a209",
   "metadata": {},
   "source": [
    "Next, we define our loss function inspired by a classical 2-layer PS. It is intended to be used with experience replay.\n",
    "\n",
    "As distance measure between probabilities, we use the relative entropy (Kullback-Leibler divergence). \n",
    "\n",
    "Since we will generate the percepts i.i.d. we will not use the glow or forgetting mechanisms. This allows us to approximate the PS update rule enforced by the loss function as $$p_{new} = p_{old} + r$$ for the chosen action given the percept.\n",
    "\n",
    "To make the loss function numerically simpler, we will not include actions that were not chosen into the loss function. This also helps us to avoid unnormalized probability distributions resulting from using probabilities instead of h-values in our approximation. Since we do not consider full probability distributions anymore, for each sample in the batch we apply the relative entropy to $(p_{new},1-p_{new})$ and $(p_{old}+r,1-[p_{old}+r])$.\n",
    "\n",
    "To prevent that $ p_{new} $ is smaller than 0 or larger than 1, we use nested ReLUs to cut off $ p_{new} $ at 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5a8b0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiPSlossFunction(pGuesses, pOlds, rewards):\n",
    "    \"\"\"\n",
    "    PS loss function using the approximation p_{new , k} = p_{old , k} + r_k for chosen actions a_k.\n",
    "    This function is intended to be used with experience replay, and k is the index of the sample in the batch.\n",
    "    Non-chosen actions and percepts are dropped, we only reinforce rewards, not old models.\n",
    "    \"\"\"\n",
    "    relu = torch.nn.ReLU()\n",
    "    pTargets = relu(pOlds+torch.tensor(rewards))\n",
    "    pTargets = 1.0 - relu(1.0 - pTargets)\n",
    "    \n",
    "    # We use regularizers to prevent log(0) and division by 0\n",
    "    reguLow = torch.tensor(0.00001)\n",
    "    reguHigh = torch.tensor(1.00001)\n",
    "    \n",
    "    Loss = torch.sum(pTargets*torch.log((pTargets+reguLow)/(pGuesses+reguLow)) + (1.0-pTargets)*torch.log((reguHigh-pTargets)/(reguHigh-pGuesses)))\n",
    "    #print(Loss)\n",
    "    return(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168fb936",
   "metadata": {},
   "source": [
    "As our next step, we define our Quantum PS agent.\n",
    "\n",
    "The middle layer (observable layer) is implemented via interferometer trees, one for each percept. While this does not allow different percepts to interfere, it is much simpler to optimize. We will later consider three loss functions that need to be optimized simultaneously, and using seperate trees at least allows us to avoid additional unitary constraints.\n",
    "\n",
    "For the last layer (experiment layer), we use an exchangable Clements layout. This is shared by all percepts (allowing for interference), but each experiment gets its own. This is our toy model for transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48782898",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QPSTreeOneExp(nn.Module):\n",
    "    def __init__(self, num_percepts = num_values**num_observables, num_hidden = num_observables*num_values, ini = \"fixed\"):\n",
    "        super(QPSTreeOneExp, self).__init__()\n",
    "        self.num_percepts = num_percepts\n",
    "        self.num_hidden = num_hidden \n",
    "        \n",
    "        self.U_percept_bag = []\n",
    "        self.U_full_bag = []\n",
    "        \n",
    "        self.ini = ini\n",
    "        \n",
    "        #Initialize the variational phases, either randomly or to some fixed values. \n",
    "        #For the middle layer, we will define far more phases than we actually use. \n",
    "        #We use a rectangular array with one index for the tree layer, and one index \n",
    "        #for the waveguide within that layer. This makes the code much more readable. \n",
    "        #For the last layer, we will use post-selection to remove all outputs except \n",
    "        #for two waveguides associated to a yes or no answer. The experiment Clements \n",
    "        #unitary therefore has a dimensionality corresponding to the size of the \n",
    "        #middle layer, i.e. #observables * #values.\n",
    "        \n",
    "        # Percept to middle layer parameters, first index is the percept:\n",
    "        if ini == \"fixed\":\n",
    "            self.CPhases_percept_bag = nn.Parameter( ( (torch.pi/4)*torch.ones(size = [ num_percepts, int(np.ceil(np.log2(self.num_hidden))) , self.num_hidden], dtype = torch.float32) ))\n",
    "            self.OPhases_percept_bag = ( nn.Parameter( ( (torch.pi/4)*torch.ones(size = [ num_percepts, self.num_hidden], dtype = torch.float32) ) ) )\n",
    "        if ini == \"random\":\n",
    "            self.CPhases_percept_bag = nn.Parameter( ( (2.0*torch.pi)*torch.randn(size = [num_percepts , int(np.ceil(np.log2(self.num_hidden))) , self.num_hidden], dtype = torch.float32) )) \n",
    "            self.OPhases_percept_bag = nn.Parameter( ( (2.0*torch.pi)*torch.randn(size = [num_percepts , self.num_hidden], dtype = torch.float32) ) )\n",
    "        \n",
    "        # Middle to final layer parameters:\n",
    "        if ini == \"fixed\":\n",
    "            self.CPhases_task_bag = nn.Parameter( ( (torch.pi/4)*torch.ones(size = [self.num_hidden, self.num_hidden], dtype = torch.float32) ) ) \n",
    "            self.OPhases_task_bag = nn.Parameter( ( (torch.pi/4)*torch.ones(size = [self.num_hidden], dtype = torch.float32) ) ) \n",
    "        if ini == \"random\":\n",
    "            self.CPhases_task_bag = nn.Parameter( ( (2.0*torch.pi)*torch.randn(size = [self.num_hidden, self.num_hidden], dtype = torch.float32) ) ) \n",
    "            self.OPhases_task_bag = nn.Parameter( ( (2.0*torch.pi)*torch.randn(size = [self.num_hidden], dtype = torch.float32) ) ) \n",
    "            \n",
    "        \n",
    "        \n",
    "        #We initilialize some dummy unitaries that we will overwrite with the interferometer unitaries\n",
    "        for p in range(num_percepts):\n",
    "            self.U_percept_bag.append(torch.zeros( size = [num_hidden,num_hidden] , dtype = torch.cfloat))\n",
    "            self.U_full_bag.append(torch.zeros( size = [num_hidden,num_hidden] , dtype = torch.cfloat))\n",
    "            \n",
    "        \n",
    "        #After initializing the variational phases, we also generate the corresponding interferometer unitaries.\n",
    "        self.recalculate_full_unitaries()\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Since we will exchange the last layer often, we implement a method to reset the last layer.\n",
    "    def reset_task(self):\n",
    "        if self.ini == \"fixed\":\n",
    "            self.CPhases_task_bag = nn.Parameter( ( (torch.pi/4)*torch.ones(size = [self.num_hidden, self.num_hidden], dtype = torch.float32) ))\n",
    "            self.OPhases_task_bag = nn.Parameter( ( (torch.pi/4)*torch.ones(size = [self.num_hidden], dtype = torch.float32) ) )\n",
    "        if self.ini == \"random\":\n",
    "            self.CPhases_task_bag = nn.Parameter( ( (2.0*torch.pi)*torch.randn(size = [self.num_hidden, self.num_hidden], dtype = torch.float32) ) )\n",
    "            self.OPhases_task_bag = nn.Parameter( ( (2.0*torch.pi)*torch.randn(size = [self.num_hidden], dtype = torch.float32) ) )\n",
    "        self.recalculate_full_unitaries()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # Converts percepts of the form [2,1,2] into a single integer (injectively).\n",
    "    # This single integer, called the index, will often be used to refer to the percept.\n",
    "    def convert_percept_to_index(self, percept): \n",
    "        onedimvalue = 0\n",
    "        for j in range(num_observables):\n",
    "            onedimvalue += (num_values**j)*percept[j]\n",
    "        return onedimvalue\n",
    "    \n",
    "    \n",
    "    \n",
    "    def recalculate_percept_unitary(self, percept):\n",
    "        percept_index = self.convert_percept_to_index(percept)\n",
    "        self.U_percept_bag[percept_index] = Tree(dim = self.num_hidden , TreePhases = self.CPhases_percept_bag[percept_index], OutputPhases = self.OPhases_percept_bag[percept_index] )\n",
    "    \n",
    "    \n",
    "    def recalculate_full_unitary(self, percept, is_index = 0):\n",
    "        if is_index == 0:\n",
    "            percept_index = self.convert_percept_to_index(percept)\n",
    "        elif is_index == 1:\n",
    "            percept_index = percept\n",
    "        self.U_percept_bag[percept_index] = Tree(dim = self.num_hidden , TreePhases = self.CPhases_percept_bag[percept_index], OutputPhases = self.OPhases_percept_bag[percept_index])\n",
    "        self.U_task = Clements(dim = self.num_hidden , ClementsPhases = self.CPhases_task_bag, outputPhases = self.OPhases_task_bag)\n",
    "        self.U_full_bag[percept_index] = torch.matmul(self.U_task, self.U_percept_bag[percept_index])  \n",
    "        \n",
    "        \n",
    "    def recalculate_full_unitaries(self):\n",
    "        for percept_index in range(self.num_percepts):\n",
    "            self.recalculate_full_unitary(percept_index, is_index = 1)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "    # This method returns the probabilities for actions in the middle layer, by taking the modulus square of the amplitudes.\n",
    "    def probs2(self, percept):\n",
    "        self.recalculate_percept_unitary(percept)\n",
    "        percept_index = self.convert_percept_to_index(percept)\n",
    "        torchUnnormedProbs = torch.square(torch.abs(self.U_percept_bag[percept_index][ 0:num_observables*num_values , 0]))\n",
    "        torchNormedProbs = (torchUnnormedProbs/torch.sum(torchUnnormedProbs))\n",
    "        return torchNormedProbs.clone()\n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    # The next method allows us to randomly sample an action in the middle layer, using the probabilities from above.\n",
    "    def sample2(self, percept):\n",
    "        torchProbs = self.probs2(percept)\n",
    "        probs = torchProbs.clone().detach().numpy()\n",
    "        sampled_action = np.random.choice(np.arange(torchProbs.size(0)) , p = probs)\n",
    "        return sampled_action   \n",
    "    \n",
    "    \n",
    "    \n",
    "    # This method returns the probabilities that the middle layer picks a certain observable to predict. These will be used to implement a curiosity mechanism. This function simply sums the probabilities from above over all values per observable.\n",
    "    def probs2Observables(self, percept):\n",
    "        torchProbs = self.probs2(percept)\n",
    "        probsObs = torch.zeros(size = [num_observables], dtype = torch.float32)\n",
    "        for obsIndex in range(num_observables):\n",
    "            probsObs[obsIndex] = torch.sum(torchProbs[obsIndex*num_values : obsIndex*num_values + num_values])\n",
    "        return probsObs\n",
    "    \n",
    "    \n",
    "    # Returns the output phases of the middle layer. These will later be forced to be close to 0, such that interference is not prevented by random phases in the middle layer.\n",
    "    def phases2(self, percept):\n",
    "        self.recalculate_percept_unitary(percept)\n",
    "        percept_index = self.convert_percept_to_index(percept)\n",
    "        phases = torch.zeros(size = [self.num_hidden], dtype = torch.cfloat)\n",
    "        for j in range(num_observables*num_values):\n",
    "            if self.U_percept_bag[percept_index][ j , 0] != 0.0:\n",
    "                phases[j] = torch.angle(self.U_percept_bag[percept_index][j , 0].clone())\n",
    "            else:\n",
    "                # Since the origin does not have a well-defined phase, we define it to be 0.\n",
    "                phases[j] = 0.0\n",
    "        return phases\n",
    "    \n",
    "    \n",
    "    # Action probabilities for the third layer. Here, post-selection is applied such that only 2 answers are possible.\n",
    "    def probs3(self, percept):\n",
    "        self.recalculate_full_unitary(percept)\n",
    "        percept_index = self.convert_percept_to_index(percept)\n",
    "        torchUnnormedProbs = torch.square(torch.abs(self.U_full_bag[percept_index][0 : 2, 0]))\n",
    "        torchNormedProbs = torchUnnormedProbs/torch.sum(torchUnnormedProbs)\n",
    "        return torchNormedProbs\n",
    "    \n",
    "    \n",
    "    # Sample a random action for the last layer, using the probabilities from above.\n",
    "    def sample3(self , percept):\n",
    "        torchNormedProbs = self.probs3(percept)\n",
    "        probs = torchNormedProbs.clone().detach().numpy()\n",
    "        sampled_action = np.random.choice(np.arange(torchNormedProbs.size(0)) , p = probs)\n",
    "        return sampled_action \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2629310d",
   "metadata": {},
   "source": [
    "Time to create our agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2549f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ini_choice = \"random\"\n",
    "ini_choice = \"fixed\"\n",
    "\n",
    "TreeAgent = QPSTreeOneExp(num_percepts = num_values**num_observables, num_hidden = num_values*num_observables, ini = ini_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236dc52f",
   "metadata": {},
   "source": [
    "Now, we define the function that will train the middle layer. The environment uniformly samples (in particular i.i.d.) a percept. The agent picks an observable, and predicts the value of that observable. In addition to the PS loss function, we have two more contributions to the full loss function:\n",
    "\n",
    "1) To prevent that interference effects in the last layer get weakened by random phases in the middle layer, we force those phases to be close to 0.\n",
    "\n",
    "2) Values of different observables cannot interfere well constructively and destructively if one observable has much more weight than the other. Therefore, we use a curiosity mechanism that encourages the agent to pick each observable with uniform probability. This is implemented via the Shannon negentropy of the observables in the middle layer. \n",
    "\n",
    "We use experience replay. This means we collect many (percept, action, reward) samples, and then use them as a training data batch for updating. We point out that our batch is chosen so large that it will contain the entire percept space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38c67477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_obs_with_replay(Agent = TreeAgent, replay_memory_size = 600, num_rounds = 20000, num_optimizer_steps = 10, coeff_Shannon = 1.0, coeff_phase = 0.01, learning_rate = 0.01):\n",
    "\n",
    "    logfile_obs = open(dirNameFigures+\"/logfile_obs.txt\", \"w\")\n",
    "    logfile_obs.close()\n",
    "    \n",
    "    # The experience replay memory\n",
    "    reward_memory = []\n",
    "    percept_memory = []\n",
    "    action_memory = []\n",
    "\n",
    "    # Memory for creating a plot at the end\n",
    "    round_list = []\n",
    "    Shannon_list = []\n",
    "    acc_list = []\n",
    "    phase_list = []\n",
    "\n",
    "    num_acc = replay_memory_size\n",
    "    \n",
    "    counter = 0\n",
    "    accuracy = 0.0\n",
    "    \n",
    "    last_round_to_draw_open = True\n",
    "    \n",
    "    perfection = 0\n",
    "    \n",
    "    add_Shannon_and_perfection = False\n",
    "\n",
    "    for roun in range(num_rounds):\n",
    "        #Each round corresponds to sampling one action.\n",
    "        #counter tells us the size of the replay memory so far.\n",
    "        counter += 1\n",
    "        \n",
    "        #Randomly sample percept.\n",
    "        percept = []\n",
    "        for observables in range(num_observables):\n",
    "            percept.append(np.random.randint(num_values))\n",
    "            \n",
    "        #Add training sample to memory\n",
    "        percept_memory.append(percept.copy())\n",
    "        agentGuess = Agent.sample2(percept)\n",
    "        action_memory.append(agentGuess)\n",
    "        r = 0.1 * rewardLayer2(percept, agentGuess)\n",
    "        reward_memory.append(r)\n",
    "        \n",
    "        # Count how many right answers agent has given.\n",
    "        if r > 0.0:\n",
    "            accuracy += 1.0\n",
    "            \n",
    "        # When we have collected enough samples, evaluate and store the accuracy. Then reset it.\n",
    "        if counter == num_acc:\n",
    "            round_list.append(roun)\n",
    "            accuracy /= num_acc\n",
    "            acc_list.append(accuracy)\n",
    "            print(\"Accuracy: \"+ str(accuracy))\n",
    "            logfile_obs = open(dirNameFigures+\"/logfile_obs.txt\", \"a\")\n",
    "            logfile_obs.write(\"Accuracy: \"+ str(accuracy)+\" \\n\")\n",
    "            logfile_obs.close()\n",
    "            last_acc = accuracy\n",
    "            accuracy = 0.0\n",
    "            counter = 0\n",
    "            # The following will be used in an early stopping criterion.\n",
    "            add_Shannon_and_perfection = True\n",
    "        \n",
    "        #When the replay memory has reached its maximum size, we train the agent.\n",
    "        if len(reward_memory) == replay_memory_size:\n",
    "            num_percepts = num_values**num_observables\n",
    "            \n",
    "            # When the agent reached a high accuracy, we make the learning-rate much smaller to allow for fine-tuning of the variational phases.\n",
    "            if last_acc >= 0.95:\n",
    "                current_learning_rate = 0.1 * learning_rate\n",
    "            else:\n",
    "                current_learning_rate = learning_rate\n",
    "            \n",
    "            # Tell the optimizer to vary the phases of the middle layer. \n",
    "            paramlist = []\n",
    "            paramlist.append( {'params' : Agent.CPhases_percept_bag})\n",
    "            paramlist.append( {'params' : Agent.OPhases_percept_bag})\n",
    "            \n",
    "            optimizer = optim.Adam(paramlist, lr = current_learning_rate)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            Agent.recalculate_full_unitaries()\n",
    "            \n",
    "            # Read the probabilities p_{old} for the PS loss function, to be used for the targets\n",
    "            oldProbs = torch.zeros(size = [replay_memory_size], dtype = torch.float32)\n",
    "            for memory_index in range(replay_memory_size):\n",
    "                current_percept = percept_memory[memory_index].copy()\n",
    "                percept_index = Agent.convert_percept_to_index(current_percept)\n",
    "                probsGuessOld  = Agent.probs2(percept = current_percept).clone().detach()\n",
    "                oldProbs[memory_index] = probsGuessOld[action_memory[memory_index]].clone()\n",
    "\n",
    "            num_steps = num_optimizer_steps\n",
    "\n",
    "            # Iterate over the number of optimization steps.\n",
    "            for step in range(num_steps):\n",
    "                \n",
    "                Agent.recalculate_full_unitaries()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Read the current probabilities p_{new} to compare them to the targets.\n",
    "                guessProbs = torch.zeros(size = [replay_memory_size] , dtype = torch.float32)\n",
    "                for memory_index in range(replay_memory_size):\n",
    "                    current_percept = percept_memory[memory_index].copy()\n",
    "                    percept_index = Agent.convert_percept_to_index(current_percept)\n",
    "                    probsGuessFullPercept  = Agent.probs2(percept = current_percept)\n",
    "                    guessProbs[memory_index] = probsGuessFullPercept[action_memory[memory_index]].clone()\n",
    "                    \n",
    "                    # Now, also collect the relative phases and the Shannon negentropy of the observables.\n",
    "                    if memory_index == 0:\n",
    "                        phasesForLoss = Agent.phases2(percept = current_percept)\n",
    "                        probsGuessObs  = Agent.probs2Observables(percept = current_percept)\n",
    "                        Shannon = (torch.log(torch.tensor(num_observables))-torch.sum(-probsGuessObs.clone()*torch.log(probsGuessObs.clone()+ 0.0)))\n",
    "                        \n",
    "                        # The following is not used for training, just for generating the graph.\n",
    "                        # Since we only care about relative phases of answers that have a significant probability to occur, we weight the relative phases with the probability.\n",
    "                        weightedPhases = Agent.probs2(percept = current_percept).clone().data*phasesForLoss.clone().data\n",
    "                    else: \n",
    "                        phasesForLoss = torch.cat( (phasesForLoss, Agent.phases2(percept = current_percept)), 0 )\n",
    "                        probsGuessObs = Agent.probs2Observables(percept = current_percept)\n",
    "                        Shannon = (Shannon + torch.log(torch.tensor(num_observables))-torch.sum(-probsGuessObs.clone()*torch.log(probsGuessObs.clone()+0.0)))\n",
    "                        \n",
    "                        #For graph and stopping only\n",
    "                        extraWeightedPhases = Agent.probs2(percept = current_percept).data*Agent.phases2(percept = current_percept).data\n",
    "                        weightedPhases = torch.cat((weightedPhases , extraWeightedPhases), 0)\n",
    "                \n",
    "                # Now, we put the loss functions together.\n",
    "                phaseLoss = torch.sum(torch.abs(phasesForLoss))\n",
    "                PSloss = MultiPSlossFunction(guessProbs, oldProbs.clone().detach(), reward_memory)\n",
    "                fullLoss = (coeff_phase*phaseLoss + 1.0*PSloss + coeff_Shannon*Shannon)\n",
    "                fullLoss.backward()\n",
    "                \n",
    "                # The following outputs help to judge the training of the agent.\n",
    "                if counter % 10 == 0 and step % 20 == 0:\n",
    "                    print(\"Amplitudes for percept [0,0,0]: \") \n",
    "                    print(Agent.U_percept_bag[0])\n",
    "                    logfile_obs = open(dirNameFigures+\"/logfile_obs.txt\", \"a\")\n",
    "                    logfile_obs.write(\"Amplitudes for percept [0,0,0]: \\n \")\n",
    "                    logfile_obs.write(str(Agent.U_percept_bag[0].data))\n",
    "                    logfile_obs.write(\"\\n\")\n",
    "                    logfile_obs.close()\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                Agent.recalculate_full_unitaries()\n",
    "                \n",
    "            # When we have collected enough samples to evaluate the accuracy:\n",
    "            # Collect the average Shannon loss and phase loss to be plotted at the end\n",
    "            if add_Shannon_and_perfection:\n",
    "                add_Shannon_and_perfection = False\n",
    "                ave_Shannon = Shannon.data/replay_memory_size\n",
    "                Shannon_list.append(ave_Shannon.numpy())\n",
    "                ave_weighted_phase = torch.sum(torch.abs(weightedPhases))/replay_memory_size\n",
    "                phase_list.append(ave_weighted_phase.data.numpy())\n",
    "                print(\"L1 norm of phases: \" + str(phaseLoss.data.numpy()))\n",
    "                print(\"Average weighted phase: \" + str(phase_list[-1]))\n",
    "                print(\" \")\n",
    "                logfile_obs = open(dirNameFigures+\"/logfile_obs.txt\", \"a\")\n",
    "                logfile_obs.write(\"L1 norm of phases: \" + str(phaseLoss.data.numpy())+\"\\n \")\n",
    "                logfile_obs.write(\"Average weighted phase: \" + str(phase_list[-1]) + \" \\n \\n \")\n",
    "                logfile_obs.close()\n",
    "                \n",
    "                # Early stopping criterion: \n",
    "                # When the agent performed near perfect several times in a row, and the average weighted phase is very small, stop the training. \n",
    "                if last_acc >= 0.99 and phase_list[-1] < 0.1:\n",
    "                    last_acc = 0.0\n",
    "                    perfection += 1\n",
    "                    #Store the last round to draw in the plot at the end.\n",
    "                    if last_round_to_draw_open:\n",
    "                        last_round_to_draw_open = False\n",
    "                        last_round_to_draw = roun\n",
    "                else:\n",
    "                    perfection = 0\n",
    "\n",
    "            percept_memory = []\n",
    "            action_memory = []\n",
    "            reward_memory = []\n",
    "            \n",
    "        if perfection >= 10:\n",
    "            break\n",
    "            \n",
    "    # Creation of the plot      \n",
    "    #fig, ax = plt.subplots(figsize=(7, 4), layout='constrained')\n",
    "    fig, ax = plt.subplots(figsize=(7, 4))\n",
    "    ax.plot(round_list[0:last_round_to_draw+1], acc_list[0:last_round_to_draw+1], label= 'accuracy (1.0 means 100 percent)')  \n",
    "    ax.plot(round_list[0:last_round_to_draw+1], Shannon_list[0:last_round_to_draw+1], label='Shannon negentropy of observable choice (0.0 is uniform)')   \n",
    "    ax.plot(round_list[0:last_round_to_draw+1], phase_list[0:last_round_to_draw+1], label=\"Average phase weighted with action probability\" )\n",
    "    ax.set_xlabel('round')  \n",
    "    ax.set_ylabel(' ')  \n",
    "    ax.set_title(\"Training of the middle layer with experience replay\")  \n",
    "    ax.legend();  \n",
    "    plt.savefig(dirNameFigures+\"/obs_training.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    ave_entropy = ((replay_memory_size*torch.log(torch.tensor(num_observables)))-Shannon)/replay_memory_size\n",
    "    \n",
    "    print(\"Last averaged observable Shannon entropy over \"+str(replay_memory_size)+\" samples: \" + str(ave_entropy.data.numpy()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save data for the figures:\n",
    "    fileName = dirNameFigures + \"/MiddleLayerRounds.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, round_list[0:last_round_to_draw+1])\n",
    "        \n",
    "    fileName = dirNameFigures + \"/MiddleLayerAccuracies.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, acc_list[0:last_round_to_draw+1])\n",
    "        \n",
    "    fileName = dirNameFigures + \"/MiddleLayerShannonNegEntropies.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, Shannon_list[0:last_round_to_draw+1])\n",
    "        \n",
    "    fileName = dirNameFigures + \"/MiddleLayerPhases.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, phase_list[0:last_round_to_draw+1])\n",
    "    \n",
    "    \n",
    "    # At last, we save the model itself\n",
    "    torch.save(Agent.state_dict(), dirNameModels+'/MiddleLayerModel.pt')\n",
    "    \n",
    "    # and the percept unitaries\n",
    "    for percept in range(Agent.num_percepts):\n",
    "        if percept == 0:\n",
    "            writeMode = 'w'\n",
    "        else:\n",
    "            writeMode = 'a'\n",
    "            \n",
    "        fileName = dirNameUnitaries + \"/PerceptTrafo.txt\"\n",
    "        with open(fileName , writeMode) as datafile:\n",
    "            datafile.write(\"Percept index:\" +str(percept)+ \" \\n \\n\")\n",
    "            datafile.write( str(Agent.U_percept_bag[percept].clone().data.numpy()))\n",
    "            datafile.write(\" \\n \\n \")\n",
    "            \n",
    "        fileName = dirNameParameters + \"/PerceptCPhases.txt\"    \n",
    "        with open(fileName , writeMode) as datafile:\n",
    "            datafile.write(\"Percept index:\" +str(percept)+ \" \\n \\n\")\n",
    "            datafile.write( str(Agent.CPhases_percept_bag[percept].clone().data.numpy()))\n",
    "            datafile.write(\" \\n \\n \")   \n",
    "            \n",
    "        fileName = dirNameParameters + \"/PerceptOPhases.txt\"    \n",
    "        with open(fileName , writeMode) as datafile:\n",
    "            datafile.write(\"Percept index:\" +str(percept)+ \" \\n \\n\")\n",
    "            datafile.write( str(Agent.OPhases_percept_bag[percept].clone().data.numpy()))\n",
    "            datafile.write(\" \\n \\n \")              \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9376ec9b",
   "metadata": {},
   "source": [
    "Now, it's time to train the middle layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fbffdc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture output_obs\n",
    "train_obs_with_replay(Agent = TreeAgent, replay_memory_size = 600, num_rounds = 100000, num_optimizer_steps = 10, coeff_Shannon = 10.0, coeff_phase = 1.0, learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09c64080-dea3-42a9-840d-a67e798b957f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.2966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[-0.1768-1.7678e-01j],\n",
      "        [-0.1768-1.7678e-01j],\n",
      "        [-0.3536+1.8078e-09j],\n",
      "        [-0.3536+1.8078e-09j],\n",
      "        [-0.3536+1.8078e-09j],\n",
      "        [-0.3536+1.8078e-09j],\n",
      "        [-0.3536+1.8078e-09j],\n",
      "        [-0.3536+1.8078e-09j],\n",
      "        [-0.3536+1.8078e-09j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 14510.125\n",
      "Average weighted phase: 2.7186325\n",
      " \n",
      "Accuracy: 0.37333333333333335\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[-0.2220-0.2715j],\n",
      "        [-0.1829-0.2237j],\n",
      "        [-0.3650+0.0740j],\n",
      "        [-0.3497+0.1478j],\n",
      "        [-0.2864+0.1211j],\n",
      "        [-0.2739+0.1158j],\n",
      "        [-0.3349+0.1416j],\n",
      "        [-0.2864+0.1211j],\n",
      "        [-0.2864+0.1211j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 12830.126\n",
      "Average weighted phase: 2.3979173\n",
      " \n",
      "Accuracy: 0.495\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[-0.2219-0.3348j],\n",
      "        [-0.1485-0.2240j],\n",
      "        [-0.2966+0.1254j],\n",
      "        [-0.2888+0.2974j],\n",
      "        [-0.1922+0.1979j],\n",
      "        [-0.2004+0.2063j],\n",
      "        [-0.2900+0.2986j],\n",
      "        [-0.1760+0.1812j],\n",
      "        [-0.2148+0.2212j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 11150.127\n",
      "Average weighted phase: 2.0755663\n",
      " \n",
      "Accuracy: 0.6233333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[-0.2088-0.3959j],\n",
      "        [-0.1121-0.2125j],\n",
      "        [-0.2232+0.1527j],\n",
      "        [-0.1632+0.4199j],\n",
      "        [-0.0866+0.2228j],\n",
      "        [-0.0973+0.2503j],\n",
      "        [-0.1689+0.4345j],\n",
      "        [-0.0885+0.2276j],\n",
      "        [-0.0885+0.2277j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 9470.129\n",
      "Average weighted phase: 1.765302\n",
      " \n",
      "Accuracy: 0.7233333333333334\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[-0.1857-0.4578j],\n",
      "        [-0.0773-0.1905j],\n",
      "        [-0.1542+0.1588j],\n",
      "        [ 0.0144+0.4918j],\n",
      "        [ 0.0059+0.2024j],\n",
      "        [ 0.0067+0.2278j],\n",
      "        [ 0.0144+0.4945j],\n",
      "        [ 0.0066+0.2251j],\n",
      "        [ 0.0054+0.1847j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 7790.131\n",
      "Average weighted phase: 1.4454331\n",
      " \n",
      "Accuracy: 0.8016666666666666\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[-0.1490-0.5077j],\n",
      "        [-0.0455-0.1549j],\n",
      "        [-0.0905+0.1409j],\n",
      "        [ 0.2110+0.4610j],\n",
      "        [ 0.0634+0.1385j],\n",
      "        [ 0.0919+0.2009j],\n",
      "        [ 0.2198+0.4803j],\n",
      "        [ 0.0795+0.1737j],\n",
      "        [ 0.0608+0.1328j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 6110.131\n",
      "Average weighted phase: 1.1333885\n",
      " \n",
      "Accuracy: 0.8866666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[-0.1020-0.5440j],\n",
      "        [-0.0206-0.1097j],\n",
      "        [-0.0408+0.1049j],\n",
      "        [ 0.3956+0.3624j],\n",
      "        [ 0.0781+0.0715j],\n",
      "        [ 0.1346+0.1233j],\n",
      "        [ 0.4065+0.3723j],\n",
      "        [ 0.1019+0.0934j],\n",
      "        [ 0.0849+0.0778j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4430.131\n",
      "Average weighted phase: 0.820647\n",
      " \n",
      "Accuracy: 0.9333333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[-0.0490-0.5728j],\n",
      "        [-0.0050-0.0589j],\n",
      "        [-0.0099+0.0573j],\n",
      "        [ 0.5253+0.1868j],\n",
      "        [ 0.0522+0.0186j],\n",
      "        [ 0.1188+0.0422j],\n",
      "        [ 0.5318+0.1891j],\n",
      "        [ 0.0900+0.0320j],\n",
      "        [ 0.0674+0.0240j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 2882.65\n",
      "Average weighted phase: 0.5332144\n",
      " \n",
      "Accuracy: 0.975\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[1.4726e-02-5.7294e-01j],\n",
      "        [1.5259e-04-5.9367e-03j],\n",
      "        [8.5821e-05+4.7391e-03j],\n",
      "        [5.6920e-01-2.3446e-02j],\n",
      "        [4.3939e-03-1.8099e-04j],\n",
      "        [9.4888e-02-4.2942e-03j],\n",
      "        [5.7729e-01-2.6125e-02j],\n",
      "        [5.1120e-02-2.3134e-03j],\n",
      "        [3.6299e-02-1.6427e-03j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4857.951\n",
      "Average weighted phase: 0.5126645\n",
      " \n",
      "Accuracy: 0.97\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.3596e-02-5.7542e-01j],\n",
      "        [ 4.8366e-05-6.3838e-04j],\n",
      "        [ 6.1057e-07-3.3649e-04j],\n",
      "        [ 5.6981e-01-7.7189e-04j],\n",
      "        [-5.8358e-04+9.6973e-07j],\n",
      "        [ 8.8617e-02-4.6470e-04j],\n",
      "        [ 5.7554e-01-3.0181e-03j],\n",
      "        [ 4.6032e-02-2.4139e-04j],\n",
      "        [ 3.2822e-02-1.7212e-04j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4781.6514\n",
      "Average weighted phase: 0.49913177\n",
      " \n",
      "Accuracy: 0.9666666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.9001e-02-5.7612e-01j],\n",
      "        [ 5.8784e-05-6.2523e-04j],\n",
      "        [-1.4703e-06+1.2839e-04j],\n",
      "        [ 5.6781e-01-1.9946e-03j],\n",
      "        [-8.8058e-05+6.1315e-07j],\n",
      "        [ 8.3801e-02-2.2480e-04j],\n",
      "        [ 5.7671e-01-1.5471e-03j],\n",
      "        [ 4.1349e-02-1.1092e-04j],\n",
      "        [ 2.9375e-02-7.8803e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 3459.802\n",
      "Average weighted phase: 0.4912265\n",
      " \n",
      "Accuracy: 0.965\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 7.2214e-02-5.6999e-01j],\n",
      "        [-1.0215e-05+9.5637e-05j],\n",
      "        [ 9.7932e-06-5.5203e-04j],\n",
      "        [ 5.7270e-01-2.8389e-04j],\n",
      "        [ 2.2508e-04-1.4341e-06j],\n",
      "        [ 7.8347e-02+7.0560e-05j],\n",
      "        [ 5.7772e-01+5.2030e-04j],\n",
      "        [ 3.6658e-02+3.3014e-05j],\n",
      "        [ 2.5931e-02+2.3354e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4439.7134\n",
      "Average weighted phase: 0.48377082\n",
      " \n",
      "Accuracy: 0.97\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 8.4207e-02-5.7161e-01j],\n",
      "        [ 4.8570e-05-4.2196e-04j],\n",
      "        [-1.8545e-06+9.5280e-05j],\n",
      "        [ 5.7256e-01-2.5429e-04j],\n",
      "        [-5.6046e-05+1.4344e-07j],\n",
      "        [ 7.2173e-02-2.9231e-05j],\n",
      "        [ 5.7587e-01-2.3324e-04j],\n",
      "        [ 3.1719e-02-1.2847e-05j],\n",
      "        [ 2.2498e-02-9.1121e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 3461.9001\n",
      "Average weighted phase: 0.4757648\n",
      " \n",
      "Accuracy: 0.9816666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 9.6148e-02-5.6556e-01j],\n",
      "        [-3.5451e-05+2.8503e-04j],\n",
      "        [-6.7245e-06+2.7532e-04j],\n",
      "        [ 5.7670e-01+1.4457e-03j],\n",
      "        [ 2.8670e-04-6.0685e-07j],\n",
      "        [ 6.6414e-02-5.4277e-05j],\n",
      "        [ 5.7689e-01-4.7147e-04j],\n",
      "        [ 2.6967e-02-2.2039e-05j],\n",
      "        [ 1.9221e-02-1.5708e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4424.9546\n",
      "Average weighted phase: 0.4681519\n",
      " \n",
      "Accuracy: 0.9783333333333334\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 1.1256e-01-5.6615e-01j],\n",
      "        [-5.3421e-06+3.7383e-05j],\n",
      "        [-1.4946e-05+4.1205e-04j],\n",
      "        [ 5.7255e-01+2.6434e-03j],\n",
      "        [-1.0860e-04-2.4801e-07j],\n",
      "        [ 6.1509e-02+5.5834e-06j],\n",
      "        [ 5.7832e-01+5.2495e-05j],\n",
      "        [ 2.2298e-02+2.0239e-06j],\n",
      "        [ 1.5883e-02+1.4417e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 3370.3284\n",
      "Average weighted phase: 0.46083674\n",
      " \n",
      "Accuracy: 0.99\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 1.2925e-01-5.6178e-01j],\n",
      "        [-2.1433e-06+1.2742e-05j],\n",
      "        [ 2.3597e-06-5.2052e-05j],\n",
      "        [ 5.7342e-01+1.9445e-03j],\n",
      "        [ 2.4963e-04+2.6833e-06j],\n",
      "        [ 5.5776e-02-1.5862e-05j],\n",
      "        [ 5.7905e-01-1.6468e-04j],\n",
      "        [ 1.7573e-02-4.9977e-06j],\n",
      "        [ 1.2566e-02-3.5738e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4195.619\n",
      "Average weighted phase: 0.4525146\n",
      " \n",
      "Accuracy: 0.98\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 1.3486e-01-5.6056e-01j],\n",
      "        [-6.9723e-06+4.4159e-05j],\n",
      "        [-1.0049e-05+2.0729e-04j],\n",
      "        [ 5.7339e-01+9.6231e-05j],\n",
      "        [ 5.4002e-04+2.8048e-06j],\n",
      "        [ 5.0213e-02+9.2985e-05j],\n",
      "        [ 5.7969e-01+1.0735e-03j],\n",
      "        [ 1.2909e-02+2.3905e-05j],\n",
      "        [ 9.1899e-03+1.7018e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 3451.397\n",
      "Average weighted phase: 0.4434949\n",
      " \n",
      "Accuracy: 0.9883333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 1.4835e-01-5.5950e-01j],\n",
      "        [ 2.1021e-04-1.2376e-03j],\n",
      "        [-9.8285e-07+1.7896e-05j],\n",
      "        [ 5.7221e-01+1.6866e-03j],\n",
      "        [ 4.0753e-04+5.0910e-06j],\n",
      "        [ 4.5411e-02-5.5647e-05j],\n",
      "        [ 5.7910e-01-7.0965e-04j],\n",
      "        [ 8.2086e-03-1.0059e-05j],\n",
      "        [ 5.8853e-03-7.2120e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4478.0054\n",
      "Average weighted phase: 0.43730748\n",
      " \n",
      "Accuracy: 0.9816666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 1.6198e-01-5.5514e-01j],\n",
      "        [ 9.2016e-06-4.9329e-05j],\n",
      "        [ 2.4886e-06-4.0874e-05j],\n",
      "        [ 5.7350e-01-1.1515e-03j],\n",
      "        [-6.8876e-05-8.8848e-07j],\n",
      "        [ 3.9705e-02-3.3128e-05j],\n",
      "        [ 5.7887e-01-4.8298e-04j],\n",
      "        [ 3.6200e-03-3.0204e-06j],\n",
      "        [ 2.6094e-03-2.1772e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 3883.661\n",
      "Average weighted phase: 0.42890763\n",
      " \n",
      "Accuracy: 0.99\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 1.8332e-01-5.4505e-01j],\n",
      "        [-1.8027e-05+7.8778e-05j],\n",
      "        [-3.8187e-05+4.3102e-04j],\n",
      "        [ 5.7772e-01+1.2380e-03j],\n",
      "        [ 2.9184e-04+6.9472e-06j],\n",
      "        [ 3.3984e-02+3.2908e-05j],\n",
      "        [ 5.7827e-01+5.5996e-04j],\n",
      "        [-4.9243e-04-7.7926e-07j],\n",
      "        [-3.5364e-04-5.5962e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4652.9443\n",
      "Average weighted phase: 0.41905573\n",
      " \n",
      "Accuracy: 0.9933333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 1.9622e-01-5.4049e-01j],\n",
      "        [-1.1229e-04+4.5360e-04j],\n",
      "        [ 2.6728e-05-2.9955e-04j],\n",
      "        [ 5.7734e-01-3.5665e-04j],\n",
      "        [-1.3716e-04-3.3980e-06j],\n",
      "        [ 2.8308e-02+5.1810e-05j],\n",
      "        [ 5.7900e-01+1.0597e-03j],\n",
      "        [-1.5624e-04-9.5670e-07j],\n",
      "        [-1.1224e-04-6.8727e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4102.7124\n",
      "Average weighted phase: 0.41219404\n",
      " \n",
      "Accuracy: 0.9883333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 2.1565e-01-5.3623e-01j],\n",
      "        [-2.0576e-04+7.4743e-04j],\n",
      "        [-2.8031e-05+2.5763e-04j],\n",
      "        [ 5.7865e-01+1.1814e-03j],\n",
      "        [-2.3627e-04-8.5052e-06j],\n",
      "        [ 2.2280e-02+3.4786e-05j],\n",
      "        [ 5.7499e-01+8.9773e-04j],\n",
      "        [ 1.7754e-04+2.3225e-06j],\n",
      "        [ 1.2819e-04+1.6769e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4998.83\n",
      "Average weighted phase: 0.40339887\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 2.1950e-01-5.3254e-01j],\n",
      "        [-2.7215e-05+1.0346e-04j],\n",
      "        [ 2.6494e-05-2.4347e-04j],\n",
      "        [ 5.7856e-01-1.1472e-03j],\n",
      "        [ 2.6373e-04+9.9233e-06j],\n",
      "        [ 1.6874e-02+6.8909e-06j],\n",
      "        [ 5.7724e-01+2.3573e-04j],\n",
      "        [-7.3685e-05-5.4986e-07j],\n",
      "        [-5.3546e-05-3.9958e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4461.108\n",
      "Average weighted phase: 0.39927438\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[2.3969e-01-5.2287e-01j],\n",
      "        [1.1783e-04-3.9443e-04j],\n",
      "        [2.1923e-05-1.6561e-04j],\n",
      "        [5.7871e-01-5.4080e-05j],\n",
      "        [2.9004e-04+1.1258e-05j],\n",
      "        [1.1182e-02-1.2745e-05j],\n",
      "        [5.7803e-01-6.5882e-04j],\n",
      "        [1.2140e-04+1.7697e-06j],\n",
      "        [8.8678e-05+1.2927e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4681.529\n",
      "Average weighted phase: 0.38598582\n",
      " \n",
      "Accuracy: 0.995\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 2.5154e-01-5.2063e-01j],\n",
      "        [-3.7617e-04+1.2226e-03j],\n",
      "        [ 7.1396e-05-5.2216e-04j],\n",
      "        [ 5.8094e-01-2.5415e-03j],\n",
      "        [-3.4550e-04-1.2773e-05j],\n",
      "        [ 5.3194e-03-2.7171e-05j],\n",
      "        [ 5.7282e-01-2.9259e-03j],\n",
      "        [ 2.2410e-05+8.5911e-08j],\n",
      "        [ 1.6218e-05+6.2175e-08j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4939.2695\n",
      "Average weighted phase: 0.37976775\n",
      " \n",
      "Accuracy: 0.99\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 2.6419e-01-5.1211e-01j],\n",
      "        [ 7.9229e-05-2.4461e-04j],\n",
      "        [-5.2822e-06+3.6559e-05j],\n",
      "        [ 5.7765e-01-2.4641e-04j],\n",
      "        [ 6.7198e-05+2.4349e-06j],\n",
      "        [ 6.7961e-04-2.5693e-06j],\n",
      "        [ 5.7815e-01-2.1857e-03j],\n",
      "        [-3.8704e-04+8.0945e-07j],\n",
      "        [-2.7959e-04+5.8473e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4592.416\n",
      "Average weighted phase: 0.3695815\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 2.8499e-01-5.0432e-01j],\n",
      "        [ 9.1364e-05-2.5748e-04j],\n",
      "        [ 1.0526e-05-6.1581e-05j],\n",
      "        [ 5.7411e-01+3.0485e-04j],\n",
      "        [-3.0651e-04-1.1193e-05j],\n",
      "        [ 1.7663e-04+1.8601e-06j],\n",
      "        [ 5.7865e-01+7.6785e-04j],\n",
      "        [ 4.9829e-05+3.5667e-07j],\n",
      "        [ 3.6182e-05+2.5898e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5055.9946\n",
      "Average weighted phase: 0.36377093\n",
      " \n",
      "Accuracy: 0.995\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 2.9787e-01-4.9264e-01j],\n",
      "        [-1.7817e-04+4.6751e-04j],\n",
      "        [ 1.7376e-04-9.5116e-04j],\n",
      "        [ 5.7848e-01+2.3873e-03j],\n",
      "        [ 8.7920e-05+3.7255e-06j],\n",
      "        [ 1.2714e-04+6.8214e-07j],\n",
      "        [ 5.7787e-01+9.9633e-04j],\n",
      "        [ 2.6643e-04-4.8336e-06j],\n",
      "        [ 1.9201e-04-3.4833e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5017.391\n",
      "Average weighted phase: 0.36051798\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 3.0399e-01-4.8860e-01j],\n",
      "        [-1.7937e-05+4.7600e-05j],\n",
      "        [ 3.4248e-05-1.8832e-04j],\n",
      "        [ 5.7936e-01+2.3639e-05j],\n",
      "        [-3.3634e-04-1.1903e-05j],\n",
      "        [-1.2416e-04-9.0169e-07j],\n",
      "        [ 5.7722e-01+3.1903e-03j],\n",
      "        [ 4.0505e-05-5.3902e-07j],\n",
      "        [ 2.8927e-05-3.8494e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5230.1104\n",
      "Average weighted phase: 0.3501548\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[3.1077e-01-4.8542e-01j],\n",
      "        [9.7939e-05-2.6561e-04j],\n",
      "        [1.9352e-05-1.0037e-04j],\n",
      "        [5.7916e-01+1.9941e-03j],\n",
      "        [9.4260e-05+3.7595e-06j],\n",
      "        [1.9040e-04+1.6656e-06j],\n",
      "        [5.7650e-01+1.5754e-03j],\n",
      "        [1.0298e-04-1.3329e-06j],\n",
      "        [7.3441e-05-9.5056e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5066.083\n",
      "Average weighted phase: 0.33876067\n",
      " \n",
      "Accuracy: 0.9933333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 3.2869e-01-4.7374e-01j],\n",
      "        [-4.7311e-05+1.1707e-04j],\n",
      "        [ 1.6459e-04-7.4060e-04j],\n",
      "        [ 5.7852e-01-1.3840e-03j],\n",
      "        [-2.1124e-04-8.3903e-06j],\n",
      "        [-2.9554e-04-2.4296e-06j],\n",
      "        [ 5.7693e-01+3.6893e-07j],\n",
      "        [ 3.1332e-04-6.4904e-07j],\n",
      "        [ 2.2469e-04-4.6544e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 4909.009\n",
      "Average weighted phase: 0.33215934\n",
      " \n",
      "Accuracy: 0.995\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 3.4395e-01-4.6350e-01j],\n",
      "        [-2.2838e-04+5.2871e-04j],\n",
      "        [ 5.3133e-05-2.2498e-04j],\n",
      "        [ 5.7652e-01-1.2792e-03j],\n",
      "        [ 2.1074e-04+9.6665e-06j],\n",
      "        [-7.3033e-05-7.1969e-07j],\n",
      "        [ 5.7834e-01-1.1816e-03j],\n",
      "        [ 1.2811e-04+2.2692e-06j],\n",
      "        [ 9.1604e-05+1.6226e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5812.2676\n",
      "Average weighted phase: 0.32952493\n",
      " \n",
      "Accuracy: 0.9933333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 3.6463e-01-4.5377e-01j],\n",
      "        [ 7.5091e-04-1.5906e-03j],\n",
      "        [-1.4201e-05+5.6057e-05j],\n",
      "        [ 5.7604e-01-1.2976e-04j],\n",
      "        [ 4.4388e-04+2.3410e-05j],\n",
      "        [-2.0181e-04-2.9544e-06j],\n",
      "        [ 5.7386e-01-2.0265e-03j],\n",
      "        [-3.8751e-04-5.2125e-06j],\n",
      "        [-2.7402e-04-3.6860e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5145.9443\n",
      "Average weighted phase: 0.31785783\n",
      " \n",
      "Accuracy: 0.9983333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 3.7860e-01-4.3138e-01j],\n",
      "        [ 4.8299e-04-9.1766e-04j],\n",
      "        [-1.8147e-04+6.7800e-04j],\n",
      "        [ 5.7701e-01+1.8797e-03j],\n",
      "        [-4.6150e-05-3.0423e-06j],\n",
      "        [ 3.4379e-04+8.1061e-06j],\n",
      "        [ 5.8105e-01+1.9309e-03j],\n",
      "        [ 1.8363e-05+3.1305e-07j],\n",
      "        [ 1.3039e-05+2.2228e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5293.2446\n",
      "Average weighted phase: 0.3079188\n",
      " \n",
      "Accuracy: 0.9933333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 3.9035e-01-4.2325e-01j],\n",
      "        [-3.0612e-04+5.6325e-04j],\n",
      "        [ 4.2310e-06-1.5349e-05j],\n",
      "        [ 5.8174e-01+7.9638e-04j],\n",
      "        [ 4.3088e-04+2.9826e-05j],\n",
      "        [-5.3579e-05-5.4676e-07j],\n",
      "        [ 5.7451e-01-2.0318e-04j],\n",
      "        [-6.3706e-04-4.4655e-06j],\n",
      "        [-4.5370e-04-3.1802e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5776.2075\n",
      "Average weighted phase: 0.30542123\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 3.9505e-01-4.2259e-01j],\n",
      "        [ 1.2382e-04-2.3519e-04j],\n",
      "        [-9.5116e-05+3.6087e-04j],\n",
      "        [ 5.7457e-01-5.8317e-05j],\n",
      "        [-7.0017e-05-5.1883e-06j],\n",
      "        [-9.1535e-05-1.4508e-06j],\n",
      "        [ 5.7899e-01-1.5761e-04j],\n",
      "        [-1.2802e-04-1.1420e-06j],\n",
      "        [-9.1333e-05-8.1474e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5175.456\n",
      "Average weighted phase: 0.29500678\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.0863e-01-4.0681e-01j],\n",
      "        [ 1.5414e-04-2.7287e-04j],\n",
      "        [-1.9540e-05+7.0173e-05j],\n",
      "        [ 5.8162e-01+1.0153e-04j],\n",
      "        [ 8.7110e-07+6.8454e-08j],\n",
      "        [ 1.3783e-05+4.9774e-07j],\n",
      "        [ 5.7378e-01+4.6705e-03j],\n",
      "        [-1.0745e-03-3.8925e-05j],\n",
      "        [-7.6804e-04-2.7823e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5516.7676\n",
      "Average weighted phase: 0.2853325\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.1784e-01-3.9844e-01j],\n",
      "        [-1.2761e-04+2.2241e-04j],\n",
      "        [-8.7702e-05+3.0413e-04j],\n",
      "        [ 5.7695e-01-1.0605e-03j],\n",
      "        [-5.7621e-04-4.2819e-05j],\n",
      "        [-2.0869e-04-5.9373e-06j],\n",
      "        [ 5.7774e-01-2.0693e-03j],\n",
      "        [ 4.1808e-05+1.5290e-06j],\n",
      "        [ 2.9673e-05+1.0852e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5853.856\n",
      "Average weighted phase: 0.28143442\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.3503e-01-3.7982e-01j],\n",
      "        [-1.7175e-04+2.7436e-04j],\n",
      "        [ 3.9336e-04-1.2266e-03j],\n",
      "        [ 5.7852e-01+9.1113e-03j],\n",
      "        [-1.4875e-03-1.5916e-04j],\n",
      "        [-1.2575e-04-5.6027e-06j],\n",
      "        [ 5.7593e-01+4.3029e-03j],\n",
      "        [ 1.2092e-04+5.8495e-06j],\n",
      "        [ 8.6001e-05+4.1603e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 6665.971\n",
      "Average weighted phase: 0.26945388\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.4236e-01-3.7364e-01j],\n",
      "        [ 1.9409e-04-3.0949e-04j],\n",
      "        [-3.2975e-05+1.0193e-04j],\n",
      "        [ 5.7398e-01-1.7281e-03j],\n",
      "        [ 1.4188e-04+1.4088e-05j],\n",
      "        [-1.1561e-03-5.1525e-05j],\n",
      "        [ 5.7902e-01+4.5073e-05j],\n",
      "        [-1.8073e-04-7.6523e-06j],\n",
      "        [-1.2752e-04-5.3990e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5965.3574\n",
      "Average weighted phase: 0.26827568\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.5032e-01-3.6410e-01j],\n",
      "        [-1.6583e-04+2.6058e-04j],\n",
      "        [ 1.6231e-05-4.9193e-05j],\n",
      "        [ 5.7610e-01-6.5788e-04j],\n",
      "        [-7.4171e-04-7.2562e-05j],\n",
      "        [ 1.2239e-03+7.3741e-05j],\n",
      "        [ 5.7684e-01-1.2516e-03j],\n",
      "        [ 4.9264e-04+2.5888e-05j],\n",
      "        [ 3.4814e-04+1.8294e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5909.7354\n",
      "Average weighted phase: 0.25714433\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.6241e-01-3.4735e-01j],\n",
      "        [-3.9470e-04+5.8148e-04j],\n",
      "        [ 4.6590e-05-1.3447e-04j],\n",
      "        [ 5.7643e-01-1.0770e-03j],\n",
      "        [ 5.5313e-04+5.5504e-05j],\n",
      "        [ 3.0497e-04+1.9464e-05j],\n",
      "        [ 5.7726e-01+4.8180e-03j],\n",
      "        [ 3.0398e-04+1.6132e-05j],\n",
      "        [ 2.1287e-04+1.1296e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5673.582\n",
      "Average weighted phase: 0.25091735\n",
      " \n",
      "Accuracy: 0.9983333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.6947e-01-3.3194e-01j],\n",
      "        [ 9.0305e-05-1.2745e-04j],\n",
      "        [ 1.5581e-04-4.4060e-04j],\n",
      "        [ 5.7913e-01+7.5078e-04j],\n",
      "        [-9.0752e-05-9.2520e-06j],\n",
      "        [-5.4491e-05-3.3026e-06j],\n",
      "        [ 5.7794e-01-4.0930e-04j],\n",
      "        [-6.8362e-04-3.8774e-05j],\n",
      "        [-4.7818e-04-2.7122e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5676.3213\n",
      "Average weighted phase: 0.24117701\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.8298e-01-3.1737e-01j],\n",
      "        [ 6.4281e-04-8.5970e-04j],\n",
      "        [ 1.5539e-04-4.0920e-04j],\n",
      "        [ 5.7661e-01+2.7872e-03j],\n",
      "        [ 6.0530e-05+6.6778e-06j],\n",
      "        [-3.5159e-04-2.7308e-05j],\n",
      "        [ 5.7751e-01+1.3502e-03j],\n",
      "        [-2.5127e-04-1.5268e-05j],\n",
      "        [-1.7582e-04-1.0684e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5737.519\n",
      "Average weighted phase: 0.23127665\n",
      " \n",
      "Accuracy: 0.9983333333333333\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.8654e-01-3.0597e-01j],\n",
      "        [-9.5956e-04+1.2532e-03j],\n",
      "        [-7.4080e-05+1.9609e-04j],\n",
      "        [ 5.7933e-01+3.7570e-04j],\n",
      "        [ 5.7217e-04+6.0700e-05j],\n",
      "        [ 7.6995e-05+5.7440e-06j],\n",
      "        [ 5.7795e-01-3.4487e-04j],\n",
      "        [-1.2897e-04-6.0089e-06j],\n",
      "        [-8.9887e-05-4.1878e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 6054.123\n",
      "Average weighted phase: 0.22490504\n",
      " \n",
      "Accuracy: 0.9966666666666667\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 4.9536e-01-2.9606e-01j],\n",
      "        [ 3.1785e-04-4.0561e-04j],\n",
      "        [-1.7436e-04+4.5477e-04j],\n",
      "        [ 5.7782e-01-4.5282e-04j],\n",
      "        [ 5.8051e-04+5.6747e-05j],\n",
      "        [ 9.3177e-05+6.5349e-06j],\n",
      "        [ 5.7714e-01-1.3328e-03j],\n",
      "        [-1.1897e-04-6.7643e-06j],\n",
      "        [-8.2422e-05-4.6861e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5196.0015\n",
      "Average weighted phase: 0.21524605\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.0367e-01-2.7737e-01j],\n",
      "        [ 1.3598e-04-1.6260e-04j],\n",
      "        [-8.2912e-05+2.1011e-04j],\n",
      "        [ 5.8062e-01+6.5125e-04j],\n",
      "        [-1.0234e-03-1.0604e-04j],\n",
      "        [ 1.6285e-04+1.0283e-05j],\n",
      "        [ 5.7642e-01-8.3739e-04j],\n",
      "        [ 1.3945e-04+6.0644e-06j],\n",
      "        [ 9.6602e-05+4.2010e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5897.7163\n",
      "Average weighted phase: 0.2074946\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.1151e-01-2.6438e-01j],\n",
      "        [-4.8560e-04+5.5553e-04j],\n",
      "        [-1.3389e-04+3.4135e-04j],\n",
      "        [ 5.8022e-01+2.9836e-04j],\n",
      "        [-3.5103e-04-3.9176e-05j],\n",
      "        [-2.6558e-05-1.4822e-06j],\n",
      "        [ 5.7601e-01-2.0659e-03j],\n",
      "        [-9.1190e-05-2.2349e-06j],\n",
      "        [-6.3601e-05-1.5588e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5575.9146\n",
      "Average weighted phase: 0.20029448\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.2161e-01-2.5416e-01j],\n",
      "        [-1.8333e-04+2.0425e-04j],\n",
      "        [-3.1651e-04+7.8452e-04j],\n",
      "        [ 5.7532e-01-1.9313e-03j],\n",
      "        [ 1.9664e-04+2.1140e-05j],\n",
      "        [ 8.1087e-04+4.7540e-05j],\n",
      "        [ 5.7648e-01-1.4702e-03j],\n",
      "        [ 4.0042e-05+7.6656e-07j],\n",
      "        [ 2.7631e-05+5.2896e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5948.192\n",
      "Average weighted phase: 0.19086678\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.2687e-01-2.4082e-01j],\n",
      "        [-3.2728e-04+3.5510e-04j],\n",
      "        [ 7.4881e-05-1.7983e-04j],\n",
      "        [ 5.7562e-01+2.3667e-03j],\n",
      "        [ 1.4426e-04+1.6170e-05j],\n",
      "        [ 5.1373e-04+3.3059e-05j],\n",
      "        [ 5.7711e-01+3.7152e-03j],\n",
      "        [-4.3309e-04-5.3829e-06j],\n",
      "        [-2.9730e-04-3.6951e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5437.2295\n",
      "Average weighted phase: 0.18243665\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.3480e-01-2.2430e-01j],\n",
      "        [-2.3823e-04+2.4663e-04j],\n",
      "        [ 1.9594e-04-4.4421e-04j],\n",
      "        [ 5.7560e-01+6.2528e-04j],\n",
      "        [-3.4132e-04-4.0618e-05j],\n",
      "        [-1.7640e-04-1.1744e-05j],\n",
      "        [ 5.7651e-01+3.3186e-04j],\n",
      "        [-3.1078e-04+9.6500e-08j],\n",
      "        [-2.1353e-04+6.6302e-08j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5353.8564\n",
      "Average weighted phase: 0.1735527\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.3772e-01-2.0390e-01j],\n",
      "        [ 5.3466e-04-5.2301e-04j],\n",
      "        [ 1.4260e-05-3.1395e-05j],\n",
      "        [ 5.7992e-01+1.6023e-03j],\n",
      "        [ 3.7694e-04+4.8730e-05j],\n",
      "        [-1.0536e-04-6.2427e-06j],\n",
      "        [ 5.7704e-01-3.2602e-05j],\n",
      "        [ 5.8466e-04+1.2291e-06j],\n",
      "        [ 3.9702e-04+8.3464e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5904.925\n",
      "Average weighted phase: 0.1714874\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.4307e-01-1.9671e-01j],\n",
      "        [-2.4618e-05+2.3879e-05j],\n",
      "        [ 1.7665e-04-3.9495e-04j],\n",
      "        [ 5.7784e-01-2.6972e-03j],\n",
      "        [-3.2601e-04-3.9232e-05j],\n",
      "        [ 2.3650e-04+1.3947e-05j],\n",
      "        [ 5.7660e-01-1.3014e-03j],\n",
      "        [ 7.2354e-05-1.4893e-06j],\n",
      "        [ 4.8925e-05-1.0071e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 6128.639\n",
      "Average weighted phase: 0.16238762\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.4198e-01-1.9656e-01j],\n",
      "        [-1.1600e-06+1.1720e-06j],\n",
      "        [ 6.4139e-05-1.4891e-04j],\n",
      "        [ 5.7584e-01-5.2911e-03j],\n",
      "        [ 1.0797e-04+1.1785e-05j],\n",
      "        [ 7.3945e-04+4.1207e-05j],\n",
      "        [ 5.7965e-01-2.5456e-03j],\n",
      "        [ 2.1478e-04-3.6240e-06j],\n",
      "        [ 1.4436e-04-2.4358e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5798.117\n",
      "Average weighted phase: 0.14597793\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.5111e-01-1.7198e-01j],\n",
      "        [ 1.5457e-03-1.4408e-03j],\n",
      "        [-4.2575e-04+8.9791e-04j],\n",
      "        [ 5.7568e-01+8.3754e-03j],\n",
      "        [-1.5051e-03-2.0573e-04j],\n",
      "        [ 4.2270e-04+2.8840e-05j],\n",
      "        [ 5.7895e-01+5.7950e-03j],\n",
      "        [ 8.7595e-04+3.2073e-06j],\n",
      "        [ 5.9602e-04+2.1823e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5483.5796\n",
      "Average weighted phase: 0.134111\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.5219e-01-1.5726e-01j],\n",
      "        [-1.0856e-03+9.7464e-04j],\n",
      "        [ 1.9486e-04-4.0713e-04j],\n",
      "        [ 5.7812e-01-3.1292e-03j],\n",
      "        [ 8.4107e-05+1.0689e-05j],\n",
      "        [-3.2607e-04-1.8129e-05j],\n",
      "        [ 5.7974e-01-3.8223e-03j],\n",
      "        [ 2.1029e-04-4.1157e-06j],\n",
      "        [ 1.4396e-04-2.8175e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5631.201\n",
      "Average weighted phase: 0.13283543\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.5729e-01-1.4067e-01j],\n",
      "        [-8.3336e-05+7.2040e-05j],\n",
      "        [-1.1805e-04+2.3920e-04j],\n",
      "        [ 5.7787e-01+6.6636e-03j],\n",
      "        [-1.5252e-04-2.2310e-05j],\n",
      "        [-1.5089e-04-8.7550e-06j],\n",
      "        [ 5.7936e-01+1.5364e-04j],\n",
      "        [ 1.2481e-04-7.0470e-07j],\n",
      "        [ 8.5452e-05-4.8249e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 6054.1924\n",
      "Average weighted phase: 0.12576628\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.6598e-01-1.2693e-01j],\n",
      "        [ 3.3157e-04-2.7442e-04j],\n",
      "        [-1.4361e-04+2.8537e-04j],\n",
      "        [ 5.7869e-01-1.0376e-03j],\n",
      "        [ 4.1259e-04+5.7477e-05j],\n",
      "        [-3.8938e-04-2.5226e-05j],\n",
      "        [ 5.7329e-01+3.3812e-03j],\n",
      "        [ 8.4931e-04+1.7479e-06j],\n",
      "        [ 5.7325e-04+1.1798e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5970.5645\n",
      "Average weighted phase: 0.12038611\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.6476e-01-1.0987e-01j],\n",
      "        [-1.6996e-06+1.3671e-06j],\n",
      "        [-1.7896e-04+3.4029e-04j],\n",
      "        [ 5.7628e-01+9.6902e-04j],\n",
      "        [-1.6968e-04-2.4783e-05j],\n",
      "        [-1.1001e-04-6.0744e-06j],\n",
      "        [ 5.8041e-01-1.1566e-03j],\n",
      "        [ 1.6331e-04-3.0553e-06j],\n",
      "        [ 1.1022e-04-2.0621e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5710.55\n",
      "Average weighted phase: 0.1177092\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.7029e-01-9.2384e-02j],\n",
      "        [ 1.8193e-03-1.3860e-03j],\n",
      "        [-3.3079e-04+6.1005e-04j],\n",
      "        [ 5.7950e-01+2.9951e-03j],\n",
      "        [ 4.8236e-04+7.6338e-05j],\n",
      "        [-1.4366e-04-8.5969e-06j],\n",
      "        [ 5.7480e-01+4.2929e-05j],\n",
      "        [-1.6462e-04+1.0099e-06j],\n",
      "        [-1.1134e-04+6.8304e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5975.2773\n",
      "Average weighted phase: 0.1061577\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.7288e-01-7.4639e-02j],\n",
      "        [-1.2285e-03+8.8536e-04j],\n",
      "        [-4.0093e-04+7.1693e-04j],\n",
      "        [ 5.7798e-01-9.9832e-04j],\n",
      "        [ 2.3067e-04+3.6598e-05j],\n",
      "        [ 2.5836e-04+1.7041e-05j],\n",
      "        [ 5.7634e-01+7.2147e-04j],\n",
      "        [-6.5029e-05+4.8822e-07j],\n",
      "        [-4.3870e-05+3.2936e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5968.4585\n",
      "Average weighted phase: 0.091322\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[5.7780e-01-6.1629e-02j],\n",
      "        [2.8316e-04-1.9902e-04j],\n",
      "        [1.0608e-04-1.8781e-04j],\n",
      "        [5.7390e-01+4.4520e-03j],\n",
      "        [4.4516e-04+7.5686e-05j],\n",
      "        [3.4079e-04+2.2032e-05j],\n",
      "        [5.7703e-01-9.0934e-04j],\n",
      "        [3.6006e-04-5.1633e-06j],\n",
      "        [2.4364e-04-3.4938e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5140.627\n",
      "Average weighted phase: 0.09033435\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.7330e-01-5.4032e-02j],\n",
      "        [-1.4066e-04+9.9394e-05j],\n",
      "        [ 1.5584e-04-2.7831e-04j],\n",
      "        [ 5.7867e-01-2.8394e-03j],\n",
      "        [ 1.5542e-04+2.5337e-05j],\n",
      "        [-1.3400e-04-8.6650e-06j],\n",
      "        [ 5.7752e-01+2.9269e-04j],\n",
      "        [-4.6941e-05+9.2848e-07j],\n",
      "        [-3.2140e-05+6.3572e-07j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5807.1846\n",
      "Average weighted phase: 0.08261516\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.7750e-01-3.1810e-02j],\n",
      "        [-7.0733e-04+4.6576e-04j],\n",
      "        [ 4.6991e-06-7.9890e-06j],\n",
      "        [ 5.7676e-01+4.3058e-03j],\n",
      "        [ 4.1486e-04+7.4253e-05j],\n",
      "        [ 2.3100e-05+1.5379e-06j],\n",
      "        [ 5.7690e-01+7.5165e-04j],\n",
      "        [-5.6012e-05+1.5188e-06j],\n",
      "        [-3.8385e-05+1.0408e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 6086.882\n",
      "Average weighted phase: 0.07701973\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.7853e-01-1.1274e-02j],\n",
      "        [ 1.5240e-04-9.4778e-05j],\n",
      "        [ 5.8390e-04-9.3414e-04j],\n",
      "        [ 5.7609e-01+1.8390e-03j],\n",
      "        [-4.1777e-05-7.7758e-06j],\n",
      "        [-4.1291e-04-2.9765e-05j],\n",
      "        [ 5.7730e-01+3.0543e-03j],\n",
      "        [ 3.5237e-04-1.4805e-05j],\n",
      "        [ 2.4298e-04-1.0209e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5447.833\n",
      "Average weighted phase: 0.07330501\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.7613e-01+6.1109e-03j],\n",
      "        [ 5.2376e-04-3.0113e-04j],\n",
      "        [ 5.9734e-04-9.1203e-04j],\n",
      "        [ 5.8331e-01-2.1606e-03j],\n",
      "        [ 6.8914e-05+1.3182e-05j],\n",
      "        [-3.7725e-04-2.6638e-05j],\n",
      "        [ 5.7250e-01-4.4512e-03j],\n",
      "        [-4.1523e-04+2.3174e-05j],\n",
      "        [-2.8986e-04+1.6178e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5171.653\n",
      "Average weighted phase: 0.066413336\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.7451e-01-3.6247e-03j],\n",
      "        [-8.3074e-04+4.8877e-04j],\n",
      "        [ 9.9038e-04-1.4821e-03j],\n",
      "        [ 5.7789e-01-3.9682e-03j],\n",
      "        [ 6.7754e-04+1.2076e-04j],\n",
      "        [-8.0020e-05-6.0120e-06j],\n",
      "        [ 5.7962e-01-3.3708e-04j],\n",
      "        [-1.6545e-04+8.0031e-06j],\n",
      "        [-1.1567e-04+5.5949e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 5454.4155\n",
      "Average weighted phase: 0.06742215\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.8117e-01+2.4379e-03j],\n",
      "        [ 9.8681e-05-5.6692e-05j],\n",
      "        [-2.4977e-04+3.6459e-04j],\n",
      "        [ 5.7693e-01-2.8049e-03j],\n",
      "        [ 2.1336e-04+3.6357e-05j],\n",
      "        [-9.7256e-05-8.0211e-06j],\n",
      "        [ 5.7391e-01-1.9036e-03j],\n",
      "        [-1.0738e-03+5.3905e-05j],\n",
      "        [-7.5152e-04+3.7725e-05j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 6781.705\n",
      "Average weighted phase: 0.058092143\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.7823e-01+2.4862e-03j],\n",
      "        [ 4.0614e-04-2.2487e-04j],\n",
      "        [-4.3156e-04+6.1941e-04j],\n",
      "        [ 5.7522e-01+5.5809e-03j],\n",
      "        [-8.3197e-05-1.4829e-05j],\n",
      "        [-1.5273e-04-1.4492e-05j],\n",
      "        [ 5.7854e-01+5.1774e-03j],\n",
      "        [ 1.0423e-04-5.2157e-06j],\n",
      "        [ 7.2593e-05-3.6326e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 6002.832\n",
      "Average weighted phase: 0.05100102\n",
      " \n",
      "Accuracy: 1.0\n",
      "Amplitudes for percept [0,0,0]: \n",
      "tensor([[ 5.7866e-01+2.0151e-03j],\n",
      "        [ 1.1287e-04-6.1993e-05j],\n",
      "        [-3.0767e-04+4.4761e-04j],\n",
      "        [ 5.7740e-01-1.5160e-03j],\n",
      "        [-2.1859e-04-3.8591e-05j],\n",
      "        [-4.2142e-04-4.0863e-05j],\n",
      "        [ 5.7598e-01+8.4180e-04j],\n",
      "        [ 1.4554e-04-7.5402e-06j],\n",
      "        [ 1.0132e-04-5.2492e-06j]], grad_fn=<MmBackward0>)\n",
      "L1 norm of phases: 6345.007\n",
      "Average weighted phase: 0.059125695\n",
      " \n",
      "Last averaged observable Shannon entropy over 600 samples: 1.098592\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce97e189",
   "metadata": {},
   "source": [
    "Next, we output the middle layer. It is supposed to be an array of 0's and $\\frac{1}{\\sqrt{3}} \\approx 0.577$'s. A 0 represents that an observable does not have a certain value, a large value indicates that it does. The array is arranged as:\n",
    "\n",
    "[values of observable 0, values of observable 1 , ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4c62a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 5.7799e-01+3.5160e-04j],\n",
       "         [ 6.0735e-04-3.3691e-04j],\n",
       "         [-6.6208e-05+9.4470e-05j],\n",
       "         [ 5.7765e-01+8.4154e-05j],\n",
       "         [ 3.1608e-04+5.9912e-05j],\n",
       "         [ 2.8872e-04+2.6627e-05j],\n",
       "         [ 5.7641e-01-1.4631e-03j],\n",
       "         [-1.0572e-04+4.3704e-06j],\n",
       "         [-7.4272e-05+3.0705e-06j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-2.7267e-04+1.9667e-04j],\n",
       "         [ 5.7761e-01-1.7365e-02j],\n",
       "         [-7.2659e-05+2.4042e-04j],\n",
       "         [ 5.7818e-01+4.5458e-04j],\n",
       "         [-7.4669e-04-1.2802e-04j],\n",
       "         [ 6.3694e-05-3.4107e-07j],\n",
       "         [ 5.7598e-01+3.1617e-03j],\n",
       "         [-1.5725e-04+7.0591e-06j],\n",
       "         [-1.7438e-04+7.8277e-06j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-4.6229e-05-9.1598e-05j],\n",
       "         [-5.5044e-05-1.0906e-04j],\n",
       "         [ 5.1462e-01+2.6192e-01j],\n",
       "         [ 5.7724e-01-8.8824e-05j],\n",
       "         [-6.0125e-04+1.9036e-04j],\n",
       "         [-3.6515e-04+1.4377e-05j],\n",
       "         [ 5.7737e-01+2.1807e-03j],\n",
       "         [ 6.6920e-04-5.0187e-05j],\n",
       "         [ 6.6840e-04-5.0127e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.7669e-01-6.6904e-03j],\n",
       "         [ 3.0026e-05-2.1314e-05j],\n",
       "         [-5.9181e-04+1.0263e-03j],\n",
       "         [ 4.1973e-04+6.9102e-05j],\n",
       "         [ 5.7642e-01-4.2425e-04j],\n",
       "         [ 6.8376e-04+9.2641e-05j],\n",
       "         [ 5.7888e-01-5.2367e-03j],\n",
       "         [-6.8356e-04-8.6194e-05j],\n",
       "         [-6.5168e-04-8.2174e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-7.7918e-04+3.3090e-04j],\n",
       "         [ 5.7826e-01-2.1457e-03j],\n",
       "         [ 4.3018e-04-5.3470e-04j],\n",
       "         [-3.7202e-04-8.5051e-05j],\n",
       "         [ 5.7812e-01+5.2446e-04j],\n",
       "         [ 3.8340e-04+2.1397e-05j],\n",
       "         [ 5.7567e-01-5.4405e-04j],\n",
       "         [ 5.4882e-05+6.3357e-06j],\n",
       "         [ 1.2609e-04+1.4556e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-3.1644e-05-2.0357e-04j],\n",
       "         [-4.8034e-05-3.0900e-04j],\n",
       "         [ 4.6589e-01+3.3406e-01j],\n",
       "         [-3.1902e-04+3.3752e-05j],\n",
       "         [ 5.7533e-01+1.4080e-03j],\n",
       "         [ 2.0733e-04-6.5446e-06j],\n",
       "         [ 5.8339e-01-2.6551e-03j],\n",
       "         [ 3.9758e-04-4.5626e-05j],\n",
       "         [ 3.4020e-04-3.9041e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.7682e-01+2.2048e-03j],\n",
       "         [-6.9544e-04+3.6538e-04j],\n",
       "         [-3.8600e-04+5.8312e-04j],\n",
       "         [-5.1233e-04-6.5409e-05j],\n",
       "         [-3.9525e-04-5.0461e-05j],\n",
       "         [ 5.7808e-01+1.2799e-05j],\n",
       "         [ 5.7714e-01+1.2779e-05j],\n",
       "         [ 1.8888e-04-7.2564e-06j],\n",
       "         [ 3.1827e-04-1.2227e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-7.8062e-05+4.5918e-05j],\n",
       "         [ 5.7714e-01+1.0675e-03j],\n",
       "         [ 2.4078e-04-3.3920e-04j],\n",
       "         [-5.8067e-04-1.3592e-04j],\n",
       "         [-4.4253e-04-1.0359e-04j],\n",
       "         [ 5.7751e-01+3.8380e-04j],\n",
       "         [ 5.7741e-01+3.8374e-04j],\n",
       "         [-3.2939e-04-2.6376e-05j],\n",
       "         [-2.4354e-04-1.9502e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 1.6471e-04+2.8674e-04j],\n",
       "         [ 2.8627e-04+4.9833e-04j],\n",
       "         [ 5.3062e-01+2.3124e-01j],\n",
       "         [-2.6163e-04+6.6722e-05j],\n",
       "         [-2.5143e-04+6.4121e-05j],\n",
       "         [ 5.7618e-01-3.5469e-03j],\n",
       "         [ 5.7703e-01-3.5522e-03j],\n",
       "         [ 6.3878e-04-4.9895e-05j],\n",
       "         [ 6.4119e-04-5.0084e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.7892e-01-2.5295e-04j],\n",
       "         [ 2.3568e-04-8.3814e-05j],\n",
       "         [-2.0897e-04+3.1482e-04j],\n",
       "         [ 5.7810e-01-1.5896e-03j],\n",
       "         [ 3.1056e-05+6.9271e-06j],\n",
       "         [-4.4621e-05+5.8896e-07j],\n",
       "         [-2.4740e-05+3.2655e-07j],\n",
       "         [ 5.7502e-01-2.4486e-03j],\n",
       "         [-1.4165e-04+1.9522e-06j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 9.0890e-04-6.8657e-04j],\n",
       "         [ 5.7988e-01-9.3461e-03j],\n",
       "         [-2.3907e-07+6.8896e-07j],\n",
       "         [ 5.7487e-01-1.7244e-04j],\n",
       "         [ 2.9912e-04+3.9519e-05j],\n",
       "         [ 2.6611e-04+1.7983e-05j],\n",
       "         [ 2.3659e-04+1.5988e-05j],\n",
       "         [ 5.7720e-01+2.8698e-03j],\n",
       "         [ 1.3080e-04+3.3563e-06j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-6.0548e-05-8.7226e-05j],\n",
       "         [-5.9726e-05-8.6042e-05j],\n",
       "         [ 5.3149e-01+2.2167e-01j],\n",
       "         [ 5.8001e-01-2.8184e-03j],\n",
       "         [-1.9976e-04+4.7119e-05j],\n",
       "         [ 5.7408e-04-1.4451e-04j],\n",
       "         [ 6.2486e-04-1.5729e-04j],\n",
       "         [ 5.7613e-01-5.3179e-03j],\n",
       "         [-1.6009e-03+1.4208e-04j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.7621e-01-3.9915e-03j],\n",
       "         [-3.1377e-04+1.5311e-04j],\n",
       "         [-6.4698e-04+1.0353e-03j],\n",
       "         [ 1.5364e-04+1.8025e-05j],\n",
       "         [ 5.7760e-01-3.2669e-03j],\n",
       "         [-4.6758e-04-3.2622e-05j],\n",
       "         [-3.8150e-04-2.6616e-05j],\n",
       "         [ 5.7820e-01-2.4723e-03j],\n",
       "         [ 3.4650e-04+2.0907e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 8.3964e-04-5.9206e-04j],\n",
       "         [ 5.7780e-01+1.8738e-03j],\n",
       "         [-6.9060e-06+1.1090e-05j],\n",
       "         [ 5.5369e-04+8.7547e-05j],\n",
       "         [ 5.7489e-01+5.2974e-04j],\n",
       "         [-3.6232e-04-1.9430e-05j],\n",
       "         [-4.2765e-04-2.2933e-05j],\n",
       "         [ 5.7932e-01-4.9359e-03j],\n",
       "         [ 4.8985e-04+1.6763e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-1.7151e-04-3.0605e-04j],\n",
       "         [-1.8674e-04-3.3322e-04j],\n",
       "         [ 5.2274e-01+2.5228e-01j],\n",
       "         [ 2.2467e-04-4.1549e-05j],\n",
       "         [ 5.7752e-01+4.3088e-03j],\n",
       "         [ 2.3210e-05-6.5562e-06j],\n",
       "         [ 2.9052e-05-8.2066e-06j],\n",
       "         [ 5.7406e-01+2.5502e-03j],\n",
       "         [-2.3258e-04+1.9250e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.7638e-01-5.0763e-03j],\n",
       "         [-1.3694e-04+9.3695e-05j],\n",
       "         [ 4.2850e-04-5.7252e-04j],\n",
       "         [-5.9430e-05-6.4817e-06j],\n",
       "         [-6.5280e-05-7.1197e-06j],\n",
       "         [ 5.7425e-01-6.0101e-04j],\n",
       "         [ 2.3398e-04+1.3402e-05j],\n",
       "         [ 5.8138e-01-3.4319e-04j],\n",
       "         [ 2.4950e-04+7.7639e-06j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.9166e-04-2.8778e-04j],\n",
       "         [ 5.7583e-01+1.0603e-03j],\n",
       "         [ 1.6667e-04-2.1950e-04j],\n",
       "         [-1.3339e-04-9.7014e-06j],\n",
       "         [-1.3479e-04-9.8029e-06j],\n",
       "         [ 5.7863e-01+6.1248e-04j],\n",
       "         [ 1.4060e-04+6.0440e-06j],\n",
       "         [ 5.7757e-01-3.2221e-03j],\n",
       "         [ 1.0074e-03+1.8348e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 2.6537e-04+6.6496e-04j],\n",
       "         [ 2.4956e-04+6.2536e-04j],\n",
       "         [ 5.0511e-01+2.8085e-01j],\n",
       "         [-2.2324e-04+2.7685e-05j],\n",
       "         [-2.1415e-04+2.6557e-05j],\n",
       "         [ 5.7735e-01-7.0320e-04j],\n",
       "         [-1.6710e-04+5.6012e-06j],\n",
       "         [ 5.7674e-01-4.4498e-03j],\n",
       "         [-2.9892e-04+1.5910e-05j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.7510e-01+2.7840e-03j],\n",
       "         [-2.6354e-04+1.8269e-04j],\n",
       "         [ 1.1056e-04-1.9100e-04j],\n",
       "         [ 5.7789e-01+9.3658e-04j],\n",
       "         [ 2.0485e-04+2.8381e-05j],\n",
       "         [ 6.6609e-04-2.9833e-05j],\n",
       "         [ 5.8530e-04-2.6215e-05j],\n",
       "         [-1.6360e-04-2.4489e-06j],\n",
       "         [ 5.7904e-01+1.5387e-03j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-4.0251e-04+2.1389e-04j],\n",
       "         [ 5.7703e-01+5.4867e-04j],\n",
       "         [-1.5318e-04+2.3688e-04j],\n",
       "         [ 5.7601e-01-6.0831e-05j],\n",
       "         [-5.4882e-04-7.0637e-05j],\n",
       "         [-4.2147e-05+3.1544e-07j],\n",
       "         [-2.9380e-05+2.1989e-07j],\n",
       "         [-2.4939e-04-2.2010e-06j],\n",
       "         [ 5.7900e-01+7.6998e-04j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.6204e-05+1.0099e-04j],\n",
       "         [ 4.1579e-05+7.4707e-05j],\n",
       "         [ 5.2066e-01+2.5547e-01j],\n",
       "         [ 5.7816e-01-1.2964e-03j],\n",
       "         [-4.0969e-04+7.2171e-05j],\n",
       "         [-9.8540e-04+1.9559e-04j],\n",
       "         [-9.7901e-04+1.9433e-04j],\n",
       "         [ 4.3313e-04-4.4065e-05j],\n",
       "         [ 5.7391e-01-2.0604e-03j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.7644e-01-3.2444e-03j],\n",
       "         [-5.2073e-04+3.4915e-04j],\n",
       "         [ 2.6260e-04-3.3200e-04j],\n",
       "         [-5.4154e-05-1.0141e-05j],\n",
       "         [ 5.7673e-01+1.3960e-03j],\n",
       "         [-1.6950e-04-5.1863e-06j],\n",
       "         [-1.1198e-04-3.4264e-06j],\n",
       "         [ 3.4612e-04+1.6727e-05j],\n",
       "         [ 5.7886e-01+1.2393e-03j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.1767e-04-2.7112e-04j],\n",
       "         [ 5.7660e-01+1.7990e-03j],\n",
       "         [-3.8619e-04+5.3494e-04j],\n",
       "         [-1.5430e-04-3.1182e-05j],\n",
       "         [ 5.7727e-01+5.5056e-03j],\n",
       "         [-3.2887e-04-5.2050e-05j],\n",
       "         [-3.6409e-04-5.7624e-05j],\n",
       "         [-2.3485e-05-2.2847e-06j],\n",
       "         [ 5.7815e-01+1.6264e-03j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-4.1522e-04-5.4972e-04j],\n",
       "         [-7.1147e-04-9.4193e-04j],\n",
       "         [ 5.3613e-01+2.1617e-01j],\n",
       "         [ 3.2681e-04-6.8492e-05j],\n",
       "         [ 5.8033e-01-2.0780e-04j],\n",
       "         [-3.4180e-04+1.5597e-05j],\n",
       "         [-3.4075e-04+1.5549e-05j],\n",
       "         [-8.9425e-04+7.1005e-05j],\n",
       "         [ 5.7363e-01-1.1056e-03j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 5.7832e-01-2.7417e-03j],\n",
       "         [-5.8756e-04+2.7809e-04j],\n",
       "         [-3.3629e-04+5.9052e-04j],\n",
       "         [-1.8867e-04-3.1745e-05j],\n",
       "         [-1.5025e-04-2.5281e-05j],\n",
       "         [ 5.7653e-01+7.6102e-04j],\n",
       "         [ 1.0249e-04+5.3805e-06j],\n",
       "         [-4.4711e-04-2.5190e-05j],\n",
       "         [ 5.7719e-01+4.0648e-04j]], grad_fn=<MmBackward0>),\n",
       " tensor([[ 1.0661e-04-4.1334e-05j],\n",
       "         [ 5.8037e-01+6.7462e-03j],\n",
       "         [-7.7166e-04+1.0955e-03j],\n",
       "         [-3.8874e-04-1.7051e-05j],\n",
       "         [-5.2779e-04-2.3150e-05j],\n",
       "         [ 5.7223e-01-8.1901e-04j],\n",
       "         [ 4.3293e-05+5.0451e-07j],\n",
       "         [ 2.2432e-04-4.9677e-06j],\n",
       "         [ 5.7938e-01-1.7779e-03j]], grad_fn=<MmBackward0>),\n",
       " tensor([[-1.2812e-04-2.6056e-04j],\n",
       "         [-1.4171e-04-2.8821e-04j],\n",
       "         [ 5.0858e-01+2.7135e-01j],\n",
       "         [ 4.8474e-05-4.1469e-06j],\n",
       "         [ 4.8844e-05-4.1786e-06j],\n",
       "         [ 5.7851e-01+1.5522e-03j],\n",
       "         [-9.3411e-06+2.0870e-07j],\n",
       "         [-3.0264e-04+1.1725e-05j],\n",
       "         [ 5.7709e-01+2.4981e-03j]], grad_fn=<MmBackward0>)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TreeAgent.U_percept_bag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e575b85e",
   "metadata": {},
   "source": [
    "Next, we write the function that trains the last layer, while keeping the middle layer fixed. So this is the transfer learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe53ff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainOnExperimentsReplayProcess(experiment_index, Agent = TreeAgent, exp_list = One_Obs_Exps, num_rounds = 10000, replay_size = 100, num_optimizer_steps = 1, learning_rate = 0.01 ):\n",
    "    \n",
    "    exp_index = experiment_index\n",
    "    \n",
    "    # Prepare a file to save the training progress of the experiment\n",
    "    logfile_exps = open(dirNameFigures+\"/logfile_exp_\" +str(exp_index)+\".txt\", \"w\")\n",
    "    logfile_exps.write(\" \\n \")\n",
    "    logfile_exps.close()\n",
    "    \n",
    "    replay_memory_size = replay_size\n",
    "    \n",
    "    # How many experiments are there, and for how many observables does one experiment want to know the values?\n",
    "    num_experiments = len(exp_list)\n",
    "    num_obs_per_exp = (len(exp_list[0])-1)//2\n",
    "    \n",
    "    counter = 0\n",
    "    accuracy = 0.0\n",
    "    weighted_accuracy = 0.0\n",
    "    for_full_accuracy = 0.0\n",
    "    for_full_weighted_accuracy = 0.0\n",
    "    normalization = 0.0\n",
    "    \n",
    "    #Replay memory\n",
    "    reward_memory = []\n",
    "    percept_memory = []\n",
    "    action_memory = []   \n",
    "    \n",
    "    #For evaluating the accuracy\n",
    "    round_list = []\n",
    "    acc_list = []\n",
    "    weighted_list = []\n",
    "    \n",
    "    #For plots\n",
    "    exp_list_for_graph = []\n",
    "    acc_exp_list = []\n",
    "    weighted_exp_list = []\n",
    "\n",
    "\n",
    "    exp_list_for_graph.append(experiment_index)\n",
    "    Agent.reset_task()\n",
    "        \n",
    "    print(exp_list[experiment_index][0])\n",
    "    print(\" \")\n",
    "    logfile_exps = open(dirNameFigures+\"/logfile_exp_\" +str(exp_index)+\".txt\", \"a\")\n",
    "    logfile_exps.write(str(exp_list[experiment_index][0]) + \"\\n \\n \")\n",
    "    logfile_exps.close()\n",
    "        \n",
    "    for roun in range(num_rounds):\n",
    "        counter += 1\n",
    "            \n",
    "        #Randomly sample percept.\n",
    "        percept = []\n",
    "        for observables in range(num_observables):\n",
    "            percept.append(np.random.randint(num_values))\n",
    "        percept_memory.append(percept.copy())    \n",
    "            \n",
    "        # Sample action and reward, also remember whether the answer is yes or no.\n",
    "        predicted_answer = Agent.sample3(percept)\n",
    "        action_memory.append(predicted_answer)\n",
    "        answeredRight, rightAns = rewardLayer3(percept, experiment_index, predicted_answer , experiment_list = exp_list)\n",
    "        r = 0.1*answeredRight\n",
    "            \n",
    "        # A no-answer is much more likely than a yes-answer, since all percept values of the relevant observables have to match the values suggested by the experiment. Therefore, we give the reward for a yes-answer a weight factor such that true yes- and no-answers have the same contribution to the weighted accuracy.\n",
    "        r = r*(((num_values**num_obs_per_exp)-1.0)**rightAns)\n",
    "            \n",
    "        # For the accuracy, remember how many right answers were given. \n",
    "        # We will also calculate a weighted accuracy with the same weight factors as the reward.\n",
    "        # For this weighted accuracy, we also need to store the normalization, which is the maximal value the weighted unnormalized accuracy can achieve if the agent always gives the right answer.\n",
    "        if r > 0.0:\n",
    "            accuracy += 1.0\n",
    "            weighted_accuracy += ((num_values**num_obs_per_exp)-1.0)**rightAns\n",
    "        normalization += ((num_values**num_obs_per_exp)-1.0)**rightAns\n",
    "            \n",
    "        # When we collected enough samples, evaluate the accuracy.\n",
    "        if counter == replay_size:\n",
    "            accuracy /= replay_size\n",
    "            for_full_accuracy = accuracy\n",
    "            weighted_accuracy /= normalization\n",
    "            for_full_weighted_accuracy = weighted_accuracy\n",
    "                \n",
    "            print(\"Accuracy: \"+ str(accuracy))\n",
    "            print(\"Weighted accuracy: \"+ str(weighted_accuracy))\n",
    "            print(\" \")\n",
    "            logfile_exps = open(dirNameFigures+\"/logfile_exp_\" +str(exp_index)+\".txt\", \"a\")\n",
    "            logfile_exps.write(\"Accuracy: \"+ str(accuracy)+\" \\n \")\n",
    "            logfile_exps.write(\"Weighted accuracy: \"+ str(weighted_accuracy)+\" \\n \\n \")\n",
    "            logfile_exps.close()\n",
    "                    \n",
    "            #Save the progress of the training.  \n",
    "            round_list.append(roun)\n",
    "            acc_list.append(accuracy)\n",
    "            weighted_list.append(weighted_accuracy)\n",
    "            \n",
    "            # Resets for the next batch\n",
    "            accuracy = 0.0\n",
    "            weighted_accuracy = 0.0\n",
    "            normalization = 0.0\n",
    "            counter = 0\n",
    "        reward_memory.append(r)\n",
    "\n",
    "            \n",
    "        # When the replay memory is big enough, it's time to train.\n",
    "        # When the agent is already very good, we use a smaller learning rate for fine-tuning.\n",
    "        if len(reward_memory) == replay_memory_size:\n",
    "            if for_full_accuracy >= 0.95:\n",
    "                current_learning_rate = 0.1*learning_rate\n",
    "            else:\n",
    "                current_learning_rate = learning_rate\n",
    "                    \n",
    "            optimizer = optim.Adam([Agent.CPhases_task_bag, Agent.OPhases_task_bag], lr = current_learning_rate)\n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            # Read the probabilities p_{old} to be used for the calculation of the targets.\n",
    "            oldProbs = torch.zeros(size = [replay_memory_size], dtype = torch.float32)\n",
    "            for memory_index in range(replay_memory_size):\n",
    "                current_percept = percept_memory[memory_index].copy()\n",
    "                percept_index = Agent.convert_percept_to_index(current_percept)\n",
    "                probsGuessOld  = Agent.probs3(percept = current_percept).clone().detach()\n",
    "                oldProbs[memory_index] = probsGuessOld[action_memory[memory_index]].clone()\n",
    "               \n",
    "            # Now, the optimizer steps\n",
    "            num_steps = num_optimizer_steps\n",
    "            for step in range(num_steps):\n",
    "                optimizer.zero_grad()\n",
    "                    \n",
    "                # Read the probabilities p_{new} to be tuned\n",
    "                guessProbs = torch.zeros(size = [replay_memory_size] , dtype = torch.float32)\n",
    "                for memory_index in range(replay_memory_size):\n",
    "                    #print(percept_memory)\n",
    "                    current_percept = percept_memory[memory_index].copy()\n",
    "                    percept_index = Agent.convert_percept_to_index(current_percept)\n",
    "                    probsGuessFullPercept  = Agent.probs3(percept = current_percept)\n",
    "                    guessProbs[memory_index] = probsGuessFullPercept[action_memory[memory_index]].clone()\n",
    "\n",
    "                PSloss = MultiPSlossFunction(guessProbs, oldProbs.clone().detach(), reward_memory)\n",
    "                    \n",
    "                fullLoss = 1.0*PSloss \n",
    "                fullLoss.backward()\n",
    "                        \n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            # Wipe replay memory\n",
    "            reward_memory = []\n",
    "            action_memory = []\n",
    "            percept_memory = []\n",
    "                \n",
    "    # Update accuracy lists    \n",
    "    acc_exp_list.append(for_full_accuracy)\n",
    "    weighted_exp_list.append(for_full_weighted_accuracy)\n",
    "        \n",
    "        \n",
    "    # Save the models \n",
    "        \n",
    "    fileName = dirNameModels+\"/AgentExp\"+str(experiment_index)+\".pt\"\n",
    "    torch.save(Agent.state_dict(), fileName)\n",
    "        \n",
    "    fileName = dirNameParameters + \"/TaskCPhasesExp\"+str(experiment_index)+\".txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, Agent.CPhases_task_bag.clone().data.numpy() )\n",
    "            \n",
    "    fileName = dirNameParameters + \"/TaskCPhasesExp\"+str(experiment_index)+\"Rounded.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, Agent.CPhases_task_bag.clone().data.numpy() , fmt = '%0.4f')\n",
    "        \n",
    "    fileName = dirNameParameters + \"/TaskOPhasesExp\"+str(experiment_index)+\".txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, Agent.OPhases_task_bag.clone().data.numpy() )\n",
    "            \n",
    "    fileName = dirNameParameters + \"/TaskOPhasesExp\"+str(experiment_index)+\"Rounded.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, Agent.OPhases_task_bag.clone().data.numpy() , fmt = '%0.4f')\n",
    "            \n",
    "    fileName = dirNameUnitaries + \"/U_task\"+str(experiment_index)+\".txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, Agent.U_task.clone().data.numpy())\n",
    "        \n",
    "    fileName = dirNameUnitaries + \"/U_task\"+str(experiment_index)+\"Rounded.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, Agent.U_task.clone().data.numpy() , fmt = '%0.4f')\n",
    "            \n",
    "    fileName = dirNameUnitaries + \"/U_full_bag\"+str(experiment_index)+\".txt\"\n",
    "    for percept in range(Agent.num_percepts):\n",
    "        if percept == 0:\n",
    "            writeMode = 'w'\n",
    "        else:\n",
    "            writeMode = 'a'\n",
    "        with open(fileName, writeMode) as datafile:\n",
    "            datafile.write(\"Percept index: \"+str(percept)+\" \\n \\n \")\n",
    "            datafile.write(str(Agent.U_full_bag[percept].clone().data.numpy()))\n",
    "            datafile.write(\"\\n \\n \")\n",
    "                \n",
    "        \n",
    "    return(round_list, acc_list, weighted_list, exp_list_for_graph, acc_exp_list, weighted_exp_list)\n",
    "\n",
    "    \n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17382a47-3071-4bbc-ae00-b1ab807ecbb8",
   "metadata": {},
   "source": [
    "The following function takes care of the parallelization of the training of the final layers, collects the data, and draws the figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fa31bcb-a574-4d69-83af-50b2edf83de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndPlotTasks(num_cores = 1, Agent = TreeAgent, exp_list = One_Obs_Exps, num_rounds = 10000, replay_size = 100, num_optimizer_steps = 1, learning_rate = 0.01):\n",
    "    \n",
    "    # Number of final layers to train\n",
    "    num_exps = len(exp_list)\n",
    "    \n",
    "    # Collects data for graphs\n",
    "    exp_list_for_graph = []\n",
    "    acc_exp_list = []\n",
    "    weighted_exp_list = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define a training function with our hyperparameters already configured. \n",
    "    def trainMulti(exp_index):\n",
    "        # Clone the agent with trained middle layer. This prevents the processes for the training of the final layers to overwrite each other.\n",
    "        taskAgent = QPSTreeOneExp(num_percepts = num_values**num_observables, num_hidden = num_values*num_observables, ini = ini_choice)\n",
    "        taskAgent.load_state_dict(Agent.state_dict().copy())\n",
    "    \n",
    "        # Define a function that has all the hyperparameters set right, and only expects the experiment index of the final layer as input.\n",
    "        result = trainOnExperimentsReplayProcess(experiment_index = exp_index, Agent = taskAgent, exp_list = exp_list, num_rounds = num_rounds, replay_size = replay_size, num_optimizer_steps = num_optimizer_steps, learning_rate = learning_rate )\n",
    "        return result\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Parallelize the training of the final layers\n",
    "    full_result_list = Parallel(n_jobs=num_cores)(delayed(trainMulti)(a) for a in range(num_exps))\n",
    "    \n",
    "    for exp, (rl, al, wl, el, ael, wel) in enumerate(full_result_list):\n",
    "        # Extract the data of the training progress for the first experiment\n",
    "        if exp == 0:\n",
    "            round_list = rl\n",
    "            acc_list = al\n",
    "            weighted_list = wl\n",
    "        \n",
    "        # Combine the accuracies of the different experiments to put them all into one plot\n",
    "        exp_list_for_graph.extend(el)\n",
    "        acc_exp_list.extend(ael)\n",
    "        weighted_exp_list.extend(wel)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Save the data used for the graphs\n",
    "    fileName = dirNameFigures + \"/Exp0RoundList.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, np.array(round_list))\n",
    "        \n",
    "    fileName = dirNameFigures + \"/Exp0AccList.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, np.array(acc_list))\n",
    "        \n",
    "    fileName = dirNameFigures + \"/Exp0WeightedAccList.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, np.array(weighted_list))\n",
    "        \n",
    "    fileName = dirNameFigures + \"/ExpListForGraph.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, np.array(exp_list_for_graph))\n",
    "        \n",
    "    fileName = dirNameFigures + \"/AccExpList.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, np.array(acc_exp_list))\n",
    "        \n",
    "    fileName = dirNameFigures + \"/WeightedAccExpList.txt\"\n",
    "    with open(fileName, 'w') as datafile:\n",
    "        np.savetxt(datafile, np.array(weighted_exp_list))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Output and plot accuracies.\n",
    "    fig, ax = plt.subplots(figsize=(7.0, 5.0))\n",
    "    ax.plot(round_list, acc_list, label='accuracy') \n",
    "    ax.plot(round_list, weighted_list, label='weighted_accuracy') \n",
    "    ax.set_xlabel('round')  \n",
    "    ax.set_title(\"Training for one experiment with two observables\")  \n",
    "    ax.legend();  \n",
    "    plt.savefig(dirNameFigures+\"/exp0_training.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    fig2, ax2 = plt.subplots(figsize=(7.0, 5.0))\n",
    "    ax2.plot(exp_list_for_graph, acc_exp_list, label='accuracy')  \n",
    "    ax2.plot(exp_list_for_graph, weighted_exp_list, label='weighted_accuracy')  \n",
    "    ax2.set_xlabel('experiment')  \n",
    "    ax2.set_title(\"Accuracies for experiments with two observables\")  \n",
    "    ax2.legend();  \n",
    "    plt.savefig(dirNameFigures+\"/exps_final_accs.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26894a32-7943-44e0-b277-7a7f9ce8a98a",
   "metadata": {},
   "source": [
    "Now, we can start the parallelized training of all the final layers. The top block contains the hyperparameters of our choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "711ffed3-0627-4ea7-858e-bed76d39dce2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 2 checks whether observable 0 has value 0 and whether observable 1 has value 2\n",
      " \n",
      "Accuracy: 0.406\n",
      "Weighted accuracy: 0.25695216907675195\n",
      " \n",
      "Accuracy: 0.542\n",
      "Weighted accuracy: 0.5760869565217391\n",
      " \n",
      "Accuracy: 0.602\n",
      "Weighted accuracy: 0.6782296650717703\n",
      " \n",
      "Accuracy: 0.66\n",
      "Weighted accuracy: 0.7864077669902912\n",
      " \n",
      "Accuracy: 0.674\n",
      "Weighted accuracy: 0.7985232067510548\n",
      " \n",
      "Accuracy: 0.72\n",
      "Weighted accuracy: 0.8325740318906606\n",
      " \n",
      "Accuracy: 0.8\n",
      "Weighted accuracy: 0.8632768361581921\n",
      " \n",
      "Accuracy: 0.83\n",
      "Weighted accuracy: 0.9089935760171306\n",
      " \n",
      "Accuracy: 0.87\n",
      "Weighted accuracy: 0.9271300448430493\n",
      " \n",
      "Accuracy: 0.91\n",
      "Weighted accuracy: 0.9507119386637459\n",
      " \n",
      "Accuracy: 0.946\n",
      "Weighted accuracy: 0.9666011787819253\n",
      " \n",
      "Accuracy: 0.964\n",
      "Weighted accuracy: 0.9801324503311258\n",
      " \n",
      "Accuracy: 0.958\n",
      "Weighted accuracy: 0.9775160599571735\n",
      " \n",
      "Accuracy: 0.966\n",
      "Weighted accuracy: 0.9783989834815756\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9738041002277904\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9916963226571768\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9889624724061811\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9932203389830508\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9942594718714122\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9925093632958801\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964994165694282\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977924944812362\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943502824858758\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987937273823885\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987623762376238\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9907407407407407\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9908151549942594\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9910313901345291\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977578475336323\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9899888765294772\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997874601487779\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9889624724061811\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979057591623036\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997874601487779\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975874547647768\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987937273823885\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9911699779249448\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978425026968716\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977220956719818\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989969909729187\n",
      " \n",
      "Experiment 6 checks whether observable 0 has value 2 and whether observable 1 has value 0\n",
      " \n",
      "Accuracy: 0.642\n",
      "Weighted accuracy: 0.7666277712952159\n",
      " \n",
      "Accuracy: 0.668\n",
      "Weighted accuracy: 0.7964705882352942\n",
      " \n",
      "Accuracy: 0.726\n",
      "Weighted accuracy: 0.8074679113185531\n",
      " \n",
      "Accuracy: 0.774\n",
      "Weighted accuracy: 0.8403451995685005\n",
      " \n",
      "Accuracy: 0.806\n",
      "Weighted accuracy: 0.8642297650130548\n",
      " \n",
      "Accuracy: 0.824\n",
      "Weighted accuracy: 0.8934977578475336\n",
      " \n",
      "Accuracy: 0.832\n",
      "Weighted accuracy: 0.8874856486796785\n",
      " \n",
      "Accuracy: 0.842\n",
      "Weighted accuracy: 0.8996499416569428\n",
      " \n",
      "Accuracy: 0.88\n",
      "Weighted accuracy: 0.9288256227758007\n",
      " \n",
      "Accuracy: 0.85\n",
      "Weighted accuracy: 0.9237029501525941\n",
      " \n",
      "Accuracy: 0.878\n",
      "Weighted accuracy: 0.9245049504950495\n",
      " \n",
      "Accuracy: 0.824\n",
      "Weighted accuracy: 0.9013452914798207\n",
      " \n",
      "Accuracy: 0.908\n",
      "Weighted accuracy: 0.9545004945598418\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.934878587196468\n",
      " \n",
      "Accuracy: 0.866\n",
      "Weighted accuracy: 0.9183222958057395\n",
      " \n",
      "Accuracy: 0.872\n",
      "Weighted accuracy: 0.9240806642941874\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.9220338983050848\n",
      " \n",
      "Accuracy: 0.894\n",
      "Weighted accuracy: 0.9357601713062098\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9358695652173913\n",
      " \n",
      "Accuracy: 0.902\n",
      "Weighted accuracy: 0.9454949944382648\n",
      " \n",
      "Accuracy: 0.868\n",
      "Weighted accuracy: 0.9229871645274212\n",
      " \n",
      "Accuracy: 0.906\n",
      "Weighted accuracy: 0.949678800856531\n",
      " \n",
      "Accuracy: 0.892\n",
      "Weighted accuracy: 0.9365904365904366\n",
      " \n",
      "Accuracy: 0.9\n",
      "Weighted accuracy: 0.9430523917995444\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9391124871001032\n",
      " \n",
      "Accuracy: 0.914\n",
      "Weighted accuracy: 0.9543039319872476\n",
      " \n",
      "Accuracy: 0.906\n",
      "Weighted accuracy: 0.9464692482915718\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9402173913043478\n",
      " \n",
      "Accuracy: 0.922\n",
      "Weighted accuracy: 0.9606060606060606\n",
      " \n",
      "Accuracy: 0.942\n",
      "Weighted accuracy: 0.9702868852459017\n",
      " \n",
      "Accuracy: 0.914\n",
      "Weighted accuracy: 0.9439461883408071\n",
      " \n",
      "Accuracy: 0.92\n",
      "Weighted accuracy: 0.9513381995133819\n",
      " \n",
      "Accuracy: 0.936\n",
      "Weighted accuracy: 0.9657387580299786\n",
      " \n",
      "Accuracy: 0.966\n",
      "Weighted accuracy: 0.971764705882353\n",
      " \n",
      "Accuracy: 0.96\n",
      "Weighted accuracy: 0.977924944812362\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9819059107358263\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.978776529338327\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9741100323624595\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9710648148148148\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9863325740318907\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9860813704496788\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9701986754966887\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9675925925925926\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9873708381171068\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9866518353726362\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9878587196467992\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9867549668874173\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9525350593311759\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9681093394077449\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9854469854469855\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964114832535885\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975247524752475\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9884816753926702\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9942129629629629\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9941656942823804\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9774011299435028\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9915611814345991\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9822064056939501\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9896049896049897\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967880085653105\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9887295081967213\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9969481180061037\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975874547647768\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9896670493685419\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965277777777778\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9900662251655629\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.99644128113879\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987293519695044\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964705882352941\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9799777530589544\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9891435464414958\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967141292442497\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9894982497082847\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9908883826879271\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979057591623036\n",
      " \n",
      "Experiment 12 checks whether observable 0 has value 1 and whether observable 2 has value 0\n",
      " \n",
      "Accuracy: 0.508\n",
      "Weighted accuracy: 0.5044150110375276\n",
      " \n",
      "Accuracy: 0.594\n",
      "Weighted accuracy: 0.6429418742586003\n",
      " \n",
      "Accuracy: 0.614\n",
      "Weighted accuracy: 0.7118055555555556\n",
      " \n",
      "Accuracy: 0.692\n",
      "Weighted accuracy: 0.7559760956175299\n",
      " \n",
      "Accuracy: 0.682\n",
      "Weighted accuracy: 0.7549668874172185\n",
      " \n",
      "Accuracy: 0.724\n",
      "Weighted accuracy: 0.7997775305895439\n",
      " \n",
      "Accuracy: 0.72\n",
      "Weighted accuracy: 0.8145695364238411\n",
      " \n",
      "Accuracy: 0.732\n",
      "Weighted accuracy: 0.8105381165919282\n",
      " \n",
      "Accuracy: 0.762\n",
      "Weighted accuracy: 0.8145695364238411\n",
      " \n",
      "Accuracy: 0.756\n",
      "Weighted accuracy: 0.8257173219978746\n",
      " \n",
      "Accuracy: 0.74\n",
      "Weighted accuracy: 0.8304455445544554\n",
      " \n",
      "Accuracy: 0.76\n",
      "Weighted accuracy: 0.8265524625267666\n",
      " \n",
      "Accuracy: 0.798\n",
      "Weighted accuracy: 0.8700564971751412\n",
      " \n",
      "Accuracy: 0.818\n",
      "Weighted accuracy: 0.8858695652173914\n",
      " \n",
      "Accuracy: 0.804\n",
      "Weighted accuracy: 0.8813559322033898\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.8958333333333334\n",
      " \n",
      "Accuracy: 0.84\n",
      "Weighted accuracy: 0.8932515337423312\n",
      " \n",
      "Accuracy: 0.886\n",
      "Weighted accuracy: 0.91353001017294\n",
      " \n",
      "Accuracy: 0.906\n",
      "Weighted accuracy: 0.941747572815534\n",
      " \n",
      "Accuracy: 0.898\n",
      "Weighted accuracy: 0.9114349775784754\n",
      " \n",
      "Accuracy: 0.934\n",
      "Weighted accuracy: 0.9591584158415841\n",
      " \n",
      "Accuracy: 0.946\n",
      "Weighted accuracy: 0.9383408071748879\n",
      " \n",
      "Accuracy: 0.95\n",
      "Weighted accuracy: 0.9641255605381166\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9724061810154525\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9605695509309967\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9919632606199771\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9883570504527813\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9706214689265537\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9944812362030905\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9952941176470588\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975874547647768\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9969481180061037\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967880085653105\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964114832535885\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9913700107874865\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987405541561712\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975460122699387\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976275207591934\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9969262295081968\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9895833333333334\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977924944812362\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977924944812362\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9974587039390089\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Experiment 18 checks whether observable 1 has value 0 and whether observable 2 has value 0\n",
      " \n",
      "Accuracy: 0.598\n",
      "Weighted accuracy: 0.739514348785872\n",
      " \n",
      "Accuracy: 0.648\n",
      "Weighted accuracy: 0.7912218268090154\n",
      " \n",
      "Accuracy: 0.722\n",
      "Weighted accuracy: 0.8388520971302428\n",
      " \n",
      "Accuracy: 0.792\n",
      "Weighted accuracy: 0.8565217391304348\n",
      " \n",
      "Accuracy: 0.78\n",
      "Weighted accuracy: 0.8495981630309989\n",
      " \n",
      "Accuracy: 0.79\n",
      "Weighted accuracy: 0.8637469586374696\n",
      " \n",
      "Accuracy: 0.83\n",
      "Weighted accuracy: 0.882091212458287\n",
      " \n",
      "Accuracy: 0.846\n",
      "Weighted accuracy: 0.9086595492289442\n",
      " \n",
      "Accuracy: 0.85\n",
      "Weighted accuracy: 0.8861209964412812\n",
      " \n",
      "Accuracy: 0.832\n",
      "Weighted accuracy: 0.9003051881993896\n",
      " \n",
      "Accuracy: 0.822\n",
      "Weighted accuracy: 0.9017660044150111\n",
      " \n",
      "Accuracy: 0.858\n",
      "Weighted accuracy: 0.9222343921139102\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.9232480533926585\n",
      " \n",
      "Accuracy: 0.838\n",
      "Weighted accuracy: 0.9098998887652948\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9210526315789473\n",
      " \n",
      "Accuracy: 0.884\n",
      "Weighted accuracy: 0.9328703703703703\n",
      " \n",
      "Accuracy: 0.886\n",
      "Weighted accuracy: 0.9375684556407448\n",
      " \n",
      "Accuracy: 0.91\n",
      "Weighted accuracy: 0.9457177322074789\n",
      " \n",
      "Accuracy: 0.922\n",
      "Weighted accuracy: 0.9548611111111112\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9836272040302267\n",
      " \n",
      "Accuracy: 0.964\n",
      "Weighted accuracy: 0.9789964994165694\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9911699779249448\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9915611814345991\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9930555555555556\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9939686369119421\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998989898989899\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978586723768736\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989528795811519\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9917440660474717\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9990176817288802\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975874547647768\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989969909729187\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.991304347826087\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976851851851852\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9828693790149893\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9872935196950444\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978260869565218\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9914984059511158\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.991304347826087\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9912376779846659\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987937273823885\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987730061349693\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998989898989899\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9901840490797545\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/krumm/opt/anaconda3/envs/QPS/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 0 checks whether observable 0 has value 0 and whether observable 1 has value 0\n",
      " \n",
      "Accuracy: 0.63\n",
      "Weighted accuracy: 0.7715269804822044\n",
      " \n",
      "Accuracy: 0.706\n",
      "Weighted accuracy: 0.8195067264573991\n",
      " \n",
      "Accuracy: 0.724\n",
      "Weighted accuracy: 0.838973162193699\n",
      " \n",
      "Accuracy: 0.752\n",
      "Weighted accuracy: 0.866235167206041\n",
      " \n",
      "Accuracy: 0.802\n",
      "Weighted accuracy: 0.8890134529147982\n",
      " \n",
      "Accuracy: 0.842\n",
      "Weighted accuracy: 0.9078179696616102\n",
      " \n",
      "Accuracy: 0.846\n",
      "Weighted accuracy: 0.9018338727076591\n",
      " \n",
      "Accuracy: 0.864\n",
      "Weighted accuracy: 0.9219288174512055\n",
      " \n",
      "Accuracy: 0.83\n",
      "Weighted accuracy: 0.907608695652174\n",
      " \n",
      "Accuracy: 0.822\n",
      "Weighted accuracy: 0.8935406698564593\n",
      " \n",
      "Accuracy: 0.85\n",
      "Weighted accuracy: 0.923155737704918\n",
      " \n",
      "Accuracy: 0.828\n",
      "Weighted accuracy: 0.8949152542372881\n",
      " \n",
      "Accuracy: 0.868\n",
      "Weighted accuracy: 0.930379746835443\n",
      " \n",
      "Accuracy: 0.866\n",
      "Weighted accuracy: 0.9254727474972191\n",
      " \n",
      "Accuracy: 0.868\n",
      "Weighted accuracy: 0.9260089686098655\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.9232480533926585\n",
      " \n",
      "Accuracy: 0.86\n",
      "Weighted accuracy: 0.92152466367713\n",
      " \n",
      "Accuracy: 0.854\n",
      "Weighted accuracy: 0.9218415417558886\n",
      " \n",
      "Accuracy: 0.864\n",
      "Weighted accuracy: 0.9172749391727494\n",
      " \n",
      "Accuracy: 0.898\n",
      "Weighted accuracy: 0.9453961456102784\n",
      " \n",
      "Accuracy: 0.852\n",
      "Weighted accuracy: 0.9136522753792299\n",
      " \n",
      "Accuracy: 0.876\n",
      "Weighted accuracy: 0.9276546091015169\n",
      " \n",
      "Accuracy: 0.854\n",
      "Weighted accuracy: 0.9181614349775785\n",
      " \n",
      "Accuracy: 0.886\n",
      "Weighted accuracy: 0.9314775160599572\n",
      " \n",
      "Accuracy: 0.908\n",
      "Weighted accuracy: 0.9492273730684326\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9264531435349941\n",
      " \n",
      "Accuracy: 0.888\n",
      "Weighted accuracy: 0.9367231638418079\n",
      " \n",
      "Accuracy: 0.888\n",
      "Weighted accuracy: 0.9297893681043129\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9352941176470588\n",
      " \n",
      "Accuracy: 0.908\n",
      "Weighted accuracy: 0.9467592592592593\n",
      " \n",
      "Accuracy: 0.924\n",
      "Weighted accuracy: 0.9560185185185185\n",
      " \n",
      "Accuracy: 0.942\n",
      "Weighted accuracy: 0.9684782608695652\n",
      " \n",
      "Accuracy: 0.954\n",
      "Weighted accuracy: 0.975557917109458\n",
      " \n",
      "Accuracy: 0.938\n",
      "Weighted accuracy: 0.963226571767497\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9851222104144527\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943946188340808\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977924944812362\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9947257383966245\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9929411764705882\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979508196721312\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.996662958843159\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9907407407407407\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.991304347826087\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9894117647058823\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9911699779249448\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9912376779846659\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987623762376238\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9911012235817576\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9908151549942594\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976662777129521\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9824753559693319\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.991304347826087\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9919191919191919\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 10 checks whether observable 0 has value 0 and whether observable 2 has value 1\n",
      " \n",
      "Accuracy: 0.536\n",
      "Weighted accuracy: 0.5192719486081371\n",
      " \n",
      "Accuracy: 0.632\n",
      "Weighted accuracy: 0.6655879180151025\n",
      " \n",
      "Accuracy: 0.562\n",
      "Weighted accuracy: 0.6367713004484304\n",
      " \n",
      "Accuracy: 0.624\n",
      "Weighted accuracy: 0.7225572979493365\n",
      " \n",
      "Accuracy: 0.652\n",
      "Weighted accuracy: 0.7005649717514124\n",
      " \n",
      "Accuracy: 0.742\n",
      "Weighted accuracy: 0.7708565072302559\n",
      " \n",
      "Accuracy: 0.744\n",
      "Weighted accuracy: 0.7984665936473165\n",
      " \n",
      "Accuracy: 0.81\n",
      "Weighted accuracy: 0.8422782037239869\n",
      " \n",
      "Accuracy: 0.776\n",
      "Weighted accuracy: 0.8113570741097209\n",
      " \n",
      "Accuracy: 0.82\n",
      "Weighted accuracy: 0.8745476477683957\n",
      " \n",
      "Accuracy: 0.824\n",
      "Weighted accuracy: 0.8358695652173913\n",
      " \n",
      "Accuracy: 0.828\n",
      "Weighted accuracy: 0.8494117647058823\n",
      " \n",
      "Accuracy: 0.824\n",
      "Weighted accuracy: 0.8319719953325554\n",
      " \n",
      "Accuracy: 0.854\n",
      "Weighted accuracy: 0.8331562167906482\n",
      " \n",
      "Accuracy: 0.894\n",
      "Weighted accuracy: 0.9119565217391304\n",
      " \n",
      "Accuracy: 0.898\n",
      "Weighted accuracy: 0.8543689320388349\n",
      " \n",
      "Accuracy: 0.912\n",
      "Weighted accuracy: 0.8870056497175142\n",
      " \n",
      "Accuracy: 0.926\n",
      "Weighted accuracy: 0.892933618843683\n",
      " \n",
      "Accuracy: 0.964\n",
      "Weighted accuracy: 0.9623529411764706\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9630044843049327\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9366963402571711\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9543937708565072\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9831121833534379\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.926673751328374\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9638118214716526\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.96440489432703\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9454949944382648\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9607623318385651\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9717391304347827\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.977924944812362\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9598163030998852\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9819341126461212\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9667049368541906\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9673278879813302\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9863325740318907\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9864406779661017\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9892125134843581\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9890470974808324\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9627601314348302\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9804822043628014\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9701986754966887\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9675925925925926\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.972972972972973\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.98\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9886104783599089\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987515605493134\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998960498960499\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978425026968716\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976470588235294\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998960498960499\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 16 checks whether observable 0 has value 2 and whether observable 2 has value 1\n",
      " \n",
      "Accuracy: 0.548\n",
      "Weighted accuracy: 0.5402567094515752\n",
      " \n",
      "Accuracy: 0.59\n",
      "Weighted accuracy: 0.6289237668161435\n",
      " \n",
      "Accuracy: 0.58\n",
      "Weighted accuracy: 0.6261780104712041\n",
      " \n",
      "Accuracy: 0.642\n",
      "Weighted accuracy: 0.696604600219058\n",
      " \n",
      "Accuracy: 0.652\n",
      "Weighted accuracy: 0.728587319243604\n",
      " \n",
      "Accuracy: 0.684\n",
      "Weighted accuracy: 0.7715231788079471\n",
      " \n",
      "Accuracy: 0.722\n",
      "Weighted accuracy: 0.7728260869565218\n",
      " \n",
      "Accuracy: 0.712\n",
      "Weighted accuracy: 0.775\n",
      " \n",
      "Accuracy: 0.732\n",
      "Weighted accuracy: 0.8211920529801324\n",
      " \n",
      "Accuracy: 0.754\n",
      "Weighted accuracy: 0.8333333333333334\n",
      " \n",
      "Accuracy: 0.746\n",
      "Weighted accuracy: 0.8199513381995134\n",
      " \n",
      "Accuracy: 0.79\n",
      "Weighted accuracy: 0.8607329842931937\n",
      " \n",
      "Accuracy: 0.784\n",
      "Weighted accuracy: 0.8532901833872708\n",
      " \n",
      "Accuracy: 0.776\n",
      "Weighted accuracy: 0.8630434782608696\n",
      " \n",
      "Accuracy: 0.836\n",
      "Weighted accuracy: 0.8993865030674847\n",
      " \n",
      "Accuracy: 0.85\n",
      "Weighted accuracy: 0.912485414235706\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9338565022421524\n",
      " \n",
      "Accuracy: 0.896\n",
      "Weighted accuracy: 0.9407744874715261\n",
      " \n",
      "Accuracy: 0.966\n",
      "Weighted accuracy: 0.9796650717703349\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9840595111583422\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9764851485148515\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9852760736196319\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9877641824249166\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9872685185185185\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9907407407407407\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9904357066950054\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9932203389830508\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9864406779661017\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968354430379747\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9940688018979834\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968119022316685\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9851936218678815\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998960498960499\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9957173447537473\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975874547647768\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964705882352941\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9896670493685419\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975460122699387\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9882352941176471\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964114832535885\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967141292442497\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987515605493134\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965556831228473\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9894982497082847\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977924944812362\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987834549878345\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9949622166246851\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976851851851852\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987515605493134\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979939819458375\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967880085653105\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997874601487779\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976851851851852\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9911699779249448\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 22 checks whether observable 1 has value 1 and whether observable 2 has value 1\n",
      " \n",
      "Accuracy: 0.532\n",
      "Weighted accuracy: 0.4717391304347826\n",
      " \n",
      "Accuracy: 0.576\n",
      "Weighted accuracy: 0.7619565217391304\n",
      " \n",
      "Accuracy: 0.648\n",
      "Weighted accuracy: 0.7623642943305187\n",
      " \n",
      "Accuracy: 0.698\n",
      "Weighted accuracy: 0.8320355951056729\n",
      " \n",
      "Accuracy: 0.744\n",
      "Weighted accuracy: 0.8532608695652174\n",
      " \n",
      "Accuracy: 0.746\n",
      "Weighted accuracy: 0.8598233995584988\n",
      " \n",
      "Accuracy: 0.762\n",
      "Weighted accuracy: 0.8514357053682896\n",
      " \n",
      "Accuracy: 0.796\n",
      "Weighted accuracy: 0.8779904306220095\n",
      " \n",
      "Accuracy: 0.832\n",
      "Weighted accuracy: 0.8986731001206273\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9347568208778173\n",
      " \n",
      "Accuracy: 0.908\n",
      "Weighted accuracy: 0.9463243873978997\n",
      " \n",
      "Accuracy: 0.94\n",
      "Weighted accuracy: 0.9694811800610377\n",
      " \n",
      "Accuracy: 0.942\n",
      "Weighted accuracy: 0.9702868852459017\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9824753559693319\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9661016949152542\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9790286975717439\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9742331288343559\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9933259176863182\n",
      " \n",
      "Accuracy: 0.952\n",
      "Weighted accuracy: 0.971049457177322\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9906651108518086\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9847058823529412\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.981630309988519\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9635974304068522\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9863874345549738\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9906651108518086\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9783599088838268\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9807280513918629\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9793340987370838\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9901423877327492\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9908151549942594\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954075774971297\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9877641824249166\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9921414538310412\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9846659364731654\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9879518072289156\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966367713004485\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9913700107874865\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9917440660474717\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9936238044633369\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9963503649635036\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9891304347826086\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9863861386138614\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9894982497082847\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9899244332493703\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9905882352941177\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.992822966507177\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9909604519774011\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9924487594390508\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9920273348519362\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9918616480162767\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9867549668874173\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9949494949494949\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9866803278688525\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9934782608695653\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9944382647385984\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9920273348519362\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9955849889624724\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9936708860759493\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9937027707808564\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9932885906040269\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976662777129521\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9899244332493703\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965556831228473\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966101694915255\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965831435079726\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9942129629629629\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.995049504950495\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943052391799544\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966367713004485\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954441913439636\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9941176470588236\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967391304347826\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965556831228473\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9925053533190579\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9947257383966245\n",
      " \n",
      "Experiment 4 checks whether observable 0 has value 1 and whether observable 1 has value 1\n",
      " \n",
      "Accuracy: 0.472\n",
      "Weighted accuracy: 0.39183222958057395\n",
      " \n",
      "Accuracy: 0.54\n",
      "Weighted accuracy: 0.638118214716526\n",
      " \n",
      "Accuracy: 0.62\n",
      "Weighted accuracy: 0.7278401997503121\n",
      " \n",
      "Accuracy: 0.654\n",
      "Weighted accuracy: 0.7903587443946188\n",
      " \n",
      "Accuracy: 0.694\n",
      "Weighted accuracy: 0.8002207505518764\n",
      " \n",
      "Accuracy: 0.736\n",
      "Weighted accuracy: 0.8073047858942065\n",
      " \n",
      "Accuracy: 0.748\n",
      "Weighted accuracy: 0.8211678832116789\n",
      " \n",
      "Accuracy: 0.776\n",
      "Weighted accuracy: 0.845398773006135\n",
      " \n",
      "Accuracy: 0.816\n",
      "Weighted accuracy: 0.9007021063189569\n",
      " \n",
      "Accuracy: 0.808\n",
      "Weighted accuracy: 0.8851674641148325\n",
      " \n",
      "Accuracy: 0.828\n",
      "Weighted accuracy: 0.8935643564356436\n",
      " \n",
      "Accuracy: 0.834\n",
      "Weighted accuracy: 0.8954659949622166\n",
      " \n",
      "Accuracy: 0.842\n",
      "Weighted accuracy: 0.9107344632768362\n",
      " \n",
      "Accuracy: 0.852\n",
      "Weighted accuracy: 0.9201725997842503\n",
      " \n",
      "Accuracy: 0.852\n",
      "Weighted accuracy: 0.9163841807909604\n",
      " \n",
      "Accuracy: 0.91\n",
      "Weighted accuracy: 0.9466192170818505\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9326086956521739\n",
      " \n",
      "Accuracy: 0.928\n",
      "Weighted accuracy: 0.9480249480249481\n",
      " \n",
      "Accuracy: 0.898\n",
      "Weighted accuracy: 0.9423728813559322\n",
      " \n",
      "Accuracy: 0.926\n",
      "Weighted accuracy: 0.9585201793721974\n",
      " \n",
      "Accuracy: 0.946\n",
      "Weighted accuracy: 0.9697309417040358\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9833147942157954\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9877641824249166\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9894982497082847\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9901423877327492\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9941656942823804\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9871414441147379\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9861111111111112\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9810126582278481\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975874547647768\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9887892376681614\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9877300613496932\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979057591623036\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967391304347826\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9892125134843581\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9880382775119617\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9905882352941177\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987834549878345\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966101694915255\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.990506329113924\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987834549878345\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978425026968716\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9955506117908788\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989528795811519\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9911699779249448\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968586387434555\n",
      " \n",
      "Experiment 11 checks whether observable 0 has value 0 and whether observable 2 has value 2\n",
      " \n",
      "Accuracy: 0.526\n",
      "Weighted accuracy: 0.4988425925925926\n",
      " \n",
      "Accuracy: 0.56\n",
      "Weighted accuracy: 0.60734149054505\n",
      " \n",
      "Accuracy: 0.584\n",
      "Weighted accuracy: 0.6094929881337648\n",
      " \n",
      "Accuracy: 0.64\n",
      "Weighted accuracy: 0.685640362225097\n",
      " \n",
      "Accuracy: 0.678\n",
      "Weighted accuracy: 0.7119021134593994\n",
      " \n",
      "Accuracy: 0.758\n",
      "Weighted accuracy: 0.7981888745148771\n",
      " \n",
      "Accuracy: 0.77\n",
      "Weighted accuracy: 0.8532901833872708\n",
      " \n",
      "Accuracy: 0.796\n",
      "Weighted accuracy: 0.8790035587188612\n",
      " \n",
      "Accuracy: 0.852\n",
      "Weighted accuracy: 0.9039145907473309\n",
      " \n",
      "Accuracy: 0.81\n",
      "Weighted accuracy: 0.888235294117647\n",
      " \n",
      "Accuracy: 0.85\n",
      "Weighted accuracy: 0.9074844074844075\n",
      " \n",
      "Accuracy: 0.854\n",
      "Weighted accuracy: 0.8946188340807175\n",
      " \n",
      "Accuracy: 0.876\n",
      "Weighted accuracy: 0.912743972445465\n",
      " \n",
      "Accuracy: 0.9\n",
      "Weighted accuracy: 0.9265212399540758\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9180327868852459\n",
      " \n",
      "Accuracy: 0.872\n",
      "Weighted accuracy: 0.908235294117647\n",
      " \n",
      "Accuracy: 0.88\n",
      "Weighted accuracy: 0.9151832460732985\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9265850945494994\n",
      " \n",
      "Accuracy: 0.888\n",
      "Weighted accuracy: 0.9346557759626605\n",
      " \n",
      "Accuracy: 0.896\n",
      "Weighted accuracy: 0.9430449069003286\n",
      " \n",
      "Accuracy: 0.898\n",
      "Weighted accuracy: 0.9419134396355353\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9238410596026491\n",
      " \n",
      "Accuracy: 0.892\n",
      "Weighted accuracy: 0.9349904397705545\n",
      " \n",
      "Accuracy: 0.876\n",
      "Weighted accuracy: 0.9336188436830836\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9293361884368309\n",
      " \n",
      "Accuracy: 0.892\n",
      "Weighted accuracy: 0.9380022962112514\n",
      " \n",
      "Accuracy: 0.902\n",
      "Weighted accuracy: 0.9423529411764706\n",
      " \n",
      "Accuracy: 0.916\n",
      "Weighted accuracy: 0.9525423728813559\n",
      " \n",
      "Accuracy: 0.898\n",
      "Weighted accuracy: 0.9432703003337041\n",
      " \n",
      "Accuracy: 0.894\n",
      "Weighted accuracy: 0.9381563593932322\n",
      " \n",
      "Accuracy: 0.894\n",
      "Weighted accuracy: 0.9357601713062098\n",
      " \n",
      "Accuracy: 0.874\n",
      "Weighted accuracy: 0.9011764705882352\n",
      " \n",
      "Accuracy: 0.902\n",
      "Weighted accuracy: 0.9467391304347826\n",
      " \n",
      "Accuracy: 0.916\n",
      "Weighted accuracy: 0.9563409563409564\n",
      " \n",
      "Accuracy: 0.91\n",
      "Weighted accuracy: 0.9412532637075718\n",
      " \n",
      "Accuracy: 0.94\n",
      "Weighted accuracy: 0.9506726457399103\n",
      " \n",
      "Accuracy: 0.954\n",
      "Weighted accuracy: 0.9722557297949337\n",
      " \n",
      "Accuracy: 0.948\n",
      "Weighted accuracy: 0.9686369119420989\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9940688018979834\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9913700107874865\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9935316946959897\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9939172749391727\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9929411764705882\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975460122699387\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965556831228473\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967391304347826\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9961190168175937\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965277777777778\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9969262295081968\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989528795811519\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965556831228473\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977220956719818\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987405541561712\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9880382775119617\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987937273823885\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978586723768736\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977753058954394\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9956521739130435\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9911699779249448\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977220956719818\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Experiment 17 checks whether observable 0 has value 2 and whether observable 2 has value 2\n",
      " \n",
      "Accuracy: 0.546\n",
      "Weighted accuracy: 0.5663430420711975\n",
      " \n",
      "Accuracy: 0.622\n",
      "Weighted accuracy: 0.6782511210762332\n",
      " \n",
      "Accuracy: 0.63\n",
      "Weighted accuracy: 0.7130977130977131\n",
      " \n",
      "Accuracy: 0.678\n",
      "Weighted accuracy: 0.7744282744282744\n",
      " \n",
      "Accuracy: 0.696\n",
      "Weighted accuracy: 0.7790432801822323\n",
      " \n",
      "Accuracy: 0.748\n",
      "Weighted accuracy: 0.8105882352941176\n",
      " \n",
      "Accuracy: 0.732\n",
      "Weighted accuracy: 0.7829181494661922\n",
      " \n",
      "Accuracy: 0.778\n",
      "Weighted accuracy: 0.8363228699551569\n",
      " \n",
      "Accuracy: 0.84\n",
      "Weighted accuracy: 0.8679678530424799\n",
      " \n",
      "Accuracy: 0.822\n",
      "Weighted accuracy: 0.8718510405257394\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9242250287026407\n",
      " \n",
      "Accuracy: 0.888\n",
      "Weighted accuracy: 0.8938156359393232\n",
      " \n",
      "Accuracy: 0.908\n",
      "Weighted accuracy: 0.9107358262967431\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.8878787878787879\n",
      " \n",
      "Accuracy: 0.896\n",
      "Weighted accuracy: 0.9383155397390273\n",
      " \n",
      "Accuracy: 0.904\n",
      "Weighted accuracy: 0.932092004381161\n",
      " \n",
      "Accuracy: 0.922\n",
      "Weighted accuracy: 0.939150401836969\n",
      " \n",
      "Accuracy: 0.896\n",
      "Weighted accuracy: 0.8993055555555556\n",
      " \n",
      "Accuracy: 0.912\n",
      "Weighted accuracy: 0.9478054567022538\n",
      " \n",
      "Accuracy: 0.906\n",
      "Weighted accuracy: 0.9442467378410438\n",
      " \n",
      "Accuracy: 0.936\n",
      "Weighted accuracy: 0.953204476093591\n",
      " \n",
      "Accuracy: 0.932\n",
      "Weighted accuracy: 0.9627601314348302\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9884816753926702\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9856396866840731\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9889624724061811\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968586387434555\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9901840490797545\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943052391799544\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9963503649635036\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978902953586498\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987405541561712\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977924944812362\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.99644128113879\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998960498960499\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964114832535885\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978425026968716\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9914984059511158\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977220956719818\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997364953886693\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976470588235294\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987730061349693\n",
      " \n",
      "Experiment 23 checks whether observable 1 has value 1 and whether observable 2 has value 2\n",
      " \n",
      "Accuracy: 0.56\n",
      "Weighted accuracy: 0.5450121654501217\n",
      " \n",
      "Accuracy: 0.59\n",
      "Weighted accuracy: 0.7679282868525896\n",
      " \n",
      "Accuracy: 0.624\n",
      "Weighted accuracy: 0.7875706214689265\n",
      " \n",
      "Accuracy: 0.67\n",
      "Weighted accuracy: 0.8026315789473685\n",
      " \n",
      "Accuracy: 0.728\n",
      "Weighted accuracy: 0.8451025056947609\n",
      " \n",
      "Accuracy: 0.75\n",
      "Weighted accuracy: 0.8425692695214105\n",
      " \n",
      "Accuracy: 0.762\n",
      "Weighted accuracy: 0.8706521739130435\n",
      " \n",
      "Accuracy: 0.792\n",
      "Weighted accuracy: 0.8878101402373247\n",
      " \n",
      "Accuracy: 0.822\n",
      "Weighted accuracy: 0.9010011123470523\n",
      " \n",
      "Accuracy: 0.838\n",
      "Weighted accuracy: 0.9119565217391304\n",
      " \n",
      "Accuracy: 0.856\n",
      "Weighted accuracy: 0.9166666666666666\n",
      " \n",
      "Accuracy: 0.876\n",
      "Weighted accuracy: 0.9341126461211477\n",
      " \n",
      "Accuracy: 0.876\n",
      "Weighted accuracy: 0.9264531435349941\n",
      " \n",
      "Accuracy: 0.918\n",
      "Weighted accuracy: 0.9505428226779252\n",
      " \n",
      "Accuracy: 0.95\n",
      "Weighted accuracy: 0.970344009489917\n",
      " \n",
      "Accuracy: 0.956\n",
      "Weighted accuracy: 0.9749430523917996\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9795037756202805\n",
      " \n",
      "Accuracy: 0.966\n",
      "Weighted accuracy: 0.9820675105485233\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9879518072289156\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9778296382730455\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.984304932735426\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9844271412680756\n",
      " \n",
      "Accuracy: 0.964\n",
      "Weighted accuracy: 0.9721913236929922\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9823529411764705\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9912376779846659\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943052391799544\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.979957805907173\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9918981481481481\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9922135706340378\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9931662870159453\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9956850053937433\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9944812362030905\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9958720330237358\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943502824858758\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967880085653105\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943052391799544\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987515605493134\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943052391799544\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9940191387559809\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964994165694282\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9944382647385984\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9946062567421791\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976851851851852\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987834549878345\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976851851851852\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976662777129521\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975874547647768\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9957492029755579\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954075774971297\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9896670493685419\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968586387434555\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977753058954394\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977753058954394\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978586723768736\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987405541561712\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977578475336323\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9955156950672646\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977924944812362\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987834549878345\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978586723768736\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Experiment 3 checks whether observable 0 has value 1 and whether observable 1 has value 0\n",
      " \n",
      "Accuracy: 0.64\n",
      "Weighted accuracy: 0.7575392038600723\n",
      " \n",
      "Accuracy: 0.626\n",
      "Weighted accuracy: 0.731585518102372\n",
      " \n",
      "Accuracy: 0.674\n",
      "Weighted accuracy: 0.7953281423804227\n",
      " \n",
      "Accuracy: 0.756\n",
      "Weighted accuracy: 0.8521739130434782\n",
      " \n",
      "Accuracy: 0.828\n",
      "Weighted accuracy: 0.8996763754045307\n",
      " \n",
      "Accuracy: 0.832\n",
      "Weighted accuracy: 0.8920521945432978\n",
      " \n",
      "Accuracy: 0.822\n",
      "Weighted accuracy: 0.8986332574031891\n",
      " \n",
      "Accuracy: 0.866\n",
      "Weighted accuracy: 0.9242937853107345\n",
      " \n",
      "Accuracy: 0.852\n",
      "Weighted accuracy: 0.9054842473745625\n",
      " \n",
      "Accuracy: 0.858\n",
      "Weighted accuracy: 0.9191343963553531\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.9098457888493475\n",
      " \n",
      "Accuracy: 0.856\n",
      "Weighted accuracy: 0.9211391018619934\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9305882352941176\n",
      " \n",
      "Accuracy: 0.892\n",
      "Weighted accuracy: 0.9380022962112514\n",
      " \n",
      "Accuracy: 0.87\n",
      "Weighted accuracy: 0.9265536723163842\n",
      " \n",
      "Accuracy: 0.872\n",
      "Weighted accuracy: 0.9309600862998921\n",
      " \n",
      "Accuracy: 0.854\n",
      "Weighted accuracy: 0.9187986651835373\n",
      " \n",
      "Accuracy: 0.888\n",
      "Weighted accuracy: 0.9395900755124056\n",
      " \n",
      "Accuracy: 0.866\n",
      "Weighted accuracy: 0.9242937853107345\n",
      " \n",
      "Accuracy: 0.894\n",
      "Weighted accuracy: 0.9401129943502825\n",
      " \n",
      "Accuracy: 0.896\n",
      "Weighted accuracy: 0.9398148148148148\n",
      " \n",
      "Accuracy: 0.936\n",
      "Weighted accuracy: 0.9629629629629629\n",
      " \n",
      "Accuracy: 0.94\n",
      "Weighted accuracy: 0.9685863874345549\n",
      " \n",
      "Accuracy: 0.946\n",
      "Weighted accuracy: 0.9725330620549338\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9835706462212487\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9880434782608696\n",
      " \n",
      "Accuracy: 0.964\n",
      "Weighted accuracy: 0.9805825242718447\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9902912621359223\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9942594718714122\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9926160337552743\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.989451476793249\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9942594718714122\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9939172749391727\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9952550415183867\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968354430379747\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997874601487779\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967141292442497\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964705882352941\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997874601487779\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975460122699387\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965831435079726\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976851851851852\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9895287958115183\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989680082559339\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9969040247678018\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9890470974808324\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9875156054931336\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9866518353726362\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987834549878345\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965556831228473\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.998062015503876\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976275207591934\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9899888765294772\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9908883826879271\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976851851851852\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9990439770554493\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9961538461538462\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9908883826879271\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987730061349693\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987834549878345\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977220956719818\n",
      " \n",
      "Experiment 9 checks whether observable 0 has value 0 and whether observable 2 has value 0\n",
      " \n",
      "Accuracy: 0.52\n",
      "Weighted accuracy: 0.49551569506726456\n",
      " \n",
      "Accuracy: 0.536\n",
      "Weighted accuracy: 0.5489614243323442\n",
      " \n",
      "Accuracy: 0.62\n",
      "Weighted accuracy: 0.6991434689507494\n",
      " \n",
      "Accuracy: 0.636\n",
      "Weighted accuracy: 0.7310734463276836\n",
      " \n",
      "Accuracy: 0.698\n",
      "Weighted accuracy: 0.7522421524663677\n",
      " \n",
      "Accuracy: 0.732\n",
      "Weighted accuracy: 0.8115631691648822\n",
      " \n",
      "Accuracy: 0.748\n",
      "Weighted accuracy: 0.8222958057395143\n",
      " \n",
      "Accuracy: 0.758\n",
      "Weighted accuracy: 0.8247058823529412\n",
      " \n",
      "Accuracy: 0.822\n",
      "Weighted accuracy: 0.8677966101694915\n",
      " \n",
      "Accuracy: 0.8\n",
      "Weighted accuracy: 0.8473360655737705\n",
      " \n",
      "Accuracy: 0.818\n",
      "Weighted accuracy: 0.908080808080808\n",
      " \n",
      "Accuracy: 0.822\n",
      "Weighted accuracy: 0.8851674641148325\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.911318553092182\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.916058394160584\n",
      " \n",
      "Accuracy: 0.842\n",
      "Weighted accuracy: 0.8914819136522754\n",
      " \n",
      "Accuracy: 0.856\n",
      "Weighted accuracy: 0.9173363949483353\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.9154616240266963\n",
      " \n",
      "Accuracy: 0.864\n",
      "Weighted accuracy: 0.9101861993428259\n",
      " \n",
      "Accuracy: 0.85\n",
      "Weighted accuracy: 0.9050925925925926\n",
      " \n",
      "Accuracy: 0.876\n",
      "Weighted accuracy: 0.928817451205511\n",
      " \n",
      "Accuracy: 0.844\n",
      "Weighted accuracy: 0.9008894536213469\n",
      " \n",
      "Accuracy: 0.87\n",
      "Weighted accuracy: 0.9092827004219409\n",
      " \n",
      "Accuracy: 0.86\n",
      "Weighted accuracy: 0.905521472392638\n",
      " \n",
      "Accuracy: 0.88\n",
      "Weighted accuracy: 0.9129411764705883\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.9090909090909091\n",
      " \n",
      "Accuracy: 0.864\n",
      "Weighted accuracy: 0.9080717488789237\n",
      " \n",
      "Accuracy: 0.884\n",
      "Weighted accuracy: 0.9195544554455446\n",
      " \n",
      "Accuracy: 0.848\n",
      "Weighted accuracy: 0.904707233065442\n",
      " \n",
      "Accuracy: 0.888\n",
      "Weighted accuracy: 0.9362186788154897\n",
      " \n",
      "Accuracy: 0.88\n",
      "Weighted accuracy: 0.9050701186623517\n",
      " \n",
      "Accuracy: 0.896\n",
      "Weighted accuracy: 0.9149840595111584\n",
      " \n",
      "Accuracy: 0.896\n",
      "Weighted accuracy: 0.9439050701186623\n",
      " \n",
      "Accuracy: 0.868\n",
      "Weighted accuracy: 0.9288025889967637\n",
      " \n",
      "Accuracy: 0.902\n",
      "Weighted accuracy: 0.933570581257414\n",
      " \n",
      "Accuracy: 0.888\n",
      "Weighted accuracy: 0.9202733485193622\n",
      " \n",
      "Accuracy: 0.892\n",
      "Weighted accuracy: 0.9298245614035088\n",
      " \n",
      "Accuracy: 0.898\n",
      "Weighted accuracy: 0.9458023379383634\n",
      " \n",
      "Accuracy: 0.91\n",
      "Weighted accuracy: 0.9398148148148148\n",
      " \n",
      "Accuracy: 0.918\n",
      "Weighted accuracy: 0.9400749063670412\n",
      " \n",
      "Accuracy: 0.928\n",
      "Weighted accuracy: 0.9265212399540758\n",
      " \n",
      "Accuracy: 0.928\n",
      "Weighted accuracy: 0.9602649006622517\n",
      " \n",
      "Accuracy: 0.942\n",
      "Weighted accuracy: 0.9647201946472019\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9939172749391727\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.97708082026538\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.98661800486618\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989680082559339\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9957173447537473\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9931113662456946\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9929789368104313\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9929411764705882\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967637540453075\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9949622166246851\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954075774971297\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967880085653105\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9858823529411764\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9933774834437086\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9865470852017937\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976470588235294\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9894982497082847\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943502824858758\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9936238044633369\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9918319719953326\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9963811821471653\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968119022316685\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9950920245398773\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9919632606199771\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9872476089266737\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943502824858758\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9920212765957447\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968354430379747\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9947780678851175\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9871645274212368\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9895833333333334\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9941656942823804\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9962546816479401\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9938962360122076\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9942129629629629\n",
      " \n",
      "Experiment 15 checks whether observable 0 has value 2 and whether observable 2 has value 0\n",
      " \n",
      "Accuracy: 0.54\n",
      "Weighted accuracy: 0.5367913148371531\n",
      " \n",
      "Accuracy: 0.564\n",
      "Weighted accuracy: 0.6103739445114595\n",
      " \n",
      "Accuracy: 0.668\n",
      "Weighted accuracy: 0.6984318455971049\n",
      " \n",
      "Accuracy: 0.696\n",
      "Weighted accuracy: 0.745676500508647\n",
      " \n",
      "Accuracy: 0.706\n",
      "Weighted accuracy: 0.7694117647058824\n",
      " \n",
      "Accuracy: 0.748\n",
      "Weighted accuracy: 0.8227848101265823\n",
      " \n",
      "Accuracy: 0.778\n",
      "Weighted accuracy: 0.8238841978287093\n",
      " \n",
      "Accuracy: 0.792\n",
      "Weighted accuracy: 0.8097949886104784\n",
      " \n",
      "Accuracy: 0.816\n",
      "Weighted accuracy: 0.8608981380065718\n",
      " \n",
      "Accuracy: 0.808\n",
      "Weighted accuracy: 0.8295964125560538\n",
      " \n",
      "Accuracy: 0.846\n",
      "Weighted accuracy: 0.8909899888765295\n",
      " \n",
      "Accuracy: 0.872\n",
      "Weighted accuracy: 0.9132369299221357\n",
      " \n",
      "Accuracy: 0.902\n",
      "Weighted accuracy: 0.9288135593220339\n",
      " \n",
      "Accuracy: 0.908\n",
      "Weighted accuracy: 0.902291917973462\n",
      " \n",
      "Accuracy: 0.886\n",
      "Weighted accuracy: 0.9057377049180327\n",
      " \n",
      "Accuracy: 0.93\n",
      "Weighted accuracy: 0.9505882352941176\n",
      " \n",
      "Accuracy: 0.944\n",
      "Weighted accuracy: 0.9643947100712106\n",
      " \n",
      "Accuracy: 0.96\n",
      "Weighted accuracy: 0.977924944812362\n",
      " \n",
      "Accuracy: 0.964\n",
      "Weighted accuracy: 0.9791666666666666\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9658314350797267\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9895287958115183\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9780876494023905\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9659949622166247\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9784250269687162\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9862869198312236\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9832535885167464\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9868421052631579\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9835294117647059\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9864406779661017\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9883313885647608\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979939819458375\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967637540453075\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976662777129521\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987937273823885\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9986449864498645\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9970326409495549\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9990503323836657\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987179487179487\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989528795811519\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987063389391979\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965277777777778\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9898305084745763\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 21 checks whether observable 1 has value 1 and whether observable 2 has value 0\n",
      " \n",
      "Accuracy: 0.492\n",
      "Weighted accuracy: 0.34705882352941175\n",
      " \n",
      "Accuracy: 0.514\n",
      "Weighted accuracy: 0.4744525547445255\n",
      " \n",
      "Accuracy: 0.61\n",
      "Weighted accuracy: 0.7573839662447257\n",
      " \n",
      "Accuracy: 0.616\n",
      "Weighted accuracy: 0.7532608695652174\n",
      " \n",
      "Accuracy: 0.686\n",
      "Weighted accuracy: 0.7803532008830022\n",
      " \n",
      "Accuracy: 0.714\n",
      "Weighted accuracy: 0.8182784272051009\n",
      " \n",
      "Accuracy: 0.752\n",
      "Weighted accuracy: 0.8720330237358102\n",
      " \n",
      "Accuracy: 0.808\n",
      "Weighted accuracy: 0.8705882352941177\n",
      " \n",
      "Accuracy: 0.812\n",
      "Weighted accuracy: 0.8697225572979493\n",
      " \n",
      "Accuracy: 0.86\n",
      "Weighted accuracy: 0.9072847682119205\n",
      " \n",
      "Accuracy: 0.858\n",
      "Weighted accuracy: 0.8968609865470852\n",
      " \n",
      "Accuracy: 0.922\n",
      "Weighted accuracy: 0.916354556803995\n",
      " \n",
      "Accuracy: 0.91\n",
      "Weighted accuracy: 0.8409343715239155\n",
      " \n",
      "Accuracy: 0.904\n",
      "Weighted accuracy: 0.9014238773274917\n",
      " \n",
      "Accuracy: 0.93\n",
      "Weighted accuracy: 0.933570581257414\n",
      " \n",
      "Accuracy: 0.956\n",
      "Weighted accuracy: 0.9661610268378062\n",
      " \n",
      "Accuracy: 0.948\n",
      "Weighted accuracy: 0.9430379746835443\n",
      " \n",
      "Accuracy: 0.956\n",
      "Weighted accuracy: 0.9529025191675794\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.981151832460733\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9742152466367713\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9957173447537473\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9845788849347569\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9905100830367735\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.991556091676719\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9830508474576272\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9956188389923329\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9945235487404163\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9910313901345291\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966367713004485\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9956521739130435\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989754098360656\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978586723768736\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979057591623036\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9986945169712794\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978586723768736\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977753058954394\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997920997920998\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998989898989899\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 1 checks whether observable 0 has value 0 and whether observable 1 has value 1\n",
      " \n",
      "Accuracy: 0.488\n",
      "Weighted accuracy: 0.4363425925925926\n",
      " \n",
      "Accuracy: 0.604\n",
      "Weighted accuracy: 0.6936067551266586\n",
      " \n",
      "Accuracy: 0.598\n",
      "Weighted accuracy: 0.7432735426008968\n",
      " \n",
      "Accuracy: 0.676\n",
      "Weighted accuracy: 0.7815845824411135\n",
      " \n",
      "Accuracy: 0.686\n",
      "Weighted accuracy: 0.7892938496583144\n",
      " \n",
      "Accuracy: 0.748\n",
      "Weighted accuracy: 0.852975495915986\n",
      " \n",
      "Accuracy: 0.76\n",
      "Weighted accuracy: 0.8273045507584598\n",
      " \n",
      "Accuracy: 0.82\n",
      "Weighted accuracy: 0.899888765294772\n",
      " \n",
      "Accuracy: 0.792\n",
      "Weighted accuracy: 0.8796296296296297\n",
      " \n",
      "Accuracy: 0.81\n",
      "Weighted accuracy: 0.8934977578475336\n",
      " \n",
      "Accuracy: 0.798\n",
      "Weighted accuracy: 0.8876529477196885\n",
      " \n",
      "Accuracy: 0.85\n",
      "Weighted accuracy: 0.9058553386911596\n",
      " \n",
      "Accuracy: 0.848\n",
      "Weighted accuracy: 0.9120370370370371\n",
      " \n",
      "Accuracy: 0.892\n",
      "Weighted accuracy: 0.941304347826087\n",
      " \n",
      "Accuracy: 0.886\n",
      "Weighted accuracy: 0.9360986547085202\n",
      " \n",
      "Accuracy: 0.916\n",
      "Weighted accuracy: 0.9518664047151277\n",
      " \n",
      "Accuracy: 0.902\n",
      "Weighted accuracy: 0.940389294403893\n",
      " \n",
      "Accuracy: 0.926\n",
      "Weighted accuracy: 0.9545923632610939\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9776470588235294\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9828693790149893\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9898305084745763\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9865470852017937\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9902173913043478\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9855521155830753\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9922135706340378\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9957173447537473\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9919759277833501\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9910313901345291\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9905759162303664\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9903640256959315\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978260869565218\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978586723768736\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 7 checks whether observable 0 has value 2 and whether observable 1 has value 1\n",
      " \n",
      "Accuracy: 0.51\n",
      "Weighted accuracy: 0.525027808676307\n",
      " \n",
      "Accuracy: 0.63\n",
      "Weighted accuracy: 0.7680461982675649\n",
      " \n",
      "Accuracy: 0.682\n",
      "Weighted accuracy: 0.817451205510907\n",
      " \n",
      "Accuracy: 0.678\n",
      "Weighted accuracy: 0.8055555555555556\n",
      " \n",
      "Accuracy: 0.704\n",
      "Weighted accuracy: 0.8148959474260679\n",
      " \n",
      "Accuracy: 0.786\n",
      "Weighted accuracy: 0.8788522848034006\n",
      " \n",
      "Accuracy: 0.816\n",
      "Weighted accuracy: 0.8890229191797346\n",
      " \n",
      "Accuracy: 0.856\n",
      "Weighted accuracy: 0.9152941176470588\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.926673751328374\n",
      " \n",
      "Accuracy: 0.826\n",
      "Weighted accuracy: 0.9075451647183846\n",
      " \n",
      "Accuracy: 0.856\n",
      "Weighted accuracy: 0.9114349775784754\n",
      " \n",
      "Accuracy: 0.848\n",
      "Weighted accuracy: 0.9141242937853107\n",
      " \n",
      "Accuracy: 0.84\n",
      "Weighted accuracy: 0.9103139013452914\n",
      " \n",
      "Accuracy: 0.884\n",
      "Weighted accuracy: 0.9397089397089398\n",
      " \n",
      "Accuracy: 0.86\n",
      "Weighted accuracy: 0.9196326061997704\n",
      " \n",
      "Accuracy: 0.896\n",
      "Weighted accuracy: 0.9426048565121413\n",
      " \n",
      "Accuracy: 0.898\n",
      "Weighted accuracy: 0.9449838187702265\n",
      " \n",
      "Accuracy: 0.908\n",
      "Weighted accuracy: 0.9458823529411765\n",
      " \n",
      "Accuracy: 0.894\n",
      "Weighted accuracy: 0.9225130890052357\n",
      " \n",
      "Accuracy: 0.94\n",
      "Weighted accuracy: 0.9671412924424972\n",
      " \n",
      "Accuracy: 0.94\n",
      "Weighted accuracy: 0.9585201793721974\n",
      " \n",
      "Accuracy: 0.964\n",
      "Weighted accuracy: 0.9738219895287958\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9688149688149689\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9819209039548022\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9894117647058823\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9876160990712074\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9875706214689266\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9935275080906149\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.989749430523918\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9899103139013453\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9796650717703349\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9886649874055415\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9882352941176471\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9916839916839917\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9802847754654983\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9741100323624595\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9913700107874865\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9942129629629629\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989680082559339\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9910313901345291\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.990506329113924\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9895287958115183\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987937273823885\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9908883826879271\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9820627802690582\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9899103139013453\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9956850053937433\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997874601487779\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9915611814345991\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9911012235817576\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 13 checks whether observable 0 has value 1 and whether observable 2 has value 1\n",
      " \n",
      "Accuracy: 0.49\n",
      "Weighted accuracy: 0.4611764705882353\n",
      " \n",
      "Accuracy: 0.586\n",
      "Weighted accuracy: 0.5453501722158438\n",
      " \n",
      "Accuracy: 0.634\n",
      "Weighted accuracy: 0.6666666666666666\n",
      " \n",
      "Accuracy: 0.686\n",
      "Weighted accuracy: 0.6984304932735426\n",
      " \n",
      "Accuracy: 0.69\n",
      "Weighted accuracy: 0.7239685658153242\n",
      " \n",
      "Accuracy: 0.726\n",
      "Weighted accuracy: 0.7798804780876494\n",
      " \n",
      "Accuracy: 0.706\n",
      "Weighted accuracy: 0.7957992998833139\n",
      " \n",
      "Accuracy: 0.794\n",
      "Weighted accuracy: 0.8483796296296297\n",
      " \n",
      "Accuracy: 0.798\n",
      "Weighted accuracy: 0.8433734939759037\n",
      " \n",
      "Accuracy: 0.812\n",
      "Weighted accuracy: 0.8576158940397351\n",
      " \n",
      "Accuracy: 0.804\n",
      "Weighted accuracy: 0.8762993762993763\n",
      " \n",
      "Accuracy: 0.86\n",
      "Weighted accuracy: 0.889294403892944\n",
      " \n",
      "Accuracy: 0.83\n",
      "Weighted accuracy: 0.851808634772462\n",
      " \n",
      "Accuracy: 0.856\n",
      "Weighted accuracy: 0.8720095693779905\n",
      " \n",
      "Accuracy: 0.86\n",
      "Weighted accuracy: 0.8660998937300743\n",
      " \n",
      "Accuracy: 0.88\n",
      "Weighted accuracy: 0.8444976076555024\n",
      " \n",
      "Accuracy: 0.894\n",
      "Weighted accuracy: 0.8739130434782608\n",
      " \n",
      "Accuracy: 0.922\n",
      "Weighted accuracy: 0.9150401836969001\n",
      " \n",
      "Accuracy: 0.898\n",
      "Weighted accuracy: 0.8473360655737705\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.8811563169164882\n",
      " \n",
      "Accuracy: 0.936\n",
      "Weighted accuracy: 0.9219409282700421\n",
      " \n",
      "Accuracy: 0.954\n",
      "Weighted accuracy: 0.9465968586387434\n",
      " \n",
      "Accuracy: 0.952\n",
      "Weighted accuracy: 0.9265850945494994\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9305239179954442\n",
      " \n",
      "Accuracy: 0.96\n",
      "Weighted accuracy: 0.951764705882353\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.969059405940594\n",
      " \n",
      "Accuracy: 0.964\n",
      "Weighted accuracy: 0.90311004784689\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9276393831553974\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9241803278688525\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9638273045507585\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9851222104144527\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9626262626262626\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9933259176863182\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964705882352941\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9916963226571768\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977924944812362\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.992638036809816\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975460122699387\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976275207591934\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9902912621359223\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968119022316685\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987293519695044\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967880085653105\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979360165118679\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989754098360656\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998960498960499\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 19 checks whether observable 1 has value 0 and whether observable 2 has value 1\n",
      " \n",
      "Accuracy: 0.618\n",
      "Weighted accuracy: 0.7018909899888766\n",
      " \n",
      "Accuracy: 0.654\n",
      "Weighted accuracy: 0.7717647058823529\n",
      " \n",
      "Accuracy: 0.728\n",
      "Weighted accuracy: 0.860655737704918\n",
      " \n",
      "Accuracy: 0.77\n",
      "Weighted accuracy: 0.8759439050701187\n",
      " \n",
      "Accuracy: 0.79\n",
      "Weighted accuracy: 0.8644646924829157\n",
      " \n",
      "Accuracy: 0.808\n",
      "Weighted accuracy: 0.8822269807280514\n",
      " \n",
      "Accuracy: 0.798\n",
      "Weighted accuracy: 0.876993166287016\n",
      " \n",
      "Accuracy: 0.844\n",
      "Weighted accuracy: 0.8929471032745592\n",
      " \n",
      "Accuracy: 0.852\n",
      "Weighted accuracy: 0.9064824654622742\n",
      " \n",
      "Accuracy: 0.858\n",
      "Weighted accuracy: 0.9184845005740528\n",
      " \n",
      "Accuracy: 0.82\n",
      "Weighted accuracy: 0.9109792284866469\n",
      " \n",
      "Accuracy: 0.852\n",
      "Weighted accuracy: 0.9201725997842503\n",
      " \n",
      "Accuracy: 0.868\n",
      "Weighted accuracy: 0.9271523178807947\n",
      " \n",
      "Accuracy: 0.852\n",
      "Weighted accuracy: 0.9183222958057395\n",
      " \n",
      "Accuracy: 0.876\n",
      "Weighted accuracy: 0.9315673289183223\n",
      " \n",
      "Accuracy: 0.848\n",
      "Weighted accuracy: 0.9090909090909091\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9363425925925926\n",
      " \n",
      "Accuracy: 0.896\n",
      "Weighted accuracy: 0.9402985074626866\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9347568208778173\n",
      " \n",
      "Accuracy: 0.922\n",
      "Weighted accuracy: 0.9555808656036446\n",
      " \n",
      "Accuracy: 0.904\n",
      "Weighted accuracy: 0.9378531073446328\n",
      " \n",
      "Accuracy: 0.938\n",
      "Weighted accuracy: 0.966304347826087\n",
      " \n",
      "Accuracy: 0.944\n",
      "Weighted accuracy: 0.9670588235294117\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9914346895074947\n",
      " \n",
      "Accuracy: 0.96\n",
      "Weighted accuracy: 0.9723360655737705\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9828693790149893\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9901423877327492\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9795121951219512\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9831223628691983\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964705882352941\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9774011299435028\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9877641824249166\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978902953586498\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977578475336323\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9973890339425587\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9914346895074947\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977578475336323\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 25 checks whether observable 1 has value 2 and whether observable 2 has value 1\n",
      " \n",
      "Accuracy: 0.422\n",
      "Weighted accuracy: 0.2947903430749682\n",
      " \n",
      "Accuracy: 0.476\n",
      "Weighted accuracy: 0.3811881188118812\n",
      " \n",
      "Accuracy: 0.516\n",
      "Weighted accuracy: 0.4582869855394883\n",
      " \n",
      "Accuracy: 0.598\n",
      "Weighted accuracy: 0.619914346895075\n",
      " \n",
      "Accuracy: 0.654\n",
      "Weighted accuracy: 0.6993166287015945\n",
      " \n",
      "Accuracy: 0.76\n",
      "Weighted accuracy: 0.8012269938650307\n",
      " \n",
      "Accuracy: 0.764\n",
      "Weighted accuracy: 0.857630979498861\n",
      " \n",
      "Accuracy: 0.864\n",
      "Weighted accuracy: 0.9165739710789766\n",
      " \n",
      "Accuracy: 0.892\n",
      "Weighted accuracy: 0.928235294117647\n",
      " \n",
      "Accuracy: 0.958\n",
      "Weighted accuracy: 0.9665071770334929\n",
      " \n",
      "Accuracy: 0.954\n",
      "Weighted accuracy: 0.9339407744874715\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9707070707070707\n",
      " \n",
      "Accuracy: 0.964\n",
      "Weighted accuracy: 0.9562780269058296\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9756944444444444\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9637952559300874\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9798206278026906\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9829157175398633\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9834437086092715\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9879372738238842\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9938118811881188\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9914110429447853\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954441913439636\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9951749095295537\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977037887485649\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954075774971297\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9933259176863182\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9986945169712794\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978902953586498\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9940191387559809\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965277777777778\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979360165118679\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989680082559339\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9944812362030905\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964114832535885\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977220956719818\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987293519695044\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997920997920998\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987730061349693\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964114832535885\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967141292442497\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975460122699387\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978425026968716\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9968586387434555\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9933774834437086\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964994165694282\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965556831228473\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9952941176470588\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997920997920998\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9955156950672646\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987515605493134\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975874547647768\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9953703703703703\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.996662958843159\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966101694915255\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975031210986267\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9965556831228473\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977753058954394\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997874601487779\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9953703703703703\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Experiment 24 checks whether observable 1 has value 2 and whether observable 2 has value 0\n",
      " \n",
      "Accuracy: 0.436\n",
      "Weighted accuracy: 0.319377990430622\n",
      " \n",
      "Accuracy: 0.536\n",
      "Weighted accuracy: 0.6747685185185185\n",
      " \n",
      "Accuracy: 0.616\n",
      "Weighted accuracy: 0.7579075425790754\n",
      " \n",
      "Accuracy: 0.632\n",
      "Weighted accuracy: 0.7870370370370371\n",
      " \n",
      "Accuracy: 0.652\n",
      "Weighted accuracy: 0.7883211678832117\n",
      " \n",
      "Accuracy: 0.672\n",
      "Weighted accuracy: 0.8161434977578476\n",
      " \n",
      "Accuracy: 0.708\n",
      "Weighted accuracy: 0.8310185185185185\n",
      " \n",
      "Accuracy: 0.72\n",
      "Weighted accuracy: 0.8414239482200647\n",
      " \n",
      "Accuracy: 0.688\n",
      "Weighted accuracy: 0.8158192090395481\n",
      " \n",
      "Accuracy: 0.724\n",
      "Weighted accuracy: 0.8522483940042827\n",
      " \n",
      "Accuracy: 0.78\n",
      "Weighted accuracy: 0.8757062146892656\n",
      " \n",
      "Accuracy: 0.81\n",
      "Weighted accuracy: 0.9019607843137255\n",
      " \n",
      "Accuracy: 0.826\n",
      "Weighted accuracy: 0.8976470588235295\n",
      " \n",
      "Accuracy: 0.838\n",
      "Weighted accuracy: 0.9105960264900662\n",
      " \n",
      "Accuracy: 0.872\n",
      "Weighted accuracy: 0.9277721261444557\n",
      " \n",
      "Accuracy: 0.878\n",
      "Weighted accuracy: 0.932146829810901\n",
      " \n",
      "Accuracy: 0.938\n",
      "Weighted accuracy: 0.9583789704271632\n",
      " \n",
      "Accuracy: 0.926\n",
      "Weighted accuracy: 0.9453961456102784\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9832535885167464\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9923913043478261\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9946466809421841\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9914841849148418\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9850107066381156\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9946062567421791\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9898305084745763\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9941176470588236\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9962546816479401\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9952941176470588\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9939393939393939\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975460122699387\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988235294117647\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9973890339425587\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987937273823885\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978425026968716\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977753058954394\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975247524752475\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987730061349693\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998960498960499\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9908883826879271\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977753058954394\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9918616480162767\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987730061349693\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 5 checks whether observable 0 has value 1 and whether observable 1 has value 2\n",
      " \n",
      "Accuracy: 0.416\n",
      "Weighted accuracy: 0.2489451476793249\n",
      " \n",
      "Accuracy: 0.52\n",
      "Weighted accuracy: 0.4876681614349776\n",
      " \n",
      "Accuracy: 0.634\n",
      "Weighted accuracy: 0.7341490545050056\n",
      " \n",
      "Accuracy: 0.702\n",
      "Weighted accuracy: 0.8378378378378378\n",
      " \n",
      "Accuracy: 0.722\n",
      "Weighted accuracy: 0.8233995584988962\n",
      " \n",
      "Accuracy: 0.748\n",
      "Weighted accuracy: 0.8395657418576599\n",
      " \n",
      "Accuracy: 0.768\n",
      "Weighted accuracy: 0.853904282115869\n",
      " \n",
      "Accuracy: 0.79\n",
      "Weighted accuracy: 0.8703703703703703\n",
      " \n",
      "Accuracy: 0.816\n",
      "Weighted accuracy: 0.888135593220339\n",
      " \n",
      "Accuracy: 0.886\n",
      "Weighted accuracy: 0.9424242424242424\n",
      " \n",
      "Accuracy: 0.846\n",
      "Weighted accuracy: 0.9227683049147443\n",
      " \n",
      "Accuracy: 0.832\n",
      "Weighted accuracy: 0.8995215311004785\n",
      " \n",
      "Accuracy: 0.866\n",
      "Weighted accuracy: 0.9091928251121076\n",
      " \n",
      "Accuracy: 0.894\n",
      "Weighted accuracy: 0.9371293001186239\n",
      " \n",
      "Accuracy: 0.938\n",
      "Weighted accuracy: 0.9655172413793104\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9851222104144527\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9758897818599311\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9884816753926702\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9932203389830508\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9956188389923329\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.99644128113879\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967880085653105\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9906651108518086\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9901840490797545\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988331388564761\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9899888765294772\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9892344497607656\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989528795811519\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9918616480162767\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9990108803165183\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9895833333333334\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9904357066950054\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987834549878345\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989130434782608\n",
      " \n",
      "Experiment 8 checks whether observable 0 has value 2 and whether observable 1 has value 2\n",
      " \n",
      "Accuracy: 0.472\n",
      "Weighted accuracy: 0.3451995685005394\n",
      " \n",
      "Accuracy: 0.568\n",
      "Weighted accuracy: 0.5776986951364176\n",
      " \n",
      "Accuracy: 0.594\n",
      "Weighted accuracy: 0.6956521739130435\n",
      " \n",
      "Accuracy: 0.68\n",
      "Weighted accuracy: 0.7669902912621359\n",
      " \n",
      "Accuracy: 0.758\n",
      "Weighted accuracy: 0.8536345776031434\n",
      " \n",
      "Accuracy: 0.746\n",
      "Weighted accuracy: 0.8369829683698297\n",
      " \n",
      "Accuracy: 0.794\n",
      "Weighted accuracy: 0.8888888888888888\n",
      " \n",
      "Accuracy: 0.786\n",
      "Weighted accuracy: 0.8624849215922799\n",
      " \n",
      "Accuracy: 0.832\n",
      "Weighted accuracy: 0.9054054054054054\n",
      " \n",
      "Accuracy: 0.826\n",
      "Weighted accuracy: 0.8985976267529665\n",
      " \n",
      "Accuracy: 0.854\n",
      "Weighted accuracy: 0.9252049180327869\n",
      " \n",
      "Accuracy: 0.858\n",
      "Weighted accuracy: 0.9136253041362531\n",
      " \n",
      "Accuracy: 0.876\n",
      "Weighted accuracy: 0.9258373205741627\n",
      " \n",
      "Accuracy: 0.858\n",
      "Weighted accuracy: 0.9139072847682119\n",
      " \n",
      "Accuracy: 0.886\n",
      "Weighted accuracy: 0.9365962180200222\n",
      " \n",
      "Accuracy: 0.882\n",
      "Weighted accuracy: 0.9338565022421524\n",
      " \n",
      "Accuracy: 0.902\n",
      "Weighted accuracy: 0.940389294403893\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9383408071748879\n",
      " \n",
      "Accuracy: 0.9\n",
      "Weighted accuracy: 0.9452354874041621\n",
      " \n",
      "Accuracy: 0.892\n",
      "Weighted accuracy: 0.9369894982497082\n",
      " \n",
      "Accuracy: 0.926\n",
      "Weighted accuracy: 0.9606801275239107\n",
      " \n",
      "Accuracy: 0.912\n",
      "Weighted accuracy: 0.9464720194647201\n",
      " \n",
      "Accuracy: 0.958\n",
      "Weighted accuracy: 0.9776833156216791\n",
      " \n",
      "Accuracy: 0.956\n",
      "Weighted accuracy: 0.9755283648498332\n",
      " \n",
      "Accuracy: 0.954\n",
      "Weighted accuracy: 0.9738041002277904\n",
      " \n",
      "Accuracy: 0.952\n",
      "Weighted accuracy: 0.9686868686868687\n",
      " \n",
      "Accuracy: 0.95\n",
      "Weighted accuracy: 0.9726177437020811\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9864864864864865\n",
      " \n",
      "Accuracy: 0.966\n",
      "Weighted accuracy: 0.9735099337748344\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9751412429378531\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.967391304347826\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9840595111583422\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9877641824249166\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9867310012062727\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9845201238390093\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9833926453143536\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9873417721518988\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9881376037959668\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9882352941176471\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9807909604519774\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9905759162303664\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9824390243902439\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987937273823885\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9798206278026906\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975669099756691\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9624724061810155\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9905882352941177\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9896800825593395\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.997364953886693\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9979057591623036\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9728813559322034\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9952153110047847\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9439906651108518\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9900662251655629\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988700564971752\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9807909604519774\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989293361884368\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9963811821471653\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9911012235817576\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987623762376238\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9759277833500501\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9913700107874865\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Experiment 14 checks whether observable 0 has value 1 and whether observable 2 has value 2\n",
      " \n",
      "Accuracy: 0.526\n",
      "Weighted accuracy: 0.5093966369930761\n",
      " \n",
      "Accuracy: 0.588\n",
      "Weighted accuracy: 0.65311004784689\n",
      " \n",
      "Accuracy: 0.61\n",
      "Weighted accuracy: 0.6711065573770492\n",
      " \n",
      "Accuracy: 0.666\n",
      "Weighted accuracy: 0.7603795966785291\n",
      " \n",
      "Accuracy: 0.716\n",
      "Weighted accuracy: 0.7761557177615572\n",
      " \n",
      "Accuracy: 0.734\n",
      "Weighted accuracy: 0.7889022919179735\n",
      " \n",
      "Accuracy: 0.798\n",
      "Weighted accuracy: 0.8597826086956522\n",
      " \n",
      "Accuracy: 0.808\n",
      "Weighted accuracy: 0.8471411901983664\n",
      " \n",
      "Accuracy: 0.814\n",
      "Weighted accuracy: 0.8462515883100381\n",
      " \n",
      "Accuracy: 0.86\n",
      "Weighted accuracy: 0.9169632265717675\n",
      " \n",
      "Accuracy: 0.89\n",
      "Weighted accuracy: 0.9043570669500531\n",
      " \n",
      "Accuracy: 0.902\n",
      "Weighted accuracy: 0.9276693455797933\n",
      " \n",
      "Accuracy: 0.928\n",
      "Weighted accuracy: 0.9234088457389428\n",
      " \n",
      "Accuracy: 0.938\n",
      "Weighted accuracy: 0.9367396593673966\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.9641089108910891\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9591584158415841\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9594742606790799\n",
      " \n",
      "Accuracy: 0.97\n",
      "Weighted accuracy: 0.970498474059003\n",
      " \n",
      "Accuracy: 0.954\n",
      "Weighted accuracy: 0.9222488038277512\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9658976930792377\n",
      " \n",
      "Accuracy: 0.974\n",
      "Weighted accuracy: 0.9682352941176471\n",
      " \n",
      "Accuracy: 0.966\n",
      "Weighted accuracy: 0.9606099110546379\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9831223628691983\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9802847754654983\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9891435464414958\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9866518353726362\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9810901001112347\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9753747323340471\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9865841073271414\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9900662251655629\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9862475442043221\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988518943742825\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9896507115135834\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9944812362030905\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976662777129521\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966887417218543\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.992638036809816\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943052391799544\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9895833333333334\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9967141292442497\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976470588235294\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975247524752475\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9945235487404163\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9964994165694282\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976470588235294\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954441913439636\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954075774971297\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988425925925926\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988789237668162\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989754098360656\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977578475336323\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9980353634577603\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9969481180061037\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.999003984063745\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988962472406181\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9956521739130435\n",
      " \n",
      "Experiment 20 checks whether observable 1 has value 0 and whether observable 2 has value 2\n",
      " \n",
      "Accuracy: 0.644\n",
      "Weighted accuracy: 0.7830508474576271\n",
      " \n",
      "Accuracy: 0.68\n",
      "Weighted accuracy: 0.8097949886104784\n",
      " \n",
      "Accuracy: 0.734\n",
      "Weighted accuracy: 0.830168776371308\n",
      " \n",
      "Accuracy: 0.79\n",
      "Weighted accuracy: 0.8686534216335541\n",
      " \n",
      "Accuracy: 0.796\n",
      "Weighted accuracy: 0.8519362186788155\n",
      " \n",
      "Accuracy: 0.798\n",
      "Weighted accuracy: 0.8521739130434782\n",
      " \n",
      "Accuracy: 0.836\n",
      "Weighted accuracy: 0.8898514851485149\n",
      " \n",
      "Accuracy: 0.842\n",
      "Weighted accuracy: 0.8870056497175142\n",
      " \n",
      "Accuracy: 0.852\n",
      "Weighted accuracy: 0.9070034443168772\n",
      " \n",
      "Accuracy: 0.874\n",
      "Weighted accuracy: 0.9288135593220339\n",
      " \n",
      "Accuracy: 0.862\n",
      "Weighted accuracy: 0.9255663430420712\n",
      " \n",
      "Accuracy: 0.904\n",
      "Weighted accuracy: 0.9448909299655568\n",
      " \n",
      "Accuracy: 0.902\n",
      "Weighted accuracy: 0.9428238039673279\n",
      " \n",
      "Accuracy: 0.912\n",
      "Weighted accuracy: 0.9441401971522454\n",
      " \n",
      "Accuracy: 0.932\n",
      "Weighted accuracy: 0.9589867310012062\n",
      " \n",
      "Accuracy: 0.96\n",
      "Weighted accuracy: 0.9766627771295215\n",
      " \n",
      "Accuracy: 0.966\n",
      "Weighted accuracy: 0.9719953325554259\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9875706214689266\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9873708381171068\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9846153846153847\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9861849096705633\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9776833156216791\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9963503649635036\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9874055415617129\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976470588235294\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9896800825593395\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976851851851852\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975669099756691\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9975874547647768\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976662777129521\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987623762376238\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9973890339425587\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989451476793249\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9904357066950054\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9986945169712794\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9914984059511158\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989047097480832\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9888765294771968\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9980879541108987\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989373007438895\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988876529477196\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9894982497082847\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989754098360656\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.998960498960499\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9986449864498645\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.99644128113879\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988137603795967\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977401129943503\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988038277511961\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976470588235294\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9989212513484358\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978425026968716\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9901423877327492\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9817767653758542\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Accuracy: 1.0\n",
      "Weighted accuracy: 1.0\n",
      " \n",
      "Experiment 26 checks whether observable 1 has value 2 and whether observable 2 has value 2\n",
      " \n",
      "Accuracy: 0.462\n",
      "Weighted accuracy: 0.2781456953642384\n",
      " \n",
      "Accuracy: 0.504\n",
      "Weighted accuracy: 0.44282238442822386\n",
      " \n",
      "Accuracy: 0.496\n",
      "Weighted accuracy: 0.49325153374233127\n",
      " \n",
      "Accuracy: 0.578\n",
      "Weighted accuracy: 0.6050516647531573\n",
      " \n",
      "Accuracy: 0.684\n",
      "Weighted accuracy: 0.7715231788079471\n",
      " \n",
      "Accuracy: 0.744\n",
      "Weighted accuracy: 0.8395480225988701\n",
      " \n",
      "Accuracy: 0.794\n",
      "Weighted accuracy: 0.8766816143497758\n",
      " \n",
      "Accuracy: 0.832\n",
      "Weighted accuracy: 0.8995584988962473\n",
      " \n",
      "Accuracy: 0.854\n",
      "Weighted accuracy: 0.9200438116100766\n",
      " \n",
      "Accuracy: 0.92\n",
      "Weighted accuracy: 0.9450661241098678\n",
      " \n",
      "Accuracy: 0.926\n",
      "Weighted accuracy: 0.9535864978902954\n",
      " \n",
      "Accuracy: 0.954\n",
      "Weighted accuracy: 0.9735935706084959\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9786995515695067\n",
      " \n",
      "Accuracy: 0.966\n",
      "Weighted accuracy: 0.971764705882353\n",
      " \n",
      "Accuracy: 0.962\n",
      "Weighted accuracy: 0.9384965831435079\n",
      " \n",
      "Accuracy: 0.972\n",
      "Weighted accuracy: 0.9601366742596811\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9842829076620825\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9911012235817576\n",
      " \n",
      "Accuracy: 0.968\n",
      "Weighted accuracy: 0.9808612440191388\n",
      " \n",
      "Accuracy: 0.976\n",
      "Weighted accuracy: 0.9706214689265537\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9803240740740741\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9857651245551602\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9894117647058823\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9916230366492147\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9910313901345291\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9869513641755635\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9955849889624724\n",
      " \n",
      "Accuracy: 0.98\n",
      "Weighted accuracy: 0.9880382775119617\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9932735426008968\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9918032786885246\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9956521739130435\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9951338199513382\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9874715261958997\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9913700107874865\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9951749095295537\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954075774971297\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9941656942823804\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9935275080906149\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943052391799544\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9966101694915255\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9952153110047847\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9876681614349776\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9769633507853404\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9679715302491103\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9929988331388565\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9977578475336323\n",
      " \n",
      "Accuracy: 0.978\n",
      "Weighted accuracy: 0.9877641824249166\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9906651108518086\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9988610478359908\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9863325740318907\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9864406779661017\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9801047120418848\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9953325554259043\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9911012235817576\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9845474613686535\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9954802259887006\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9943052391799544\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9938118811881188\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9942594718714122\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9955849889624724\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9958115183246073\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9779338014042126\n",
      " \n",
      "Accuracy: 0.998\n",
      "Weighted accuracy: 0.9987293519695044\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9867310012062727\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9869565217391304\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9883103081827843\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9877641824249166\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9976662777129521\n",
      " \n",
      "Accuracy: 0.99\n",
      "Weighted accuracy: 0.9937027707808564\n",
      " \n",
      "Accuracy: 0.982\n",
      "Weighted accuracy: 0.9801980198019802\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9768211920529801\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9978094194961665\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9931662870159453\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.9750889679715302\n",
      " \n",
      "Accuracy: 0.988\n",
      "Weighted accuracy: 0.9854260089686099\n",
      " \n",
      "Accuracy: 0.984\n",
      "Weighted accuracy: 0.9914346895074947\n",
      " \n",
      "Accuracy: 0.994\n",
      "Weighted accuracy: 0.9890470974808324\n",
      " \n",
      "Accuracy: 0.986\n",
      "Weighted accuracy: 0.992827868852459\n",
      " \n",
      "Accuracy: 0.996\n",
      "Weighted accuracy: 0.9889570552147239\n",
      " \n",
      "Accuracy: 0.992\n",
      "Weighted accuracy: 0.9878587196467992\n",
      " \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAHUCAYAAACUMzRUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAADPNUlEQVR4nOydd3xUVfr/P3f6pEx6QgKBhB46goAgUgUD2HZtW1QsuyK4LrKuK3ZZV1Zd+aGuYkHE8lXZXRVRaaGjgAgi0ouUUJKQ3pNp5/fHuefOTKZkZnKnn/frlRfDnTP3njtzy3Of8nkEQggBh8PhcDgcDidkKEI9AQ6Hw+FwOJxYhxtkHA6Hw+FwOCGGG2QcDofD4XA4IYYbZBwOh8PhcDghhhtkHA6Hw+FwOCGGG2QcDofD4XA4IYYbZBwOh8PhcDghhhtkHA6Hw+FwOCGGG2QcDofD4XA4IYYbZBy8+uqrEAQBAwYMCPVUgs6WLVsgCAK2bNkS1O1u3LgRw4cPR3x8PARBwMqVK4O6/UjimWeegSAIoZ5Gu7zxxhtYvnx5qKfh8vtyNzd2/P/vf//za1s7duzAM888g5qaGr8+HyrGjx8f1de7M2fOQBAE/Otf/2p37PLlyyEIAs6cORP4iXE8wg0yDpYtWwYAOHToEL7//vsQzya4XHbZZdi5cycuu+yyoG2TEIJbbrkFarUaq1atws6dOzFu3LigbT/SuPfee7Fz585QT6NdwsUgc/V9BWpuO3bswLPPPhtxBhmHE45wgyzG2bNnD/bv34/p06cDAN59990Qz8g9TU1Nsq/TYDBg1KhRMBgMsq/bHRcvXkRVVRVuvPFGTJo0CaNGjUJKSkqH1tnc3Ixoa0vLfu8uXbpg1KhRIZ5N5MC/r8jBYrGgtbU11NPghAncIItxmAH2z3/+E6NHj8ann37q0vC5cOEC/vjHPyI3NxcajQY5OTm46aabUFZWJo2pqanBX/7yF3Tv3h1arRaZmZmYNm0ajh49CsB9eJC51+2f4GfOnImEhAQcOHAAU6ZMQWJiIiZNmgQAKCoqwvXXX48uXbpAp9OhZ8+euO+++1BRUeE076NHj+I3v/kNsrKyoNVq0bVrV9xxxx3SRdDdnPbs2YPrrrsOqamp0Ol0GDp0KP7zn/84jGlqasLDDz+M/Px86HQ6pKamYvjw4fjkk0/cft/PPPMMunTpAgD429/+BkEQkJeXJ73/7bffYtKkSUhMTERcXBxGjx6Nb775xmEdLMSwfv163H333cjIyEBcXJzHC3tdXZ00V41Gg86dO2Pu3LlobGyUxsyaNQs6nQ579+6VllmtVkyaNAlZWVkoKSlx2H5RURHuuusupKamIj4+Htdeey1OnTrltO0NGzZg0qRJMBgMiIuLw5gxY7Bx40an70UQBPz444+46aabkJKSgh49eji8Z09eXh5mzJiBr7/+GkOHDoVer0dBQQG+/vpraY4FBQWIj4/HiBEjsGfPHqd5efMbs33dvHkz7r//fqSnpyMtLQ2/+tWvcPHiRYf5HDp0CFu3boUgCA6/q9VqxXPPPYc+ffpAr9cjOTkZgwYNwiuvvOL29yKEICsrC3PmzJGWWSwWpKSkQKFQOJx3ixYtgkqlkrxUbb8vT3NjmEwmPP7448jJyYHBYMDkyZNx7Ngxt/Nj2/nrX/8KAMjPz5fWvWXLFvz1r39FUlISLBaLNP5Pf/oTBEHASy+9JC2rrKyEQqHAa6+9Ji0rLi7G73//e2RmZkKr1aKgoAAvv/wyrFarx/kA9Lt+8cUX0bdvX+kadMcdd+D8+fMux2/fvh2jRo2CXq9H586d8eSTTzrMGQCWLFmCwYMHIyEhAYmJiejbty8ee+wxhzGlpaW477770KVLF2g0GuTn5+PZZ5+F2WyWxrDr3IsvvojnnnsO+fn50Gq1+M9//gONRoMnn3zSaX5Hjx6FIAh49dVXAQDl5eWYPXs2+vXrh4SEBGRmZmLixInYvn272+/jH//4B7p27QqdTofhw4c7nXvu8Oa8LS8vl+4LWq0WGRkZGDNmDDZs2ODVNjhtIJyYpampiSQlJZHLL7+cEELI0qVLCQCyfPlyh3Hnz58n2dnZJD09nSxatIhs2LCBrFixgtx9993kyJEjhBBC6urqSP/+/Ul8fDxZsGABWbduHfnss8/In//8Z7Jp0yZCCCGbN28mAMjmzZsd1n/69GkCgLz33nvSsjvvvJOo1WqSl5dHFi5cSDZu3EjWrVtHCCFkyZIlZOHChWTVqlVk69at5P333yeDBw8mffr0IUajUVrHTz/9RBISEkheXh558803ycaNG8lHH31EbrnlFlJXV+d2Tps2bSIajYaMHTuWrFixgqxdu5bMnDnTaY733XcfiYuLI4sWLSKbN28mX3/9NfnnP/9JXnvtNbff+blz58jnn39OAJA//elPZOfOneTHH38khBCyZcsWolarybBhw8iKFSvIypUryZQpU4ggCOTTTz+V1vHee+8RAKRz587kj3/8I1mzZg353//+R8xms8ttNjY2kiFDhjj8fq+88gpJSkoiEydOJFarlRBCSHNzMxkyZAjp3r07qa6uJoQQ8tRTTxGFQkHWr1/vtP3c3Fxy9913kzVr1pC3336bZGZmktzcXOmzhBDy4YcfEkEQyA033EA+//xz8tVXX5EZM2YQpVJJNmzYII17+umnCQDSrVs38re//Y0UFRWRlStXOrxnT7du3UiXLl3IgAEDyCeffEJWr15NRo4cSdRqNXnqqafImDFjyOeff06++OIL0rt3b5KVlUWampp8/o3Zvnbv3p386U9/IuvWrSNLly4lKSkpZMKECdK4H3/8kXTv3p0MHTqU7Ny50+F3XbhwIVEqleTpp58mGzduJGvXriWLFy8mzzzzjNvjhBBCbrvtNtK7d2/p/7t27SIAiF6vJ//3f/8nLS8sLCQjRoxw+i69mRs7/vPy8sjvfvc78s0335BPPvmEdO3alfTq1cvtMUUIPZb/9Kc/EQDk888/l9ZdW1tL1q5dSwCQHTt2SOP79u1L9Ho9ufrqq6VlK1asIADI4cOHCSGEXLp0iXTu3JlkZGSQN998k6xdu5Y88MADBAC5//77PX5fhBDyxz/+kQAgDzzwAFm7di158803SUZGBsnNzSXl5eXSuHHjxpG0tDSSk5NDXn31VbJu3Try4IMPEgBkzpw50rhPPvlEOlfXr19PNmzYQN58803y4IMPSmNKSkpIbm4u6datG3nrrbfIhg0byN///nei1WrJzJkzpXHsOte5c2cyYcIE8r///Y+sX7+enD59mtx4440kNzeXWCwWh/155JFHiEajIRUVFYQQQo4ePUruv/9+8umnn5ItW7aQr7/+mtxzzz1EoVA4XMPYtnJzc8mVV15JPvvsM/Lf//6XXH755UStVjv8LuwYP336tLTM2/N26tSpJCMjg7z99ttky5YtZOXKleSpp55yuF5xvIcbZDHMBx98QACQN998kxBCSH19PUlISCBjx451GHf33XcTtVotXTRdsWDBAgKAFBUVuR3jq0EGgCxbtszjPlitVmIymcjZs2cJAPLll19K702cOJEkJyeTS5cu+TSnvn37kqFDhxKTyeQwdsaMGSQ7O1u6aA4YMIDccMMNHufnCra/L730ksPyUaNGkczMTFJfXy8tM5vNZMCAAaRLly6S4cQuoHfccYdX21u4cCFRKBTkhx9+cFj+v//9jwAgq1evlpadOHGCGAwGcsMNN5ANGzYQhUJBnnjiCYfPse3feOONDsu/++47AoA899xzhBBqCKamppJrr73WYZzFYiGDBw92aUQ89dRTTvN3Z5Dp9Xpy/vx5adlPP/1EAJDs7GzS2NgoLV+5ciUBQFatWiUt8/Y3Zvs6e/Zsh3EvvvgiAUBKSkqkZf379yfjxo1zmv+MGTPIkCFDnJa3B3tAKi4uJoQQ8txzz5G+ffuS6667jtx1112EEEKMRiOJj48njz32mPQ5V9+Xu7mx43/atGkOy//zn/8QAGTnzp0e5/jSSy853cwJob+9RqMhCxYsIITQhzoA5G9/+xvR6/WkpaWFEELIH/7wB5KTkyN97tFHHyUAyPfff++wvvvvv58IgkCOHTvmdi5Hjhxx+Vt9//33BIDDdzRu3Din6wWbj0KhIGfPniWEEPLAAw+Q5ORkj9/BfffdRxISEqTPMP71r38RAOTQoUOEENt536NHD4cHR0IIWbVqFQHg8OBjNptJTk4O+fWvf+1222azmZhMJjJp0iSH85FtKycnhzQ3N0vL6+rqSGpqKpk8ebK0rK1B5st5m5CQQObOnevx++F4Dw9ZxjDvvvsu9Ho9brvtNgBAQkICbr75Zmzfvh0nTpyQxq1ZswYTJkxAQUGB23WtWbMGvXv3xuTJk2Wd469//WunZZcuXcKsWbOQm5sLlUoFtVqNbt26AQCOHDkCgIYTt27diltuuQUZGRleb+/kyZM4evQofve73wEAzGaz9Ddt2jSUlJRIoZwRI0ZgzZo1ePTRR7FlyxY0Nzf7vZ+NjY34/vvvcdNNNyEhIUFarlQqcfvtt+P8+fNOISRX340rvv76awwYMABDhgxx2J+pU6c6hWt79uyJd955BytXrsSMGTMwduxYPPPMMy7Xy74jxujRo9GtWzds3rwZAE34rqqqwp133umwXavVimuuuQY//PCDQ8jUl30CgCFDhqBz587S/9nxOX78eMTFxTktP3v2LADffmPGdddd5/D/QYMGOazTEyNGjMD+/fsxe/ZsrFu3DnV1dV7tHzuXWPinqKgIV199NSZPnoyioiIAwM6dO9HY2Njh864j++eKuLg4XHHFFQ5zT05Oxl//+lcYjUZ8++23AOi+2c9906ZN6NevH0aMGOGwvpkzZ4IQgk2bNrndJjvuZs6c6bB8xIgRKCgocAq3JSYmOu33b3/7W1itVmzbtk36bE1NDX7zm9/gyy+/dJkW8fXXX2PChAnIyclxOJYKCwsBAFu3bnUYf91110GtVjssKywsRKdOnfDee+9Jy9atW4eLFy/i7rvvdhj75ptv4rLLLoNOp5Oufxs3bpSuffb86le/gk6nc9jna6+9Ftu2bXMKzTJ8OW9HjBiB5cuX47nnnsOuXbtgMplcrpPjHdwgi1FOnjyJbdu2Yfr06SCEoKamBjU1NbjpppsA2CovAZonwPKe3OHNGF+Ji4tzSra3Wq2YMmUKPv/8czzyyCPYuHEjdu/ejV27dgGAZBRVV1fDYrH4PCeWm/Pwww9DrVY7/M2ePRsApIvyq6++ir/97W9YuXIlJkyYgNTUVNxwww0Oxqy3VFdXgxCC7Oxsp/dycnIA0Hwbe1yNdbdPP//8s9P+JCYmghDidJOZPn06srKy0NLSgnnz5kGpVLpcb6dOnVwuY/Nk3+VNN93ktO0XXngBhBBUVVX5tU8AkJqa6vB/jUbjcXlLS4vDvLz5jRlpaWkO/9dqtQDglRE+f/58/Otf/8KuXbtQWFiItLQ0TJo0yWVemz3dunVDjx49sGHDBjQ1NWHnzp2SQcYM9A0bNkCv12P06NHtzsMTHdk/d0yePBm7du1CY2MjNmzYgIkTJyItLQ3Dhg3Dhg0bcPr0aZw+fdrBIKusrPTpHLCHvefu820/m5WV5TSOHdNs7O23345ly5bh7Nmz+PWvf43MzEyMHDlSMogBejx99dVXTsdS//79ATgfS67mp1KpcPvtt+OLL76QcgGXL1+O7OxsTJ06VRq3aNEi3H///Rg5ciQ+++wz7Nq1Cz/88AOuueYal7+Vu3PUaDSioaHB6T22P4B35+2KFStw5513YunSpbjiiiuQmpqKO+64A6WlpS7XzfGMKtQT4ISGZcuWgRCC//3vfy41iN5//30899xzUCqVyMjIcJsUy/BmDHtSa5t87uqpE4BL7amDBw9i//79WL58Oe68805p+cmTJx3GpaamQqlUtjuntqSnpwOgN9Ff/epXLsf06dMHABAfH49nn30Wzz77LMrKyiRv2bXXXisVMngLS9ZmifP2sORxNjeGt9pc6enp0Ov1DkZ22/ftmTVrFurr69G/f388+OCDGDt2rMsqUFcX3dLSUvTs2dNhva+99prbqr+2N8Vg6I358hvLgUqlwrx58zBv3jzU1NRgw4YNeOyxxzB16lScO3fOwZvXlkmTJuHLL7/E1q1bYbVaMX78eCQmJiInJwdFRUXYsGEDxo4dKxlQ4cSkSZPw5JNPYtu2bdi4cSOefvppafn69euRn58v/Z+Rlpbm0zlgDzMqS0pKnB7ELl686PRZ+8IIBjum7Q3Uu+66C3fddRcaGxuxbds2PP3005gxYwaOHz+Obt26IT09HYMGDcI//vEPl/NixiTD3TF+11134aWXXsKnn36KW2+9FatWrcLcuXMdHog++ugjjB8/HkuWLHH4bH19vct1ujtHNRqNgyfeHl/O2/T0dCxevBiLFy9GcXExVq1ahUcffRSXLl3C2rVrXX6W4x7uIYtBLBYL3n//ffTo0QObN292+vvLX/6CkpISrFmzBgB1p2/evNlj1VVhYSGOHz/uMaTAKrt+/vlnh+WrVq3yeu7sYtb2BvTWW285/F+v12PcuHH473//69bgc0WfPn3Qq1cv7N+/H8OHD3f5l5iY6PS5rKwszJw5E7/5zW9w7NgxnyU64uPjMXLkSHz++ecOT7pWqxUfffQRunTpgt69e/u0TsaMGTPwyy+/IC0tzeX+2FfcLV26FB999BH+/e9/Y9WqVaipqcFdd93lcr3/93//5/D/HTt24OzZsxg/fjwAYMyYMUhOTsbhw4fdfpfMexVM/P2N20Or1bbrUUpOTsZNN92EOXPmoKqqql0xzsmTJ6OsrAyLFy/GqFGjpHlNmjQJX3zxBX744QevwpXezM0fPHnSRowYAYPBgMWLF6O0tBRXX301ALpP+/btw3/+8x/069fPwWCZNGkSDh8+jB9//NFhXR988AEEQcCECRPczmXixIkAqNFizw8//IAjR444GH4ANWLaXns+/vhjKBQKXHXVVU7rj4+PR2FhIR5//HEYjUYcOnQIAD2/Dh48iB49erg8ltoaZO4oKCjAyJEj8d577+Hjjz9Ga2ur07knCILTte/nn392q9P3+eefS55hts9fffUVxo4d69bz7e9527VrVzzwwAO4+uqrnX4/jndwD1kMsmbNGly8eBEvvPCCdPO0Z8CAAfj3v/+Nd999FzNmzMCCBQuwZs0aXHXVVXjssccwcOBA1NTUYO3atZg3bx769u2LuXPnYsWKFbj++uvx6KOPYsSIEWhubsbWrVsxY8YMTJgwAZ06dcLkyZOxcOFCpKSkoFu3bti4cSM+//xzr+fet29f9OjRA48++igIIUhNTcVXX33lEEJgLFq0CFdeeSVGjhyJRx99FD179kRZWRlWrVqFt956y+1N96233kJhYSGmTp2KmTNnonPnzqiqqsKRI0fw448/4r///S8AYOTIkZgxYwYGDRqElJQUHDlyBB9++CGuuOIKj14PdyxcuBBXX301JkyYgIcffhgajQZvvPEGDh48iE8++cRv79HcuXPx2Wef4aqrrsJDDz2EQYMGwWq1ori4GOvXr8df/vIXjBw5EgcOHMCDDz6IO++8U7oRvPvuu7jpppuwePFizJ0712G9e/bswb333oubb74Z586dw+OPP47OnTtLYb+EhAS89tpruPPOO1FVVYWbbroJmZmZKC8vx/79+1FeXu70pB8svP2NfWHgwIH49NNPsWLFCnTv3h06nQ4DBw7EtddeiwEDBmD48OHIyMjA2bNnsXjxYnTr1g29evXyuM6JEydKEifPPvustHzy5MmSh9gbg8zd3DoKW8crr7yCO++8E2q1Gn369EFiYiKUSiXGjRuHr776Cvn5+ZKMyZgxY6DVarFx40Y8+OCDDut76KGH8MEHH2D69OlYsGABunXrhm+++QZvvPEG7r//fo8PJX369MEf//hHvPbaa1AoFCgsLMSZM2fw5JNPIjc3Fw899JDD+LS0NNx///0oLi5G7969sXr1arzzzju4//770bVrVwDAH/7wB+j1eowZMwbZ2dkoLS3FwoULkZSUhMsvvxwAsGDBAhQVFWH06NF48MEH0adPH7S0tODMmTNYvXo13nzzTa9TJ+6++27cd999uHjxIkaPHu3kqZ0xYwb+/ve/4+mnn8a4ceNw7NgxLFiwAPn5+Q4SGwylUomrr74a8+bNg9VqxQsvvIC6ujqHY6kt3p63tbW1mDBhAn7729+ib9++SExMxA8//IC1a9e69Txz2iF09QScUHHDDTcQjUbjsfrwtttuIyqVipSWlhJCaIn73XffTTp16kTUajXJyckht9xyCykrK5M+U11dTf785z+Trl27ErVaTTIzM8n06dPJ0aNHpTElJSXkpptuIqmpqSQpKYn8/ve/J3v27HFZZRkfH+9ybocPHyZXX301SUxMJCkpKeTmm28mxcXFBAB5+umnncbefPPNJC0tjWg0GtK1a1cyc+ZMqcrLXeXn/v37yS233EIyMzOJWq0mnTp1IhMnTpQqUgmhFWHDhw8nKSkpRKvVku7du5OHHnpIKlF3h7sqS0II2b59O5k4cSKJj48ner2ejBo1inz11VcOY1hVVNuqSU80NDSQJ554gvTp04doNBqSlJREBg4cSB566CFSWlpKGhoaSN++fUm/fv0cKhQJIWTOnDlErVZLlW9s++vXrye33347SU5OJnq9nkybNo2cOHHCadtbt24l06dPJ6mpqUStVpPOnTuT6dOnk//+97/SGFYZaC9N0PY9e7p160amT5/uNBZtZAsIcf99e/Mbu/uuXR03Z86cIVOmTCGJiYmShAchhLz88stk9OjRJD09XToG77nnHnLmzBmn+bti6NChBAD57rvvpGUXLlwgAEhaWppUfctw9X25mxvbD/vfwv47sz8n3TF//nySk5NDFAqF03fyyiuvEADkD3/4g8Nnrr76aqfKV8bZs2fJb3/7W5KWlkbUajXp06cPeemll5wkIVxhsVjICy+8QHr37k3UajVJT08nv//978m5c+ccxo0bN47079+fbNmyhQwfPpxotVqSnZ1NHnvsMYfK2/fff59MmDCBZGVlEY1GI133fv75Z4f1lZeXkwcffJDk5+cTtVpNUlNTybBhw8jjjz9OGhoaCCGez3tGbW0t0ev1BAB55513nN5vbW0lDz/8MOncuTPR6XTksssuIytXriR33nmn9Jvab+uFF14gzz77LOnSpQvRaDRk6NChknwQw5XsBSHtn7ctLS1k1qxZZNCgQcRgMBC9Xk/69OlDnn76aadrCMc7BEKiTN6bw+EElOXLl+Ouu+7CDz/8gOHDh4d6OhwOhxMV8BwyDofD4XA4nBDDDTIOh8PhcDicEMNDlhwOh8PhcDghhnvIOBwOh8PhcEIMN8g4HA6Hw+FwQgw3yDgcDofD4XBCTEwJw1qtVly8eBGJiYlBadHC4XA4HA4ndiGEoL6+Hjk5OVAoPPvAYsogu3jxInJzc0M9DQ6Hw+FwODHEuXPn2u3YEFMGGWuVc+7cORgMhhDPhsPhcDgcTjRTV1eH3Nxcr/rjxpRBxsKUBoOBG2QcDofD4XCCgjdpUjypn8PhcDgcDifEcIOMw+FwOBwOJ8Rwg4zD4XA4HA4nxHCDjMPhcDgcDifEcIOMw+FwOBwOJ8Rwg4zD4XA4HA4nxHCDjMPhcDgcDifEcIOMw+FwOBwOJ8Rwg4zD4XA4HA4nxHCDjMPhcDgcDifE+GyQbdu2Dddeey1ycnIgCAJWrlzZ7me2bt2KYcOGQafToXv37njzzTedxnz22Wfo168ftFot+vXrhy+++MJpzBtvvIH8/HzodDoMGzYM27dv93X6HA6Hw+FwOGGHzwZZY2MjBg8ejH//+99ejT99+jSmTZuGsWPHYt++fXjsscfw4IMP4rPPPpPG7Ny5E7feeituv/127N+/H7fffjtuueUWfP/999KYFStWYO7cuXj88cexb98+jB07FoWFhSguLvZ1FzgcDofD4XDCCoEQQvz+sCDgiy++wA033OB2zN/+9jesWrUKR44ckZbNmjUL+/fvx86dOwEAt956K+rq6rBmzRppzDXXXIOUlBR88sknAICRI0fisssuw5IlS6QxBQUFuOGGG7Bw4UKv5ltXV4ekpCTU1tby5uIcDofD4XACii92hyrQk9m5cyemTJnisGzq1Kl49913YTKZoFarsXPnTjz00ENOYxYvXgwAMBqN2Lt3Lx599FGHMVOmTMGOHTvcbru1tRWtra3S/+vq6jq4N57Zs+jXSGs4Kdv6GjTpyPvj/yExtZNs6+QEl93vPAhlaw2GzloGhSrgp1vQIVYrflhyLwRtIi6/ZzEgCKGekuy0NtXi0Ft3w5wzHCNunR/q6Ug0N9Th+Os3w9BaKsv6rIKA6oLfY/hND8uyPjmouHAKZe/fiThzYK/docasUKN1/FMYcOV1oZ5KQDi1bzNa1j4N3TXPoPvQiaGeTtgS8DtEaWkpsrKyHJZlZWXBbDajoqIC2dnZbseUltILTUVFBSwWi8cxrli4cCGeffZZmfakfQxNxci3npFvhS1ncPo/f0birBXyrZMTNOrOH8GIC+8DAM5uegvdpswJ8Yzkp6z4KEaU0/SD8r0TkDH8htBOKACc/fJ5XFa7AQ213wHk0bAxOk9+/xUGN++SdZ3Ggwthmngz1KndZF2vv5R/8Sj6G38O9TQCjxX4adfbQDQaZKZmJHxzP7qbS3Dpm9nAgH2AWh/qWYUlQXlkF9pcwFiU1H65qzFtl3kzxp758+dj3rx50v/r6uqQm5vr2+R9YdpLONBQK8uqjp/6Bdef/jvyS9cCR74GCmbIsl5O8Kjb9wWYgzp990vAVb8HdEkhnZPcNF86I71Wb3wSGDINUGlCNyG5qTmHvOPLAAAJaIalvgxKQ5h4rCtOAAD2qYdCNfahdga3j2XT8xiCoyj9+ll0umNZh9fXYUp+RkHFOgDAml7PoktufognFBiqTuzEuHNLkNRaEuqpBITWzS8h00z3LdNcgtbNL0I75ekQzyo8CbhB1qlTJycv1qVLl6BSqZCWluZxDPOIpaenQ6lUehzjCq1WC61WK8dueEXvy8bLtq7kAU14e9E+zFatgvXrh6DIGwPoU2RbPyfwaH9ZCwAwEwXizdUgW1+CMPW5EM9KXsw156TXyc3FwO63gdEPhHBG8mJe/zQ0xCj9v+HCESSFiUGmqfkFAFCaNASFV13f4fW9UdyMISfvQ+apL4BLfwUyCzq8zo7QuOZJxAP4yjoaV/1qDpL06pDOJ1DsVKcC55YgzVwW6qnIT8UJqHe+AgBYYR6PW1VboN75KjD0NiCjT4gnF34EXIfsiiuuQFFRkcOy9evXY/jw4VCr1R7HjB49GgCg0WgwbNgwpzFFRUXSmGgjNzUOG7Puwi/WbCgaLwHrHg/1lDi+UF+K9BoaanncfA8AgHz/JlD5SyhnJTuk9gIAoIJQX6B1ywtAY0UopyQf53ZDdfgzWImAM1b64NdSejTEk7IRX38aANCY2F2W9Q0aORlrLJdDASusG4KX6uGS09sQX7wFJqLE9i73Ra0xBgCadBoeNpB6oCWKcuUIAb6ZBwUxY5NlCJ5T3Y8NlqFQEDPw9Tz6PscBnw2yhoYG/PTTT/jpp58AUFmLn376SZKfmD9/Pu644w5p/KxZs3D27FnMmzcPR44cwbJly/Duu+/i4YdtiaN//vOfsX79erzwwgs4evQoXnjhBWzYsAFz586VxsybNw9Lly7FsmXLcOTIETz00EMoLi7GrFmz/Nz18GfywG74q+k+WCEAP/0fcGJDqKfE8ZZjqyGAYJ+1J1YpJmOLZTAUVhOw/olQz0xWlPXUIPvYMhEHrXlQGOuATVHgBbRagbU0gf+/lnHYZB0KALCUnwjlrBxIaabXXEuKPAbZyO6peFv1O1iIAMXxNcDZnbKs12cIATY8A4AeV8MvGxaaeQSJREMKqkgC/U/tOc+DI4kD/wVOb0MLUeNp8514ckZ/PG2aiSaiBc5+C/z0cahnGHb4bJDt2bMHQ4cOxdCh9AI1b948DB06FE899RQAoKSkxEEbLD8/H6tXr8aWLVswZMgQ/P3vf8err76KX//619KY0aNH49NPP8V7772HQYMGYfny5VixYgVGjhwpjbn11luxePFiLFiwAEOGDMG2bduwevVqdOsWHsmngaBwQCf8SHpjueUauuCrP0fXE1Q0c/QbAMB6y3D8flRX/N38e5ihAI6tBn7ZHOLJyYe6keaGlCIDz5rEB7Ef3wdKD4ZwVjJw8H/AhT1oJDr8y3wzSlSdAQDKqjDxcDZVId5C81WF9J6yrFKtVKB3/2FYYRlPF2x4JjRejCOrgAt70Ui0eMP6K0zp5z4tJRow6NQ4TzIAAKT6bIhnIxPNNcC6xwAAr5lvhCI1HzcP6wJlaje8Yv4VHbP+CaCpKnRzDEN8NsjGjx8PQojT3/LlywEAy5cvx5YtWxw+M27cOPz4449obW3F6dOnXXq1brrpJhw9ehRGoxFHjhzBr371K6cxs2fPxpkzZ9Da2oq9e/fiqquu8nX6EUVeejz6ZRvwkulmNMR1AerOS0+OnDCmpRbk1FYAwHrrMNw7tjsuqrriQ/PV9P11jwEWcwgnKB/6JprX2Sm3J/agL762jASIFVg3P3JDEsYm6Tx73XwdNMk50GX3BQDo606HcGJ2VFJ5nYskFUlJybKttnBgJ7xi/jVaoAHO7QKOr5Vt3V5hMQMb/w4AWGqZjt49eiA5LoqKRFyQpLcZZK0VZ0I7GbnY9HegsRyl6q54xzIdhQOyIQgCCgd2wruWQlzQ5APNVUDRk6GeaVjBe1mGOdMGdkIzdPh3woN0wZ53gdO8ZVRYc6IIgtWEk9YcNCR2R5ZBhwl9M7DY/Gs0qwzApcPAj8tDPUtZSGiliciGrDwM75aCf5p/C7NCA5zeJnkJI44drwF1F1CpysK7lmmYNrATjEk0LJjQdB4wG9tZQRAQKyxPWbORFi+fwTK6RzqadZl4zzyVLtjwLGC1yLb+dvnpI6DyBGoFA94xT8P0gdnB23aI0KkVKAE1yEyVZ0I7GTk4vxf44V0AwN9a7oQRaul3nD4wG2ao8HDzXXTsvo+As+61RGMNbpCFOdPEA3np+Vy0Dr6TLlz1AGBsDOGsOB45+jUAYL11OHplJgIACgdkoxYJeFtxKx2z6R9Ac3WoZigPrfXQWxvo66QuKByQjfMkA6v0diEJc6v7z4cjdReB7xYDAJ43/Qat0KBwYDZUyZ3RRLRQwAJUnwnpFAEAldQgO02ykSqjQaZRKTClfycsMV+LZmUiUH4E2P+pbOv3iLEJ2Ey7riw2Xo9mRTym9A+PitZAIggCKtV0P0l1hLcCtFqAbx4CQHCh63XYaipAlxQ9BnSmRT8DOyehc7IeO009cS7/ZvqZrx8Kj4ecMIAbZGFO94wE9O2UCLOVYE32bMDQmd4QNv0j1FPjuMLcCpyg1cDrLMPRK4sm607smwmtSoHX6saiNbkXdddvfSmUM+04YoVlLYlDfGISrhlAbypPVk2BJT4LqD4NfP9mKGfoOxsXAKYm1KRfhs9aL0d2kg5DuiQjNUGH00Q0Dirl68bhLxbmISPZSIuXV9pn2sBOqEMCluIGumDz84CpRdZtuOT7N4GGUtTpcvB/lskY1T1VVmMznKnT0mNLURfhSf0/LAVK9gO6JLyhngmAOhWYXqggCJg2kO7r68rbgbh0oPwosNO73tjRDjfIIoDCAdRLtupoPXAt1XTBrjeAc7tDOCuOS05tBYwNqFam4WfSXfKQxWtVGN8nA2ao8FW2qNO1+y0p9BSR1J4HAFwkaUjSq5GTrMfQrsloJDrszBf3cetLQMOlEE7SBy7sBfbT3rnvJfwRgIBrBnSCQiEgLV6DU0QMn1WG/jezitWeZ5EDg15eOckxPdORqFXh342TYIzLprmrPyyVdRtONFUB3y4GACxV/QZGqKXrXizQoM8BAGgazod4Jh2grkTK/zONfworT5gA0OI0ewrFqM9Xx5thnEzHY+uL4eF5DjHcIIsA2BPF9hPlqMsdDwz+LQACfDknOE+uHO8Rw5WbcDkIFJKHDLCFn984nwfSaypgNUe2vlwdvXmUiAYZAClX5PXKYUDOUMBYTxN8wx1CgLW0Kswy8FYsO5MKwLY/qfEanCL0phlyI9pqgbLmDACgUt/NY7cSf9CqlJjcLwut0GBtxky6cPu/gBZ5upC45NtFQGstjOn98FrFUCgEYGoMhCsZLfG0ildjrIncSvp1j9HzvfMwbE6YhkajBTlJOgzJTXYYNjQ3GTlJOjQaLdiimQDkjQXMzcA3D0duIZBMcIMsAuiVlYiemQkwWQg2HikDpv4DSMgCKo4DW18I9fQ4DKuFyloA+KKZysL0zLAZZBP7ZkKjVOBUeSNOD3sMUKiAE+uAkxGqLyeGLEtIGpLiqEHGwpbfn6lB9VUL6LgfP6RhjHDm0Be0qlAdh+/z56C+xYzMRC0u60q7Y6TGa/CLlXnIQhyyrCmGwtKKVqKGKT4nIJtgXo0XSy4DSe9D8x2/eyUg20LteeD7twEAm3JmgUCBEfmpyEgMXpeVUKONT0Z1JGuRndwIHPocEBTAjP+HNYeoV7zQLlzJEAQB14jezzWHyoAZ/w9QaoCTRcDhL4M+9XCCG2QRAvOurD5QCsSlAtMX0Te+ewW4uC+EM+NInP8BaCyHRWPA99YCpCdokWKXA5OoU+Oq3ukAgC/PxQEj/kjfWPd4RMpgkDpbyNKgowZZl5Q4DO6SBCsBVtd2Awb8GoDofQrXp19TC1Ak9tYb82d8IUqNFYrhSgBIS9DgtBiyJKE2yMRuD2dIFlISA9Ok+areGYjXKHG+zoTTg8V+wDvfAOpLPX/QH7YsBCytQLcr8VZJDwC2612sQKUv6LUBNRGW2G9qAVaLQu8j7kNrxgBsOEyrr1l0py1s+YbDZWhN7g6MmUvfWPto5HoIZYAbZBECO4C3Hi9HQ6uZNhvv/yuAWIAvH+BVKuGAGK48n3EVTFChV2aC0xB2o1lzsAQY9wigT6VJrXvCoJmzj1hrWMgy1aG1TaH08FACTH4WUOmpMveRVSGZZ7vseh2oLQYMnWEa9QDWizeTQjujIDVeIyX1C43lVPgyVFTaEvpTZU7oZ+jUSkwqoIKsn9QOBLqMoGEluT3yl45Kiu3lo+Zj37laCDEWrgQctcgiziD79v8BVaeAxGxgwmP47mQF6lvNyDJoMTTXdf/ly7qmIMugRX2rGd+drADG/gVI7Q7UlwCbY7dgjRtkEUKfrER0T4+H0WylYUsAmPYSEJcGlB2kJwUndBACHKEG2V497a9qnz/GmFSQBbVSwPGyBpysVwETxRyyLc9HnGo162N5SUhHnEYpLZ8mhiN2napCpSoTGCNq6K1/IvxyHuvLgO2it3nS09hR3IzaZhPSE7S4PC9VGqZVKQGtAWUkmS4IpZfMocIycFWI7CFw9cEykMmiB3Hv+0CFjPu+cQEVEu47A6sqaR7V8G4pyDLo5NtGBBCxBlnlLzT/DwCmPg/oDPjmZ+pFLRyQLXmY26JQCFLRxjc/lwJqHTD9Zfrm7rdjNurDDbIIgakcA8CaA2LYID4dKHyRvt72ElB2OESz4+DSESrzoNRig3EgALj0kCXp1biyJw1NrD5QClw2E8jsR3N0IikfkBAoxD6W9dpODnkiXdPiMKCzARYrQdHhMmDMn4HEHHqj2fV6qGbsmk1/B4wNQOdhwMCbseYAbQV1zYAsKNvcTFLjNThlDYPE/gBpkLVlXO9M6NVKXKhpxgHVAKDXVOqRl6tIo/h74Ng3NO9o0lPSdx9r4UoAMESiQSY2D4fFCPSYBPS/EUazFUWH6f2pvd+R5SkWHS6F0WwFekwEBtxEDfSvHwquIHGYwA2yCIId4JuPXUJjq5hzNODXQJ/pgNVEqy4jMBcpKhDDlegxAQcr6IWkV1aiy6EOIT2lij5ZAsDud4DyYwGfqiw0V0Nhpt6uFr1zeIk9/a4+WApo4oHJz9A3ti8KTB6SP5T8TJXCAWDqQpgJsO6QeDNxIblgH7YMqYdMzCE7ZQ2sQabXKDGxIBOA+PAw+WkAAnB4JZUI6Qh2DcQx9Pco1XTDnrNUKPmaAbEVrgQiNIfs4GfAqS2AUkujNYKAHb9UoK7FjIxELYZ1cx2uZAzPS0V6ghZ1LWbs+KWCLpz6PKBNoh4yUe0/luAGWQTRL9uAbmlxaDVbseVYOV0oCNTVq0sCLv4Yfh6IWEE0yIy9puFcdRMA1x4yAJjSLwsqhYCjpfU4Vd4A9JgA9JlGvQ9iQ96wR9QgKycG6OPinN5mT787TlagpskIDLwZ6DyceqM2hoEMBiHA2vkACH2o6ToS35+uQnWTCanxGozIT3X6SFq8Br+EWovM2AjUUc/kLyQnoCFLwGaYrj5QApLZDxh8G32jo43Hj68DincAKh0wfr5kCF/WNRnZSYEpVAhn7BuMR4RB1lJru1aN/QuQRosxWPTmmv6dnDzMbVEqBFwzIMvhc0jMAiY/RV9vXEC1zWIIbpBFEIJgi7uvPmB3oBqygam05Qg2/SP0OkmxRs05KusgKPBLylgQQr0paQmuE66T4zQYLYYt1xwUL0RTngMUaiqBISr9hzV1NskLg11CP8O+w8T6w2WAQgFc80/65k//F/ockaNf00IDlU7y3n0jnlNT+2dBpXS+NDpqkYXIQyZ65mpgQC0SAq5kP75PBnRqBYqrmnDoYh0w4TEqUXB6G/DLJv9WarUAG5+lr0feBxhypOtZLIYrAeohu8A8ZM1VQGt9aCfUHpueAxrKgLSewJVzAQAmixXrxHBloZvqyrYwg3/d4VKYLFa6cNjd4sNbPbBuvuxTD2e4QRZhsETbTUcvodloF2Mf8lsax7e00qpLqzVEM4xBWBPt3FE4Vk+NsJ5uvGOMaaIHac1B0bBO60FvTgB98rSYAjJV2ah1FoVtCxNVZblByL0cGHgLAAKseTR0MhjmVmD9k/T1FQ8AyV1hsRKsO+g59yU1wU6tv+qX0JxjokF2GnQeaQmBNcjitSqM703DlmsOlgDJXYHL76VvbnjGv+/g5/8Alw5Tr/6VD+FSfQt2n6EFLYWxapDFqdGAOJsWWU0Ya5Fd3Gfr3DD9ZUBFr3m7TlWipsmEtHgNRuQ5e5hdMSI/FWnxGtQ0mbDrVCVdqKBaZhCUVB/wRITqNPoBN8gijIGdk9AlRY9mkwVbj9u1pBEE2lZJk0AFLn94J3STjDVY/ljBDBwvo0+27sKVjCmiS//ghToUV9IQJ8Y9Qnu7VRwPfKuajuLQNsl16x52c/32ZAVqm0UDc/IzgDqOHqOHvgjGTJ35/i1agJGQBVz5EABg9+kqVDYakRynxqjuaS4/lhavwXlC21/B3CJ1KggqomfuhJka9IGSvbCHeTtWHygFIQQY+zCgSQRKf6ZioL5garHJGlw5D9CnYN2hMhACDM5NRufk2AtXApAeasI+j8xqoQn3xEoT8LuPl95aLYYdpw7o5NLD7AqVUiE1kGefBwBkDwJG3U9ffzMPMDXLMv1whxtkEQZtzmonEmtPci5wtRgK2PAM7w0WDJqqgLM76Ou+03HiUgOA9g2y1HgNRnWnT5GrmZdMl2Qng7EQaKwMyJRlQQpZprr1kPXMTEDvLLsOEwCQ1NkmAln0VPAvtI0VtCIZACY9BWjp78Q8lVP6ZUHt5maSGq+FBUqUqUQvTihSA+w0yBQCkOzmu5eTSQVZ0KgUOF3RiGNl9UB8Gq2cBWjFpS8aiHvepUr0iTmSR1iqrozBZH5GvEYJpUII/zyyPcuoh0ybZCtGAmC2WLHeQ0GMJ1jUZ/2hUpgtdh7X8fMBQ2eg5qztnI1yuEEWgbCE6Y1HytBialMaPOxuoNuVgKkJWPWn8FVHjxaOr6XJ+FkDgZQ8nGQGmZsKS3umtQ3pAcBldwJZA2jS7JaFAZmyLNi3TfJgFLjMeRz9J8DQhd6Yd/w7oNN0YvM/gNY6oNMgsScsYLESKZfPU8iMhQeLBaqXFZJKSzsNspQ4jVudJzlJ0Kowrjc1FFb/LP6OV8wG4jPpQ9+P73u3opZaYNu/6OvxjwJqPSobWqVQVSw1E2+LIAhttMjOhnZCrqgvo4n2ADDpSZqAL8I8zClxaozs7l24kjGqexqS49SobDRi92k7LUZtgk3W6btXqYhwlMMNsghkiF1z1m3Hyx3fVCiA616l6uint3l/seT4hygGi77T0WKy4GxlI4D2PWQAMKVfJygEYP/5WpwXKzOhUALXiIbYnmVU3ywcqbMPWbo3yKYPojfZbccrUN8ihi01cTZP7reLgldJVXYY2Lucvr5mIT1XAOw9W43y+lYYdCqM6ZHu9uOsovGkJUTSF4TYJC8CrEHWFptIrOiV18TTEDtA9fNaG9pfyY7XaMJ6em9gyO8AAOsPl8FKgAGdDeia5lytG0sYdKrw9pCtf5w+zOQMBYbf7fDWasnD3Mmth9kdaqUCU/ux46vNtaDvdKB3IZV1+mZe1DsYuEEWgTg0Zz1Y6jwgrQd9ggGAdU9I+T4cmTE22SrNCmbgdEUjrIReWL1pjJyRqJXkFdbYh5/zrwL6zqCet7Xzw+8iZLVKRtRFku7RIOuVmYAeGfEwWqzYdNQu53HAr4HckdSTyyruAgkhtFiCWIGCa4G8K6W3mPfu6n6doFG5vyQyA+iIkSa5Bz1k2VAGGOtBoEAxyQqqQTapIAsapQInLzXghJgniWEzabubxnJgZztyO/WltjGTnqL6e0DMV1faE9ZaZKe2AAf+KzUPh8LWmcNiJVh7kLUb8y/szD639mAZLFa7650gANNepHmnZ7+T2mxFK9wgi1CmD7Jrzmp2oWg8chbQ5XJaOvz1Q+F3U48GftlI+/sldwWyBkj5Y72zEh2U6z0h5QO2fTKc8ncqL3BqM9VsCicaLwFWEyxQ4BKSpcbirnDMeSyxf8PmCdz/CXC+g0Kj7XFiPf0ulRrg6gXSYquVYK1UXen5ZpImJtAfD5WHTDQA6/WdYYQ64BWW9hh0aoztZddhAgCUamDiE/T1jldpfp47tr5Ije8ul9OHDQDVjUbs+IWHKxkGvRoXwtFDZm4FvvkLfX35vdRDZseeM1WoaGhFkl6NMT3de5g9MbpHOgw6FSoaWrHnTJsWcsldaYgboO3Xwjm3toNwgyxCGZrbpjlrWxRK4PrX6Q3oxHpaas6RFyZ30XcGIAg4ySosXfSwdMc1/TtBEIB9xTW4WGOX4J7a3VZltO6x8GoeL+aPVSAFFihd6pDZw262W46V2zpMALRd0eDf0NdrAyiDYTEB68RiiVH30+9WZN+5apTWtSBRq8KVvTzfTPQaJfRqpU2LrPYc9ZIGCzGhv0LbFQCC6iED2nSYYPS7EcgeTAV/WX5YWyp/saVOTH6GGuMAig5Tb0hBtgH56fEBnHlkELZaZN+9Qh8+ErJsBrgdLEpztYeCmPbQqBS4uh+TAnIR9Rk1G8jsT7+Xoqf82kYkwA2yCMWpOasrMvrYnizWPOL5CZbjGxYzcGwNfS0+8TMPWc/M9hP6GZkGHS7vRsOWa9teiMY+DMRnUM2r3W93fM5yIeaPXSB03p5ClgBQkJ2IPLHDhEPYEgAmPQ2o44Hzu2krlkDww7vUmIlLp9+pHczbM7lfFm0g3g6p8RpUIRFmTRJdUHVK9um6RcwfK1F1FucSeMkLe64uoB0mjpXVS8UrUCiAyWLI+Yelriu7Nz0HWM1ArymOoeKDvLrSniS9GvWIQ4vKQBeEgxZZ5S82Q3vq87QS3A6rlUgVyu15mNuDfX7NwRJYrW0ezpRqGioFgJ8+As5816FthSvcIItgnJqzumL0g0CngUBLDfDt/wve5KKds9/R7zQuDeg6CgC8lrxoS6HdhcgBnQGYKOYCbn0xfAxqpkFmpXpdSXGeDTL7sKXTPhqygbFUCwxFT8nvcWqqslWrTnyCfqcihBCpwrXQS6OAhgkFNCbk0QXBbKEkhizPCtRDF+i2SW1JirOFpNba/449JlA9KqsJ2Py844cu7hO1ygRqfIvUNpkkz/60QTxcCdgebKrV4rEY6rAlIcDqv1Kx8e7jad5nG34srkZZXSsStSq/w5WMK3ulI1GrQlldK34srnYe0HUkzVsEaBpOOEUNZIIbZBGMy+asbVGqbY2df1gK1F0M2vyCyoW9No9VMGDhyj6FgEIJo9mKMxVihaUPIUvA1kx5z9lqlNW1OL459PfUoG6ttQlqhhoxZHmRpEEhAAka18Kw9jCDbPPRcjQZzY5vXvEAkNSVapvteFXeuW75JzWcM/sDl93h8Nb+87W4WNuCeI0SV4myDu3BwoTV+m50QTBbKInG3wlLtsNcgsk0O5FYB9g15uf/AKUHbctZA/FBtwCdBkiLi46UwWQh6JOViB4Zvp0v0QoL/ZerRDmJUBtkx9bQPFmlBpj2shRqtsdXD7MntColJvfLclivE5OeFsWzj9EWbFEGN8giGKVCkJ7s17g7gAHaUqnrFVRdfPvLQZpdEDG1AB/cCHxym81QCiSEOOaPAThT2QizlSBBq0Ing86n1WUn6XFZ12Ta77pt2FKhtPWA3Lvc8WYXKupY26RUGPRqr7Sw+ucYkJtKO0xsOdZGqkWtt5PBWCxfVXD5MVvHg2ued6gMA2y5UBMLsqBTe3czYUZQmSaXLgiWh8xsBKqpNtVhscozmEn9jCn9aIeJwyV10gMIAJro3f9GAMRWNfvLJlqdp9QAEx53WI/kmexgmCuaYB6yUkGs4g21FtlJsafusJlAek+ntx3DlfJ4OQsHeAhbAkBcqq3BfSh0AANM+4+2nLCmcGAnfLjrLNYdLsVzlgGukyoFgYZrlk8H9r5Pw5gp3YI/2UBxajP1IAHA1/OAbmMAfXLgtlfyEzVK1PFS65ATZSx/LMHrCkt7pg3Mxo/FNVh9oAR3js5zfDPvSqDgOuDIKprgf+eqjs2/o9h5yDxVWNojCAKmDcjGW9tOYfWBEucLeP8bgd3vAMU7gA9vBFLyqQGlUAIKFe1rp1D5tuz4Oiod0rvQocULQMOVq/1QiGdhwvOKHIwEgndTqD5D90WTgBNNCQBMUtVnMEmJ12B0jzRsP1GB1QdLMHu83Y164pPAka9oEdHp7Tbv2PB7HK43dS0mbD8hhiu53IWErX2S6K2tDXEOGZN1ybnM5ds/na9BiehhHttOQYy3XNU7A/EaJUpqW/DT+Rpc1jXFeRC7trfUyrLNcIIbZBHOiDzanLWy0Yhdpyoxtpeb0EvelfSmdGoLsO1FWoEZLRz5yva6oZQKGAZy/5gYbM9J1LsD4MQl73pYuqNwYDae++YIdp+pQnl9q7OO2ZS/0/08vZXmksXLcwH0C6ltkmdR2LZMG0gNsk1HL6HFZHH0SjEZjHcm0F6eFcflmatCBUx5zmnxwQt1OF/dDL1aifF9Mr1eHUukl5qMV5ykHlM/jHCfED1xJK0Hqs+YxLkE30MG0KrZ7ScqsOZAqaNBltaDdprY8y7wnztoRZwmEbjKsZBi05FLMFqsYmst7wtgoh12Lp0xi71UQx2yZOdgem+XbzMv5yQfPMztoVMrMakgC6v2X8SaAyWuDTKtWFjQWifLNsMJbpBFOKw56ye7i7H6QKl7gwwAJjxBDbKfPgHGPOTSDR1xWEzAsdX09cQngE3/APZ9RBNQe0wMzDbbhCsBOGiQ+UPnZD0G5yZj/7karDtUit+PauPBTMmjfd3qztPKvlAZZBYTFfkEFYXt64NBNqhLEjon63Ghphlbj5djav82nqmcIcA9G+iNwGqmHiGrmQrRWs1tllnEP7bcarfcblnPyS6Pc1bhN7FvJvQa728mzEN2zJgJQKCe2cZyIMF7o84vRG9Fa1IPSR0kpZ1iikAxpX8Wnlh5AAcu1OJcVRNyU+0U9sf9jerKNYtaUqP/5HSsfsN7V7qEGWS/mMTWQ6E0yJprqBAxAKT3cnqbepi90+/zlWkDO2HV/otYfaAUj00rcI44sErPFm6QccKQaQOpQbb+UCn+fn1/qNxpweReDvS+hvZf3LIQuOnd4E40EJz9DmiupomeV84DGi5RiYhVfwZm75SaR8tG5S9A+RHqeek9RVp8koUsfUzot2fagE7Yf46GLZ0MMgBIzbcZZLkj/N5Oh6gvAUBgEdSoRKJPHjJBoDmPS789jTUHSpwNMgDoMoz+BRCH6kofbyZSDlmzACTn0ptmxYnAG2Sih6xBrO5MjlO7P88DTHqCFiPz07DzVCVWHyjBfeN62N5MzAKumEObQcdn0Nd2NLSasVVs9+apb2gsws6lY60pgBJAUyVtSSX3NcwbWCg+MduhMplx4EItLtRQD/O43vIe++N6Z0KvVuJCTTMOXKjFoC7JjgPYfKLQQ8aT+qOAUd3TkOKqOasrWHLtwc+AskOBn1ygYeHKvtNo3tCkp2nFXm1xYFryHBXDlXlXAnrqTjdbrDhV4Z/khT0sn2bXqUpUNrQ6D2CCpqIeVUgQ88fqtZkgULQrCtsWdhPecISGLUPB4ZI6nKlsglalwAQfwpUAkCom0lc1GoE00fMWjDwy8Tev0oVGFLYtTKpitSsRzyvnAVf9FbjlQydjYtPRSzCarchPj0ffTjxcaQ87lypMOhBdMl0YqjwyKVzp7B0DbF7OiQW+eZi9Qa9RYmLfTIftOKAVDbIozCHjBlkUoFYqMMVdc9a2ZA8C+t0AgDhrBkUaVqstn6vgOvqvNgG47hX6evfbwNmd8m7TRbjybFUTTBaCOI0SOUl6v1edmxqHgZ2TYCW06bITaaInIphipG0R88dqVPSC6YuHDACG5iYjO0mHhlYzvj0RGl01VpE8oU8m4rW+BQlYyLKysRVIE29Wwai0FEOWZepch3mEiqn9syAIwP5zNThf3UY7ThNH0we6XeH0uTVS78pOfhW/RDOJWpWUimg2iFW8oQpbesgfox5mMVwZoJZXkm7hgVKQth08mIcsCkOW3CCLEtw2Z3XFhMdok9ijX1Phxkjlwh6axK810IbcjB4TqX4XAKx6ADA1u/68r9SXAed209d9p0uL7SssvZGA8EShpPPkwrBmHrJQGmTiE3uFkuYq+mqQKRSCpLvW7sNDALCvrvRHcoF5plpMVhiTxd8j0FpkzdVAEzVezytyHOYRKjITdbg8z02HCTc0Gc3YfIx2auC9K51RKAQkig8IrfG0G0PoDDLxIcOFQXboYh2Kq5qgUyswvo93+n2+Mr5PBnRqBYqrmnDoYhvDS8tDlpwwx2Nz1rZk9AEG3kJfbwoTsVF/OCLKP/SeCqjaViX+A0joRMNJW/4pz/aOrQZAaA9GQ460+ITYw7JnB8KVDHaj2vFLJaob2yhRh4VBRj1kZaCVYAa972mo7Om36HCZ+w4TAeJ4WQNOVTRCo1JIYRFfSNCqoBFzt2ri8ujCQHvImMGXmIOyVvp9B7ttkiumSZpR3hlkm4+Wo8VkRdfUOPTPcc5L4ti6XjTGMYMsRFpkHkKWTHtsfG/fPczeEq9VYbyYm+bcwURM6je3RJ1aPzfIogSNilZbAl5eIMf/jWo2nSwCincFeHYBgBBb/ljBtc7v65OBGYvo6x2vARd+7Pg2Wf6YnXcMsG+Z1PGcmPz0eBRkG2CxEhS1DVum5NN/W2poS6BQUGfTIAN895ABwLCuKchM1KK+xSy1zwkWLCflql4ZSPRSQ80eQRAk71SFVgwrVZ+h1aeBguWopfWguWsIfcgSsOUD7j1bjZLa9r3QzCNayMOVbmHnU61W9CCGwkNmMdke+tp4yByqKwPc8qrQriuEQ9hSa3edjTIvGTfIogiPzVnbktrdFtbb5KzTFPaUHaQ3QpWOShu4ou90Kn9BLMCXD3TsaaqlDji1VVzvDIe3/O1h6Y7pA92E9DRxQKLomQuVl0xU0T9noQUN/hhkDmFLV6HZAMJymKYP8r9UnynklyENUOmpxEZ1AD0ZzAOX3guVokEW6pAlAGQZdBjejR4H7YUtm40WbBYbywcq7ygaYOdTlSqE/Syrz9BjWh1vu96IHC2tx+kOeJh9YVJBFjQqBU5XNOJoab3tDYWS6tsBUZfYzw2yKGJMz3aas7blqr/StiZntlN9skiCecd6TgY08e7HFb5IG4BfOtSx5uoni2jz5LReNOQrYrES/FIuGmQdkLywh3kevjtZgdqmNp6XUIctRQ/ZaWMyAP8MMsAWtlx/uAwmS3DClifK6nHiUgPUSgGTCrL8Xg8zhiqbzHaVlgEMW7J8nrReqGoQPWQhaJvkikK75GtPbD1ejiajBZ2T9RjUJSkYU4tI2Pl0ScnaJ4XAIJPClT0BhaOJwB5oxvXOQEKAwpWMBK0K48Qes2vaPrjporPSkhtkUYRXzVntSc4Fht1FX296DmhbzRLOeApX2hOfTo0ygGojlR32c3uuw5XnqppgNFuhUyvQJSXOxQd9p0dGAvpkJcJkIdhwpE3YMi2E0hemZqqNBOBkazIA/w2yy/NSkZ6gQW2zCTt/qZRrhh5hofyxvTK8bvnkChYurGpstYnOVgTQIGMhy/ReUsgyHDxkACRP5w9nq3CprsXtuNW8utIr2HF5EWKyPNMiCyYeKiyZzIncYrDukJrZt/XARmliPzfIoox2m7O2ZexfaNjl/A+0B10kUHESuHRYFGed2v74Ab+m/QytJuDLOYDF7Nv2zK3ACbHRbhsDkIUre2QkQNnBCkt73FZbhtJDVncRAEDU8Sgx0gbq/hpkSoUgCcMGK2wpVVd2UCGeJdRXBkOLzGq1Gd9pPcIqZAnQDhNDcpNBCLD2kOuHwBaTBRvFBwsuBusZdj6Vm3S25PVga5GVuzbITpTV4+SlBmiUig55mH1hUkEW1EoBJy81SMVTAKJWrZ8bZFFG2+as7ZKYBYz8I3296Tl6Awh3joresfyrJHFWjwgCTfDXJgEXfwR2veHb9k5vA4z1tGqzTaPdjvawdAcL6W0/UYG6FruwZSgNMjF/zJKYA4Aan/4kxjPYPq47VApzgMOWv5Q34GhpPVQKAVf369jNhIULqxqMdlpkATLIas8BllZAqYHV0BXVTSypP/RVlozp4u/ozrDefqICjUYLspN0GNJWdZ3jABOHrW02AclUBDjoYUs3FZasIGZsr/QOeZh9waBTS+0AHURio1StnxtkUQZrzgq4iLu7Y8xcmiRZ+rPN2AlnvA1X2mPIAaaKxQub/+FbyM++urJNTgVrmdRL5ibJvbMS0TMzAUaLFZuOXLK9IRlkIQhZivljxnh6A07UqjrkFRyZn4rUeA2qm0z4vr0OEx2EJZ2P6ZmO5LiOeZdSpZClMfAhS5abltoddUarpDGYEh+aPpauYGHL3aerUF7v3GFCalM1ILvDOn3RTpKDQSa2TwumQUaIWw0ylicYbC/nNFd5ilGq1s8NsihkmrtyYXfEpQJXzKavNz9PGzSHK7XngQt7AQhAn+ntDndg6O1A9/FUv2bVn7zzBlotwFGxeXlf5+2xkKUcGmRtmeaqEpEZZM3VwZe+EDXImnV0Xr62TWqLSqnA1P4s5zGwYUv7HKaOIiX124csGy8F5uYghSt7SuHKRK0KWpW87Wo6Qm5qHAZ1YR0mHMOWrWYLisRwZbDyjiIZZpDVOXjIgqhF1nAJaK2lwuGpth6lJy814FiZ6GEOUriScXVBFlQKAcfEkCmAqFXr5wZZFNK2OatXjJoN6JKB8qPAgf8FdH4dgrUu6jqKhlt9QRCAa18B1HG0KfneZe1/5vweerPVJgF5Yx3eslqJdIGQO2QJ2J5EtxwvR0OrmPemiaehUwCoPi37Nj0i5rLUa+n37m/+mD1MCHfdodL2O0z4ydnKRhy6WAelQsDV/TpuFKTZe8h0SUC8WBEXiLClVGHZ05bQHyYVlvaw37GtYf3dyQrUt5iRmajFZV29SC+IcZJchiyDmEPGwpXJ3QC1Tlq8VpTgGdMzXRKvDRZJcWqM6ZnuMA+e1M+JGNptzuryQ8nAmAfp6y0LAyt02RH8CVfak5JHG5ADQNHT7V/sWAi39xRA5XgjvFDTjGaTBRqlAl1T5amwtKdvp0Tkp8fDaLZi01EXYcvKIOeRiSHLaj/7WLriih5pSI5To6LBiN0BCluyiuMruqfJkgzvELIEbLk2gWihZK9B1hBeCf32MO/XrlNVqGywhS3Zd184oBMPV3qBwaWHLIghSzcVlt8cCG51ZVvYdtk8eFI/J6Lw2JzVHSPuA+LSqeflp48DODs/aaygni3ASZzVJ0b8EcgdBRgbgK/+7F7ugxC3cheALaG/e0Y8VEr5TyVBEGxiv/aGdVqIEvvFkGWlkj6tymGQqZUKTBGT7J1apMgEW+80mXJfWEJ9Q6sZrWZLYLXImJGX1stOpT98EvoZ3dLi0T/HscOE0WzF+kPsRs6rK73BtYcsmAaZ7QGAcbqiEUdKqId5igweZn+4ul8nKBUCjpTU4XRFo13IsiYk8wkU3CCLUjw2Z3WHNgEYO4++3voilXsIJ46tBogVyB4MpHTzfz0KBXD9vwGlFvhlI7D/E9fjyo9S41SpddkN4ESAEvrtYaGgzccuockohi1DVWkpesjKBPkMMsBOXPRgqXdSLT5wrqoJP5+vhUIApvSXJ/fFoFdBJXp7aGJ/gCotjU1AHa1spRpk9HwMh7ZJrmBGF9OM2nmqEnUtZqQnaDFcbETO8Qw7pxqNFpgSu9CFTRWAsTE4E3DhIWMPNKN7pCElRMdearwGo3uk2eajFT1kPGTJiQQ8Nmf1xPC7gcRseiPY+36AZucnHQ1X2pPeCxj/KH29dj5QX+Y8hnnHuo937J8mInfLJFf0zzGga2ocWkxWbDlWTheGotKypU66+F2w0purP43FXTGmRzoSdSqU17dirzcdJnyAVVeOzE9DeoI8niVBEKQbU2WDXWK/3CFL9vvqU4C4VFQ0hG8OGWDTd9txsgI1TUas/pled64ZkCWrRl80Y9DZzqk6EmcLzQUrj8xFhaVUXRnillds+2sOlPKkfk7k4bY5qyfUetpSCQC2/4s+pYcDLbW29k4F18mzztEPUm9bSw3wzTzn0CWTuyhwHR4NhkEmCIKzSCyrfgqmh0z0jkGXjEojNQjk8pBpVApJG+ybn+UNW34jY3WlPWkOlZZ2HjI5dfzsWiYBCKvG4q7onpGAvp0SYbbSBtTrxIpL3rvSe1RKhdSSqK7FHNywpbEJqBW3Ixpk56qacOCCvB5mf5nSPwsKAThwoRZl4jWIe8gAvPHGG8jPz4dOp8OwYcOwfft2j+Nff/11FBQUQK/Xo0+fPvjggw8c3jeZTFiwYAF69OgBnU6HwYMHY+3atQ5jzGYznnjiCeTn50Ov16N79+5YsGABrJEgZBoi3DZnbY+ht9MLQUMZ8MM7gZugL5woAixGeqGw6yXZIZQq4PrXqeL/0a+Bwytt79WcA0p+AiBQlf82EEJwUlSOlquHpTvYDW3T0UtoMVmA1Hz6RlMl0FwT0G1LiPljSOpC81sgn0EG2PZxrYxhyws1zfjpXA0EAZjaQXX+tqTat09K6UaPIXOzzXCVA7uWSXRb4ZvUz2Bhy0VFx1HTZEJqvAYj8nm40hdca5EFQfqCHW/6VCCehgfZQ6CcHmZ/SU/QYmQ+ndfWs2JBTax7yFasWIG5c+fi8ccfx759+zB27FgUFhaiuNi1Bb9kyRLMnz8fzzzzDA4dOoRnn30Wc+bMwVdf2QRIn3jiCbz11lt47bXXcPjwYcyaNQs33ngj9u3bJ4154YUX8Oabb+Lf//43jhw5ghdffBEvvfQSXnvtNT92Ozbw2JzVEyoNME4M5327ODwO+iOr6L9yhCvt6TQQuFLMm1v9V5u21zFRe6zrKCAhw+ljJbUtaDRaoFII6Jbmobm5DAzqkoTOyXo0GS00bKlNtEktBEv6guUyGTpLBllHdcjsGds7HQlaFUrrWrDvXI0s62ThysvzUpGZqGtntG+k2ocslWogRTSS5cwjY+sSQ6Lh1jbJFcwTWSFWWk7t3ykgBS/RTMjU+l3kj0m9KweFh5eTzWPdL810QUttZPVgbgefz5RFixbhnnvuwb333ouCggIsXrwYubm5WLJkicvxH374Ie677z7ceuut6N69O2677Tbcc889eOGFFxzGPPbYY5g2bRq6d++O+++/H1OnTsXLL78sjdm5cyeuv/56TJ8+HXl5ebjpppswZcoU7Nmzx4/djh3cNmdtj0G30htBcxXw/ZsBmJkPmJrd9pKUhlistqR3X7nqYSCjL9BYDqwVDVGWr+ammpOFK/PT46EO8A3HodqS5QOmiWHLYDUZlzxknQPiIdOqlJhcQI3M/+45hxNl9R3++2o/7b05TWbvGNBGiwwITE9LOw0yui2W1B9+VZaMnpmJDiF8LgbrOyyPLPgGmWOF5fnqJuxnHuYQhysZU/tnQRCA7y+KskxWExX6jhJ8yso1Go3Yu3cvHn30UYflU6ZMwY4dO1x+prW1FTqd49OpXq/H7t27YTKZoFar3Y759ttvpf9feeWVePPNN3H8+HH07t0b+/fvx7fffovFixe7nW9raytaW22VgnV1YeDpCTL2zVmPl9Wjt7cVgUoVMH4+8Nk9wI7XgMvvpYr+oeCXTYCpCUjKBbKHuBzym7d34WxVE9bNvcp3D4JKS0OX714N/LwC6DEROCsezy7kLgBIjW4DHa5kFA7MxjvbT2PjkUswW6xQpXYHincCVUHykNU6e8jkNMgAuo8rf7qIT384h09/kC+J+ZoA5DCxBuM2LbKewHHI10KJEIeQJSEkrIVh7SkcmI0TG08gOU6NUd3TQj2diCNk0hcVx+i/oodsgyhfEggPs79kJupweV4qfjhtBYEAAYRGcNT6UE9NFnx6tK+oqIDFYkFWlqO1nJWVhdJS1x6YqVOnYunSpdi7dy8IIdizZw+WLVsGk8mEiooKacyiRYtw4sQJWK1WFBUV4csvv0RJiS3M9re//Q2/+c1v0LdvX6jVagwdOhRz587Fb37zG7fzXbhwIZKSkqS/3NxcX3Y3KrBvzupze5r+vwIy+9HEyZ3/DsDsvMS+ulJwrtaqbTJhz9lqlNe3+q9l1WU47VYAACtnA8QCZA2w5Wu1gUle9MwMnOSFPUO6JEOpENDQaqahKzavYCX2s5BlUhcqWgl5Q5YAlWoZ3YOKt8r1d+cV3dApSf6bCTOKKiUPGUvsl8kga7hEzztBAaR2R32rGSYLDc2Ea1I/43cju2JIbjIemtw74N7jaMR1+6RgesioQXamkhZ0hVuHhRF5qSBQoEUpPgxHUWK/X3XrQpubIiHEaRnjySefRGlpKUaNGgVCCLKysjBz5ky8+OKLUCppP7ZXXnkFf/jDH9C3b18IgoAePXrgrrvuwnvvvSetZ8WKFfjoo4/w8ccfo3///vjpp58wd+5c5OTk4M4773S57fnz52PevHnS/+vq6mLSKJs2MBubjl7CmgOlmDu5d/sfYCgUwITHgRW/A3a9CYy832U+VUCxmGz5XG7ClSfLbQULaw6U4ncj/dQom/A43RYzctx4xwCbKGwgKyztUSgEpIiK9pUNRmRJlZbBDVlaEnNQ30r3XW4PmValxMd/GCXrOgNFuju1frlClmw9yV0BlRZVNVSHKk6jhE4dPn0sXZFl0GHlnDGhnkbE4uAhSxLvV0yLTBOgfFWrxW0RSXqYeWSzk+kDVqMQBz3qwyPHWSZ8enxJT0+HUql08oZdunTJyWvG0Ov1WLZsGZqamnDmzBkUFxcjLy8PiYmJSE+nApMZGRlYuXIlGhsbcfbsWRw9ehQJCQnIz7d5J/7617/i0UcfxW233YaBAwfi9ttvx0MPPYSFCxe6na9Wq4XBYHD4i0VcNmf1lr7TaZjQ1Ah8tzgQ0/PMme00cTM+A8gd6XII81YBVIxSukn6iiYOuM6uSMRN/hghRMoh8zoELAMOLXuCKQ5LiFQ92Ki1nedyG2SRhFP7JJZDVnOO5jx2lErH/LFISOjnyIODh0yfbBNBDaQWWe05moul1ND2cgjfqt5s0eNdT8QwZRSp9ftkkGk0GgwbNgxFRUUOy4uKijB69GiPn1Wr1ejSpQuUSiU+/fRTzJgxAwqF4+Z1Oh06d+4Ms9mMzz77DNdff730XlNTk9N4pVLJZS+8wGVzVm8RBGDik/T17neAuosyz64dpOT66YDCtWfghJ2RSVu3+FjAYE/elcCNbwPT/gVkD3I55FJ9K+pbzFAqBOSly9/D0h1SZV9jq80gaywP/BNiU5WUOFutph7SOI0ypsNRaSxkyfo2xmeIN04ij5EcYRpkHPlgzbtZrmZQwpb2BSTidTZcHwI6GaghVm0RDbIoCln6fEWdN28eli5dimXLluHIkSN46KGHUFxcjFmzZgGgYcI77rhDGn/8+HF89NFHOHHiBHbv3o3bbrsNBw8exPPPPy+N+f777/H555/j1KlT2L59O6655hpYrVY88sgj0phrr70W//jHP/DNN9/gzJkz+OKLL7Bo0SLceOONHdn/mGGanUisz/ScBHS9ArC0Atv+JfPMPGC12NTyPchdMIOsWxo1jvzaR3sG3wqM+IP77ZXZtqdVBS98lGafSK4zUCMACLyXjOWPxWei1kgvGbHsHQNsSf11LWaYLFb64JLOFPtlyCOTwkeOFZbhdnPkyI9B584gC6AWmSR5YethGa5VvcxDVmURc0NjNWQJALfeeisWL16MBQsWYMiQIdi2bRtWr16Nbt1o3k5JSYmDJpnFYsHLL7+MwYMH4+qrr0ZLSwt27NiBvLw8aUxLSwueeOIJ9OvXDzfeeCM6d+6Mb7/9FsnJydKY1157DTfddBNmz56NgoICPPzww7jvvvvw97//3f+9jyGmiM1ZD5fU4UyFj33RBAGY+AR9/eMHQPUZ2efnkvM/AI2XqOch7yq3w5hA6wMT6M3rO7F1S6AIdv4YwylMFqywZYAlLyKRZL0arBtQtVNivwx5ZJIGGV2nzVsRXjdHjvw45JABQfKQOWqQhXNVb3KcGlqVAnUQ8+miyEPmV1L/7NmzMXv2bJfvLV++3OH/BQUFDgKvrhg3bhwOHz7scUxiYiIWL17sUeaC454UsTnr9hMVWH2wBLPH9/RtBXlX0p6Op7YAW18Cbng9ENN0hIUr+1xDxWpdUN9iwsVaGk6b0q8T3u10GkdL61F0uAw3Dw9MAYetZVLw8scA+5ClnUF27vsgeMhEgyxAorCRCC2y0KCy0YjKRiMyDTr5tMgsJttDD9MgE/tYpoXZzZEjPwZ3BlltAHPI2lRYhnNVryAIyE7Sob6W5ZBFj0EWu0kgMYhDc1Z/mCB6yfZ/DFzYK9Os3ECIV+r8rEghM1GLpDi1bR99FcL1gZNiyDJYGmQMdjNmN+fgechskheSQaaLbYMMcOGxlCtkWX0GsJoBdTxgyHHYBg9ZRj+h9ZCJOYviNSZcq3o7JelQDzF/t6U2tJOREW6QxRD2zVnPVfnRNDz3cqD/jQCxAv+ZaWszFAhKf6YXIJUe6DHJ7TDJWyUaRyxXbvuJctS1mGSfFiEEx8WQZc+YCVnaRGHrmmk3hFgPWQIuPJb2WmQdaeciJVj3kHT3wjXBmiM/7NxqaDXTvq6BNsiaqmhxEOAiRB6ex1t2kh71RDTIoihkyQ2yGMK+OavPIrGMGYtpWXRtMbDyfiBQVa4sXNlrMpWjcMPJNuHDXlm0dYvJQiSlaTmpaDCipskEhQD0yAiNQVYpJtsGzSCr4zlkrrB5LMXfg7Wzaqmljd/9pU0PS4BXWcYS7NwiBKhvMdsMssZywOjHg3R7sOPN0BnQ0mtauB9vjh4ybpBxIhTWnNXn3pYMfTJw8/uAUgscXwvseEW+ydkjqfNf53EYa2Fk760qHCjuY0erLV1tT/SOdU2NC7orP61tux5mkDWUAa31bj4lAyyp39CFG2R2OHks1XqbkGdHwpZMg8yh4o3lkPGk/mhHo1JAL15battqkQUij8xDhWW4eshyknSo4x4yTqTDmrPuP1eD89V+Pm3lDAEKxebwG/8OnPlOtvkBAMqPA+VHAYUa6DXF41Bbgr1zQ+NtJ8pRL3PYknnkgtUyyR52caxuMsFssdILdZzYKzBQPS2tFqBe1J5L6iy1TUrS+1UPFFWwisdKeyFiKbG/AwZZhWOFJSFE8oqGq8eCIy8GvV2DcSCwYcs2FZYAjQQA4VvV2ylJz3PIOJEPa84KAGs7kvg+bCYw6Fba8/F/d9Pee3JxVPSOdR9HjQ43NBnNOF9NVdF72Snm98lKRPf0eBjNVmw6KuO8YNMgC3ZCPwCkxNm8UtVN4oU60GHLhks0wVxQAAmdbB6yOO4hY8ZRZYOdQca8DLJ4yKhx12S0oMVEUwPC1WPBkRf3if0B0CIrdzbIbB7Z8DzespN0dkr93CDjRDDTBlAPUocqEQUBmPH/gIy+QEMp8Nk91JsiB/bNxD3wyyWqp5YmNpK2TU1Aoegl87ui1A2h0iADAJVSgWTREApaYj/LH0vMBpQqXmVph1PIErBL7Pezx2hzjS3BWuxXytavVSkQpwm/ijeO/AS10tJlyDL8c8jqRA8Z4SFLTiTDcqz2nq1GSW0H+u5p4oFbPqDl+ae3AVvc9xX1mppzwMV9AASgj/vm3oCdceTCWzVN3MfNxy6hsdXc8XmJtC0iCDZpTon9AW4ybid5AYDnkNnh9FsAtsR+f0OWzJBL6ES7McAWEk2L10AQqy450U3QDDJzq03zzs5DFu5VlqlxGrQqxOt+S13HqprDCG6QxSBZBh2Gd0sB0MGwJQBk9AGuFRP7t70EnNjQsfUdFVsldRsNJGR4HHq8zL1x1C/bgG5pcWg1W7H5mDxhy6pGo5Rb0SMzXpZ1+orbxP5A5ZDZicICkKREuEFmUzB38JAxL0PVacDix4OAy4T+VoftcaIfJg4rSfcEyiCrOk3TTjSJ1AvOFrOcxTA95hQKAToDvYcJxAKYAlB9GgK4QRajMC+ZLCG9QTcDw++mrz//g82r4g9ehisB4KQHD5kgCB0XwnXaHjUAu6ToEacJTVJ70LXI7NomWa3ELqmfG2Tst6hpNsFiFZ/QDV2odp7V5F++jwvJi8owT7DmyE/QPGT24Uo772tVBBxzKYZkmIlowkSJ9AU3yGKUQjGP7IezVbhU19LxFU5dCGQPBpqrgP/OBMx+9JJsuASc3UFf953R7vATUsWj63wuVm256eglNBs7nt8WyvwxBvOSSInkaaJBVl8CGH3sUeoNrLG4oQsajGZIdgc3yJASR38LQoBq1jtVobALW/rRQkkSheUaZLGMc4NxUU5Fbi0yFxWWtKo3/I+5TsnRV2nJDbIYJSdZj6Fdk0EIsPaQDB4ktY7qk2mTaFPwDU/7vo5jqwEQIGeo7QLkhhaTBcVitwF3+VwDOyehS4oezSYLtsgQtrRVWIYmfwywXSAlD5k+hf4BgQlbSjlknVErVnZqVYqwbKcSbNRKheTJcEzsFw0yfyotmRHnIsE6XPN5OPLj5CHTJQNamlMoqxZZhXOIvMloQas5/Kt6HSotoySxnxtkMcy0AUxA1U/V/rak5gM3LqGvd70BHP7St8/7EK78pbwBhADJcWqku8lzEARBSu73WwjXjrDwkLmq7Atk2LLWlkPGE/qdSWvrsQQcWyj5gtVqS+q3D1lygyzmYOcYSxGAIAQmbOnCQxYpVb3RqNbPDbIY5hoxbLn7dBXK61vbGe0lfacDo/9EX3/5gPfl/801wKmt9HU76vyAfbVjgsfKMxaa3XSkDC2mjoUtw8FD5tQ+CQhcpaXZSLsAAEBSF+nmwMOVNpw8loDN2+Cr9EXdBcDcTAWRk7tJi3nIMvZw8pAB8muREWLnIXOusAz3qt7sJB3qiFhc1cpDlpwIJzc1DoO6JMFKgPWHZdTrmvQ00PUK6kb+z52AyQtpjRPraSJ0Rl8H97nb4WXeKeYPyU1GTpIOjUYLth0v92r6rqhtMuGSaLQGu6m4PU5VlkDgPGT1JQAIoNQAcem8wtIFNo+lvfSFn+KwzKOWmg8obUUj3EMWezDh5TqXBplMHrL6UsBYDwhKesyJREpVL1XrZ+Kw3EPGiQLkrkQEACjVwE3LgLh0oOwAsOaR9j9zZBX914twJeB9+JCKxIr72IGw5clyur2cJB0StKFrG+Q5ZClzDpkkeZEDKBQ8ZOkC1+2TRI9lQ6lvN4o2LZMY4S5BwJEfzx4ymXLIWLgyJQ9Q2aopI6WqN9suZGlprgntZGSCG2QxDqtE3Hmq0vEm31EMOcCvlwIQgB8/AH76xP1YY5NNv8xrg4x6yHp7ET5k+7jhcBlazf6FLSWPXAjDlYDtplzdZIKVlTwyg8xfdXh3SJIXtMCCG2TOuAxZ6pOBeFFDz5dKyzYtkxiRIEHAkRdWZVnXYgZhoqdye8hc5I8BkRMiT0/QokE0yJrrq0M8G3ngBlmM0y0tHv1zDLBYCdbLUW1pT48JwPj59PXXDwFlh12P+2UjzZ1J7gp0GtTualvNFpytFCssvegpOTQ3BVkGLepbzfj2RIXX07fHVRPzUMCkFixWYnt6Zh6Z+ovylsRLkhdUFJYbZM7YcvraPMz400Kp0tlD1mKyoFGUbOEhy9iBnWMWK0ED6zQiu0HmXGEJRE5Vr1IhgGho5Sk3yDhRg5yViE5c9Vegx0RqcP3nDqC13nmMVF15nYM4oTtOVzTCYiVI1KmQmdi+10ChsInErvYzNBsuBplGpUCijoZMK+2lL3RJ9DVrgyIHdqKwgM0g40n9NpjHsqqhjUHGvFy+VFpWOIvCspujWinAoAtdqJwTXHRqBTRKent2EodtvORdXm57uPGQRVTOop5e94yNNaGdh0xwg4wjVSLuOFmBmiYZw5YAFcr81TtAYg69OX31Z8e+Y2YjcGwtfe1tuLLMuwpLe9g+Fh0uhVHU2PGFk2XuuwIEG6cwmSDY5ZHJGLZs0zaptpk+qXPDwIbLnD7AZlR5m9hvarbpS7nQIEuJC++KN468CIIgPfi41CKTI48swkOWAKCOTwYAWJpqQjoPueAGGQfdMxLQt1MizFaC9YfL5N9AfDpw83JAoQIOfgb8sNT23plttGQ5IQvoMsKr1Z3wo8H38LxUZCRqUddixne/+Ba2rG8x4WIt7WbQXlVnMHBZ2SdJX8hYackMBLGxOG+b5Ez7IUsvDbLKXwAQetONS7MtjiRvBUdWkvT0wadOfBCSVYustd72wNUmZBlJx5xONMi4MCwnqpgm9baUSSS2LV1HApOfpa/XPQZc2Etfs3Bl3+nUm+YFnnpYukOpEHBNf+ol83UfmeZZlkEbFsaIy8q+QEhf1Lb1kHGDrC1MhqS6yWgrsgActcjsPcLusFfot+8pyCssY5aAapGx4y0uHYhLdXgrko65OAOdu9LoIhUmAuEGGQeArRLx25MVjhcAObliDu1RaTEC/5kJNFYCR7+h73kZrgTsNch8Cx8Wivu4/nAZTBbvw5b+eOQCCQslOKjDy22QGZtoX1JAyiHjHjJnUuJtyddMpw0AFXYVlICpCai72P6KmCctzbHCkv3GabzCMuZwUusH5POQsVB6Rh+ntyJF9gIADMnUm6w2cYOME0X0zExEr8wEmCwEG48EIGwJ0Cf/61+nuje1xcDyabRZri4JyBvr1SpMFitOV9Am2r4q5o/IS0VavAY1TSbsOlXp9edOttPEPNgwwUaX/RMrZTLImBGhjqdhNNh5yOK4QcbQqpRI1LYpsgAAlYYe54B3YUsXCf1A5FS8ceTHKYcMkNEgY/ljjuHKFpMFTWJVbyR4yJJS0wEAOmtjiGciD9wg40gwAVV/KxG9Qp8M3PIBoNQC5Ufpsj7TqJisF5ytbITZShCvUSInSefTplVKBaaKyf2+7OOJMEroB+w8ZK5ClnXn5anAYpIXSZ0BQQAhhIcs3ZDqqp8lYLvZeZPYX+lagsDmIQv/myNHXjyHLOUyyFxXWKqVgvSgEc6kp1G9vwTSBKulY63xwgFukHEkposG2bYT5ahvCVDYEgCyBwOFL9j+33eG1x+1F2j1p+qMNVRfd6gUZi/DluEWsnSZ1B+XZqvAqpah150keUET+puMFpjFHClukDni8vcAbN6u9rTICHGpQQbYJVhHgLeCIy+BNcice1gC9iLEkVHVm5ZGPWQKgaCyuirEs+k43CDjSPTOSkD3jHgYzVZsOnopsBsbNhMY9zdg4C1Arylef+x4Wcf0wEZ2T0VKnBpVjUbsPt3+CdxkNON8dXOHtik3aQliUr+9R0Zu6QsnyQt6U1ApBOjVyo6vP4pw6bEE7AyydjxkjRVASy0AwaGnIGCXYM09ZDGHR4OsI1pkVotjEYkdlayPZQTkjwGAWhsHE6gnr6LC/17F4QI3yDgSgiBIHqTVgaq2tG0MmPAY8Ot3aL6Nl3jbw9IdaqUCU/qJYcuD7e/jL5dobkJ6ggYpYXJTdNmuB5A3sb+WhSxFyQu7xuKR8OQcTCQPmb8hS2awJecCar3DW7Ycssi4QXLkg+WQORSL6JIBjeip91eLrOYsLaxS6aS2aIxI0iADAAgCGoV4AEBVlX9dWMIJbpBxHGCViFuOlaORtewII1iCfUfyudg+rj1YBovVsyQBMwDDJaEfsBkA1U1GW587QF6DrK2HrInnj7nDpQwJYAs/1hQDphb3K2AGW5twpf06eVJ/7OHSQyaHFlmFXUWvwtHbHYlFJEYVvTbXVntfqBWucIOM40C/bAPy0uLQGoywpY+YLVacKhcrLDuQzzWmZzqS9GpUNLRizxnPYUsWIvWmiXmwYBdLk4WgrsXOaJYqLWUIWdbaJfWDt03yhFuPZUKmmNdHgOrT7lfgJnxkNFtRL/6+EeOx4MgGazDuJEPUUS0yNxWWQGQ+AJjV9NrcWMsNMk6UIQiCVG25xouQXjAprmqC0WKFTq1A52R9+x9wg1qpwNX9sgC0H5o92cEQaSDQqZWI19An2yqX4rAebv7eIonC0pAlr7B0T5orGRKAejOYkewpbFnpWvKiWmxjplQI/HuPQVzqkAEyeMhcV1gCtrB7JD0AEC01yKKhwTg3yDhOsDyyzUfL0WQMn7DlCTs9MIWiY3lMTAh3zcFSR4V1t9sMHw8ZYK9FZt8+STTIas8B5lYXn/KSllqAKV+38ZBxw8AZt+2TAO9aKNmHkOxgRRspceoOH++cyIPp/dU2mxxTE5hBVutnDpmbCksgMqt6FbroaTDODTKOEwM6G9AlRY9mkwVbj4VP5cpJGeUnxvRMR6JWhUv1rfix2PWTVYvJguKqJrrNMNEgY0h5S/aJ5PEZYsIvAarP+L9y5h3TJQMamjBbJ4Usw1+bKNgwFX0n2QvALrH/pOsPW0y2cGabEFIk5vNw5IM9/JgsBM0mO42tjnrIyo/Rf12ELCOxqlcVnwIAsDTXhngmHYcbZBwnBEGQNMlWHwygSKyPMIFWORLstSqlXdjS9T7+Ut4AQqiHItwuUC7zlgQ72YSOJPazhH6xwhKAlKvGPWTO2HdOIG37VkrSF24MsppiwGoG1HFAYo7DWzYJgvA69jjBIV6jhFL0jEoNxoGOGWSNlbaWaG08skBkVvXqEpLpi9Y65/MvwuAGGcclLI9s45EytJjCQwHZJtAqj7fKPlfOVdjS3iMXblIPbsNkclRatpG8AHjI0hNpdkUW9W0rk9vTImPho9QegMLxcmyTIIicmyNHPgRB8KxF1lDmuxYZyx9LypW83/ZEYlJ/XCJtMB5vbUB1UwAFzYMAN8g4LhncJQk5STo0GS3Yejz0YUuLlUgGklwVj2N7pSNeo0RJbQt+Ol/j9L6tK0B4hSsBL7TIOlJp2UbyAuAGmSd0aiXiWJFFWy0yltTfXE29E22RWiZ58lZEzs2RIy8uDTJ9ik2LjD08eYuHhP5IrepVxtEcskShGSW1MrSNCyHcIOO4xKHaMtAisV5wvroJrWYrNCoFclPjZFmnTq3EpAIatnS1jx0VoQ0kqe4MMmYAdMhDxkKW3CDzFrceS028VKnqMmzppmWS/bq4QRa7GHQ0Z9O9FpmP0hceDLKIreoVW8YlogmltR70/iIAbpBx3MIqETccuYRWc2jDlsxb1SMjQcqrkAO2j6sPlDrlH5yQ2SMnJwENWbLG4gbnkCXXIXONW48lYPN+uQpbVrjWIAPsJAgiqOKNIy8GVx4ywP88MqnC0sUDQKRW9eqYh6wJF7lBxolWhuamoJNBh4ZWM7YfD21bCrnzxxjj+2QiTqPEhZpmHLhgq9JpNVtwtrIpINuUgzRXshdAG+kLF8aBN3APmc+4bTAO2PLIXGmRMSONeTbt4CFLjsuQJdABg8yDBlmkHm865iFrRikPWXKiFYVCwDUDvO/7GEgCFT7UqZWY0DcTAPCNXdjydEUjLFYCg06FjMTwS6p2KXsBAAlZgDoeIFb/lLwJ8ZhDxtTDOY64bZ8E2GmRtQlZttTRxGz7MXbwKkuOrAaZqcV2TXCpQRahx5sYsjQIjSjhHjJONDNNzCMrOlwGo9kasnnI0cPSHUwId41d2JKFSHtlhV+FJWALkVW2lVoQhI6FLZsqAbN4UTNQGYYWk0X67ZlYJccRyWPZ1kAG7EKWbQwy9v+ELOkp3x5eZcmRVa2/6hR9UNMm0bZebaiUQuQRdryxkCWaeQ4ZJ7oZ1i0FGYla1LeY8d0voQlbWu0qLAOhmD+hbwZ0agWKq5pw6GIdgMCFSOWCPcUazVY0Gtvk93VEi4xVbcVnAip6YWY3A4UAJGi4MKwr3BZZADbvV9UpwGr3W3lI6DdbrKgRv3eeQxa7yGqQ2fewdPGQaXsAiLDjjSX1C824VNMY4sl0DG6QcTyiVAgoZGHLn0MTtrxY24wmowVqpYBuafJUWNoTp1FhQh/6xMj6d7IelnKI0AaCOI0SWhU9fd1KLfgjfVHnPn/MoI+wZN8gwgyyClcGWVIXQKkFLEbHMHKF+/yx6iYTCKH3zZS4CLtBcmSj3aR+X7TIPLRMAiK4qtfOu1xXVxPR4rDcIOO0S6EY0lt/uAwmS/DDlsxblZ8eD7UyMIcsk/hg1Zb2IctwRBAEu7Clm8R+vzxkXIPMH9I8JfUrlHZNxu3ClpXuK96YtyJZr5a1qpgTWbjNIdOnABrxYdFbLTJ7D5kLIrFtEgBApQVR6QAAalO9Y1eDCIMbZJx2GZGfirR4DWqbTdj5iwtxywBzMgjG0cS+mdCoFDhd0YiDF+pwuoK6vsM1ZAnYcj3cisP6Y5AxyYukXGkRN8jaRwpZusohA1y3UPKoQRahCdYcWXFrkPmjReahwhKIzLZJDEFK7G9CSV3kVlpyg4zTLkqFgKli2HJNCKotj5cFXqA1QavCuN4ZAIAlW0/CbCVI0KqQnaQL2DY7instMtEbU1NMm1f7ggfJC15h6Z40uypLlyET5pVgXjGr1RZS9uAh4wn9sY1bgwzwLY/Mao3ekCVgJ33RFNGVltwg43gFaza+7lAZzEEOW56w6ykZSKbbhS0Bmj8WjhWWDLdipImdAJUeIBbfdYpcSF7UcQ9Zu7AG461mK5raFlkAzlpk9RcBUxOgUNlurHZErCYUR1akpP6WDhpk9RcBUyM93ljRTxukh4BILCKREvsjW63fL4PsjTfeQH5+PnQ6HYYNG4bt27d7HP/666+joKAAer0effr0wQcffODwvslkwoIFC9CjRw/odDoMHjwYa9eudVrPhQsX8Pvf/x5paWmIi4vDkCFDsHfvXn92geMjI/NTkRKnRlWjEd+frgradgkhAZW8sGdiQSY0djlq4RyuBDxU9nVE+kLykNmr9NOcDK7S7554jRIaVmThjRYZM8xS8gGl8/fKJAhSI/HmyJENds61mKzO3VIkg+xc+yti4Uo3x5vZYkWN2Jg7Ih8C7KQvYspDtmLFCsydOxePP/449u3bh7Fjx6KwsBDFxa6t9CVLlmD+/Pl45plncOjQITz77LOYM2cOvvrqK2nME088gbfeeguvvfYaDh8+jFmzZuHGG2/Evn37pDHV1dUYM2YM1Go11qxZg8OHD+Pll19GcnKy73vN8RmVUoGp/VmboeCFLUvrWtDQaoZSISAvLT6g2zLo1BjbK136f6ANwI7CbtZO4rAAkOZHk3GrhTcW9xNBEJDuLoQM2LTI6kuA1gabYeY2wTpCJQg4spKoVUkKFR0Sh2UPABl9XL5dLRpjEVvVq7P3kMVQDtmiRYtwzz334N5770VBQQEWL16M3NxcLFmyxOX4Dz/8EPfddx9uvfVWdO/eHbfddhvuuecevPDCCw5jHnvsMUybNg3du3fH/fffj6lTp+Lll1+WxrzwwgvIzc3Fe++9hxEjRiAvLw+TJk1Cjx7OJeOcwFAohS1LYbEGp7SYVTvmpcVJHohAwvYRCHyItKN4rOzzx0PWUEbDnIKShj1FuEHmHanu2lkBtCouTjT2K0/aJfT3dLkuHrLkALRbSqKWav91SIus3QrLCK/qZUn9sZRDZjQasXfvXkyZMsVh+ZQpU7Bjxw6Xn2ltbYVO55gYrdfrsXv3bphMJo9jvv32W+n/q1atwvDhw3HzzTcjMzMTQ4cOxTvvvONxvq2trairq3P44/jP6B5pSNKrUdFgxO4ghS2DlT/GuLogCxqlAoIA9OkU3gYZq4ZyGSLzxyBj4crEbCrVwBZzg8wr3LazYthXWkoaZK4NMl5lyWGw7hjOHrJu9N+GUtoWyRPtVFhG/PFm12A8ZgyyiooKWCwWZGVlOSzPyspCaWmpy89MnToVS5cuxd69e0EIwZ49e7Bs2TKYTCZUVFRIYxYtWoQTJ07AarWiqKgIX375JUpKbKGxU6dOYcmSJejVqxfWrVuHWbNm4cEHH3TKR7Nn4cKFSEpKkv5yc3PdjuW0j1qpwJR+9LcPVrUlE2gNVvgwKU6NpXcOx79/cxlykvVB2aa/uK2yBGyVllU+hCwlyYvOjou5QeYVbossGOl2if0eNMjs18GrLDmyaJG1U2EZ8cebnYcs5pL621aeEULcVqM9+eSTKCwsxKhRo6BWq3H99ddj5syZAAClkj6Fv/LKK+jVqxf69u0LjUaDBx54AHfddZf0PgBYrVZcdtlleP755zF06FDcd999+MMf/uA2VAoA8+fPR21trfR37pwXyY8cj7DelmsOlsIahLAlC1kGUzH/qt4ZmD4ou/2BIcajAcA8ZL5IX7gQhQVsFV4GPW+b5AmP7ZMAW2J/2UFbIrYLDTL7dUSsx4IjG7b2SW0ET73VImupo7mLQPSGyO08ZA2tZtS7qkqNAHwyyNLT06FUKp28YZcuXXLymjH0ej2WLVuGpqYmnDlzBsXFxcjLy0NiYiLS02lORUZGBlauXInGxkacPXsWR48eRUJCAvLzbeW52dnZ6Nevn8O6CwoK3BYTAIBWq4XBYHD443SM0T3TkKhToby+FXuLqwO6LUKIFLLsHaaK+aGE5Sw1GS1oMbWpwErMBlQ6wGoGar18EKlzrrAEeMjSWzx6LAGbN+zUVgCE3kTi052GWa1ESrKOSAkCjqx0WIuMeWMTsgB9sushkV7VKyb1pyppQn+kesl8Msg0Gg2GDRuGoqIih+VFRUUYPXq0x8+q1Wp06dIFSqUSn376KWbMmAGFwnHzOp0OnTt3htlsxmeffYbrr79eem/MmDE4duyYw/jjx4+jW7duvuwCp4NoVUpcXUCN70BXW5bXt6K22QSFQNsmcRxJ1KqgVlLPtJMRoFDQEncAqPQyj4yFPbhB5hfthiyZd8JYb/u/i8hCbbNJKpqJyIo3jqx02CBrJ1wJREFVrxiyTFFSQyxS88h8DlnOmzcPS5cuxbJly3DkyBE89NBDKC4uxqxZswDQMOEdd9whjT9+/Dg++ugjnDhxArt378Ztt92GgwcP4vnnn5fGfP/99/j8889x6tQpbN++Hddccw2sViseeeQRacxDDz2EXbt24fnnn8fJkyfx8ccf4+2338acOXM6sv8cP2Bhy7UBDlsy71i3tHjo1Mp2RscegiB4btnD+id6m9jvQvLCZLEJnXKDzDPteshS8mkFK8NNuJJ9PlGnCkplMSe8YR0y/DfIPFdYAtEQsqQGWZIQ2R4yn5NCbr31VlRWVmLBggUoKSnBgAEDsHr1aslTVVJS4hBGtFgsePnll3Hs2DGo1WpMmDABO3bsQF5enjSmpaUFTzzxBE6dOoWEhARMmzYNH374oYPG2OWXX44vvvgC8+fPx4IFC5Cfn4/Fixfjd7/7nf97z/GLK3ulI0GrQkltC/adq8GwbikB2c4JsWVSMPPHIo3UeC3K6lpR4VL6QvSQeWuQeWibBACJvHWSR9I8yV4AgEoDpHSz/R7pnvN5ItZbwZEVQ4c9ZJ4rLAGgoiHCqyxFD1kCmgBErofMryzd2bNnY/bs2S7fW758ucP/CwoKHAReXTFu3DgcPny43e3OmDEDM2bM8HqenMCgUysxqSATX/50EWsOlATOIJMkL7hB5o40Tx4yX6QvzEaqQwYABnuVfnoTSNSpIlOfKIhIMiTuZC8A6hVjv4fbhP4IvzlyZEW+kGX7HrL0hAitshST+vXWRgBAaYQ2GOf+cI5fFA6wVVu6bKYsAyeC1DIpkvFY2eeL9EX9RQAEUGodEs15Y3HvYb9Fo6siC4Z9lZtbDTIWPorQmyNHVjwbZO1okVnMdk3s288hi9iHANEg01iboYI5Yj1k3CDj+MX4PhmI0yhxoaYZP5+vDcg2TgZZFDYS8axFJnrIqs/SC7MnJMmLHIdEc65B5j0Gna3Iol0tMgi2HL82MA8bD1lyAHvZCxcGWXtaZNVnAKsJUOkdPN/20KreCD/mtLZ7RAKaIzaHjBtkHL/QqZWY2DcTQGCqLSsbWlHVaIQgAD0yuIfMHR7bJxk6U4+X1WQTfXUHl7zoMIIgSFWRbtX6Mwrovyl5gNq18DAzrrnkBQdoxyATBCBJFDx3pUUm5Y/1pJXXLqhpNoHVZqVEqkGmVAPqOACRrdbPDTKO37Bqy9UHS2QPW7JwZZcUPfQaXmHpDlv/RBcGgEJhS+xvr8k4e7puKwrLDTKfsHks3ST2dx0FTH4WuO5Vt+uojPTwEUdWPCb1A57zyLxI6GcPc9TDG8EmgRi2NKAZtc0mNBnbiQqEIRH87XNCzfg+GdCpFThX1YxDF+XtEyoJwvJwpUdYqxO3UgveJvZzD5kspHkykAHq0bhyLpB/ldt1sBsk95BxANu512i0wGSxOg9gBpkrAWgpob+P2/Uzb25apCb0M8RKy0wNPX8iMWzJDTKO38RpVJjQJzBhS0nygif0e6RdA0AyyE57XpELyQvAziCL4waZN3hs+O4lkmo6T+rngHquGC7Dll55yKJYg4whapHlxtHviBtknJhDClsekDdsyXpY8oR+z3gUhgXsDLJ2QpYsx8zAPWQdIa09cVgv4DpkHHtUSgUStNQo80n6ghCvQpZREyIXPWSd9XR/LnKDjBNrTOibCa1KgTOVTThSUi/berkGmXewm3Z9qxmtZhdSC96GLN14yFhDY/undI572jWQ24EQW8VbxN8gObIhJfa3uMiLcmeQNVYALTXwVNELRNEDgOghy9LQ/SmtjTwtMm6QcTpEglaFcb0zAABrDsoTtqxuNErK0T24QeYRg04tCbZWN7p4epakL84AVjfaWMYmoLlKXKHrkKWBe8i8goWQ/fWQ1bWYYbJQTzM3yDgMz2r9ohZZfQlgtismYd6x5K5uK3qBaApZ0qT+DDX9DiKx0pIbZJwOw8KW38gUtjxZTr1jnZP1kque4xqFwk5qwVVlX1IXQKkBLEbXOkWALaFfkyBd1Bg8ZOkbHmVIvIDdHOM1St6/lSPBPNQuDbK4VEAdT1/bn+NehCuB6AtZpqqoIcZzyDgxycSCTGiUCpwqb5RCjR2B5Y/xHpbekeZJrV+hpJpXgPuwpb3kheDYHokbZL7R0aR+qW0Sr7Dk2OFRrV8Q7MKWdlpkUoWlZ4Msaqp6pQbjkdvPkhtknA5j0KlxVW/abuebnzsetjxxieai8fwx7/DYPgloP4+sznX+GMB1yHzFY+cEL+AVlhxXeBSHBVznkXlRYQlE0TGnpd591mC8tI4bZJwYxdbbsuMG2Unew9InmDfFrTp8ewaZ1DbJ0SCzWAnqW2kSMTfIvEMqsmgxw2h2oRnVDlGTYM2RFY8eMqAdg6w9D1mUHHOih4w1GK9qNLrvKRumcIOMIwuTC7KgVgo4XtaAk5c6Vm0pSV5kcckLb/AYsgS88JCJIcs2orD1LbaLP0/q944kvV2RRZPvXrKoyefhyIrPHjJTs+21B4Msqqp6xfxXlakeejH/sizCvGTcIOPIQlKcGmN60rDlmgOlfq+nttkkuZp5Dpl3tBsm89ZD5kalP06jjOyWKkGEFlnQm6dbj6UHosZbwZEVJszstYes8iQAAuiSgfh0t+uNqqpeMalfaKlDdpIOQOTlkfGrLEc27Kst/YWFKzsZdDDouFfGGyQx0gY3lX32av1WF2G0OtchS57Q7x/t5vR5IGokCDiy4nPI0j5c2aZQxx52zYiKql4xZImWWnQSDbJIq7TkBhlHNqb0y4JKIeBoaT1OlftXbcnCnTx/zHvarexLygUUasDSajO+7GnHQ8YNMt9ot8G4B3jIkuMK9nDq3iBro0XmdYVllPSxBCQPGVrrJIOMe8g4MUtynAZX9EgDAKw56F/Ykkte+E67HhmlCkgRL9htw5YttYBRzPnjorCykNYB6YuokSDgyIpHYVjAWYuMecgyYkSDDLB5yMwt6JxIvX2RptbPDTKOrLCwpb/VlraWSTyh31u8Uod3l0fGNMj0KYAmzvEt7iHziw6FLKNFgoAjK+2GLNtqkcVahSVg85AByI2n1eHcQ8aJaab0y4JSIeDghToUVzb5/HkueeE7zACobTbBZHEjtZAq9rJr22RckrxwDFey9QHcIPMVf7XICCHSZ6LiBsmRDXYO1reYYbG66YaSnEv/rT4DVJykr700yKLCQ6ZQAhr6IN9ZS69d3CDjxDRpCVqM6p4KAFjto5esodWMCzXUxdwzgxtk3pISp5Hydt1KLdgn9tsjSV64EoVljcW5QeYLksfSXZGFGxqNFrSK2mVRcYPkyIb9Q1GDqwbjgM1DVrwLMDfTvFGWW+YGSRQ2WkLkrMG4lhpi3CDjxDySSKyP1Za/iN6x9AQtUvgNyWuUCgHJ4gXbZy0yN6KwAPeQ+Yu/IUsWrtSpFYjTRHjFG0dWNCqFpK3VbqXlyY3037QeNH/UA1LOYrRcb8WwJWswXtHQ6pdAc6jgBhlHdqb27wRBAPafr8X5au/Dlrb8Me4d8xXJCHCnfZVmZ5DZS1941TaJN3j3BX9DlpXSzVELwYNUASc2Meg9NBgHbAZZUwX9t52WSYB9Un+U5CyKHjKD0ASNqJ0YSeKw3CDjyE5GohYj8mjYcq0P1ZYnymi1X2+eP+YzrGzdrRGQ1BVQqABzCy2NZ7Ck/qRcp49IHrI47iHzBX+rLKMqn4cjO15rkTHayR8DoiypH5DU+oXWepsWGTfIOLHO9EG+i8QyD1lP3jLJZ9ptn6RU2S7Y9mFLN6KwAA9Z+gszqGqaTDC7K7JwQVRJEHBkp32DrE2+mA8GWdQcc6zSsiUytci4QcYJCCxsua+4BhdrvNOCOcFEYXnI0me8CpO1rbQkBKi7SF+7CFlyg8w/Uuw8itVNbm6eLog6bwVHVto1yOLSALWddE07IUv7qt6oMcjs1PpzJLX+yNEi4wYZJyBkGXQY3i0FgHdhyyajGeer6YnDDTLfsXnIPFT2tU3sb6qkIUwIQGKO0/A6sbk4r7L0DZVSIRllvoQto85bwZEVJg7Lzksn7LXIACDNs0HWaLRICe9RI0TsoNavB8A9ZBwOALtqSy/kL06VN4IQejOKijYeQcaryr62BlntOfpvQiagcrwgW63ELqmfG2S+4k/7pKiTIODISrseMsBmkCVm27xFbnCs6o2Swh2dLWSZHYH9LLlBxgkYhQM7AQD2nK1ut9KFhSt5yyT/SGVJ/e6qLAFaBg8Alcwgc58/1mA0g+lP8tZJvuNPYn/USRBwZMUng8yrCktbVW/UICb1o7WW55BxOPZkJ+lxWddkENJ+2JL1sOThSv9oN6kfcPSQEeJR8qJWzH3SqhTQqbkmlq/4o0VWFW0SBBxZabfBOABkD6b/dhnR7vqiMkSuFQ2yllruIeNw2sJ6W65up9qSa5B1DK8MgKRcQFBSFe/6UpvkBW+bJDupklq/9wZZ1CVYc2SFnYt1ngyyIb8D7t0EjHuk3fVF5fGmc66yvFTf4lO1cyjhBhknoFwzgIYtd5+pQnm9+3waWw9LLnnhD8xDVt1kdN/rTqWx9burOuWlKCw3yPzBK49lG3iVJccTXoUsFUqgyzBA1b6XNSqPN7uk/vR4LVQKAVYClPvYxixUcIOME1C6pMRhcJckEAKsO+Q6bNlisuBsZSMA3lTcX1irKSsBatz1swQcpS9YDlkS95DJja8hyxaTBU1GC/0sT+rnuIAJNHs0yHyA9VqNVg+ZQiEgy0C9ZBdrIiNsyQ0yTsBpL2x5qrwRVkJv/hm8wtIv1EoFDDpaKeV1HpkkCutskEmSF9wg8wtfqyxZ+EitFJCojZKKN46seBWy9AF2zEVVVbuU1F8HEBJxeWTcIOMEHCZ/setUpfRUZo+9ICzv4ec/7bZPAmwGWcUJLgobQHytsmQSBKnxGn4OcFwiGWQtZhDiJi3BB6I6ZGkxAuYWu0rLyBCH5QYZJ+B0TYvDgM4GWAmw/nCZ0/u2/DEeruwIXoXJmPRF8S6AWGh/y4Qsp2HcIOsYvoYsmSeNV1hy3MGqLC1WgoZWc4fXF5VVlpoEAOIDTQRqkXGDjBMUmJfMVdiSSV70zOQJ/R3Bu/ZJooesqYL+m5hNE4HbwAwyHrL0D6Z8Xt1kgtVdkYUdUemt4MiKTq2ARklv2XLkkUWlELFC4VqtP0IajHODjBMUWB7Zjl8qUd3GYOA9LOVBquzzJLWQ3A0Q7E57F6KwAFDbTJ/AuYfMP1Li6G9hsRKvbp5R6a3gyIogCNIDkhwGWdQ+BESwWj83yDhBIT89HgXZBlisBEV2YUuj2YozlU0AeMiyo6R6089SpaF6ZAwX+WMAD1l2FI1KgUSxyMKbxP6KBm6QcdonSU+PqY4aZM1GC5pNYlVvtB1zLtT6uUHG4bRhmqhJttqut+WZykZYrAQJWhU6iSXKHP/wKmQJ2MKWgFsPGavkYpWbHN9hngdvxGF52ySON9gqLTuWQ8YeEjRKBRKiraqXhSxbapEjhizL6lrc6zOGEdwg4wSNQjFs+d3JCqk1jy1/jFdYdhSWt9RuIrm9QeZCgwzgwrBy4Etif1U0ShBwZEcu6Qv7EHnUXXftQpYZiVooFQLMVuKywj/c4AYZJ2j0zExA76wEmCwEG47QsCXLH+vNw5UdxmupBVZpCbg1yKSQZRw3yPyFVUy267FElLax4ciOXDlkUX282SX1KxUCMhPpeRgJTca5QcYJKm1FYm1NxXmFZUeRK2RJCOE5ZDLgS/skm4csCm+QHNnwqn2SF7DCn6g83lgOWUsdANhpkXGDjMNxgBlk209UoK7FJHnIenIPWYeRpBYajZ6FI9sJWTYZLTCL+RbcIPOfVG9DyHAUhuVw3CGbQRbNHjKdzUMGwK7SMvzFYblBxgkqvTIT0CMjHkaLFesOluJ0RaO0nNMx2MXVbCWek35TuwPpvYHsIUBcmtPb7GKvVgrQq501yjjekealx7LVbEG9KPTJk/o5npDLIIuJkGVLLQCgkyFytMi4QcYJKoIgSF6yt7adgslCEKdRStUwHP/RqpRSxZRHqQWlGrh/J/CHTYCLhF7WxzJJr46+hN8gYiuy8JxMXN1Iv2+lQpDU2DkcVxik9kkd9ZBFcVWvXVI/gIjSIvPLIHvjjTeQn58PnU6HYcOGYfv27R7Hv/766ygoKIBer0efPn3wwQcfOLxvMpmwYMEC9OjRAzqdDoMHD8batWvdrm/hwoUQBAFz5871Z/qcEMMMMtYyqWdmAhQKfuOXA68r+5Qqlwr9AKQKWG4cdAwpqb8d2QtmPKfEafh5wPGI/CHLKKzq1TqGLKUcspooNMhWrFiBuXPn4vHHH8e+ffswduxYFBYWori42OX4JUuWYP78+XjmmWdw6NAhPPvss5gzZw6++uoracwTTzyBt956C6+99hoOHz6MWbNm4cYbb8S+ffuc1vfDDz/g7bffxqBBg3ydOidM6NspEfnp8dL/e/JwpWx4ndjvAd42SR68TeqPWsV0juzwkKUX6JLpv2LIknnISuqiMIds0aJFuOeee3DvvfeioKAAixcvRm5uLpYsWeJy/Icffoj77rsPt956K7p3747bbrsN99xzD1544QWHMY899himTZuG7t274/7778fUqVPx8ssvO6yroaEBv/vd7/DOO+8gJSXF16lzwgRBEFAoisQCvMJSTnyp7HMHr7CUB3azq27yXGQR1QnWHFlhXmu5dMiis8rStYesrLbVq76yocQng8xoNGLv3r2YMmWKw/IpU6Zgx44dLj/T2toKnc5RgV2v12P37t0wmUwex3z77bcOy+bMmYPp06dj8uTJXs23tbUVdXV1Dn+c8ICFLQGe0C8nvoiRuoMbZPLAfguThaCuxX2RRVQ2eeYEBKYLWNts8lxJ3Q5RXdXbJqk/M1EHQQCMFiuqmvy/LgYDnwyyiooKWCwWZGVlOSzPyspCaWmpy89MnToVS5cuxd69e0EIwZ49e7Bs2TKYTCZUVFRIYxYtWoQTJ07AarWiqKgIX375JUpKbC12Pv30U/z4449YuHCh1/NduHAhkpKSpL/c3Nz2P8QJCv1zDBjUJQmJWhWGdE0O9XSiBnZTr+iAKjVX6ZcHnVqJeA3N0/NkIPOQJcdb2DlpshCpF6Wv2Ff1pkdjDpl9Uj8h0KgUSBc7YIR7Yr9fSf1tK68IIW6rsZ588kkUFhZi1KhRUKvVuP766zFz5kwAgFJJL1avvPIKevXqhb59+0Kj0eCBBx7AXXfdJb1/7tw5/PnPf8ZHH33k5EnzxPz581FbWyv9nTt3zo+95QQCQRDw8R9GYesjE6SThdNxeMgyvEj1otIyqvN5OLISr1FCKRZ++JtHxq4NKoUAgz7K+lgCNg8ZsQCmJgB2eWTRZJClp6dDqVQ6ecMuXbrk5DVj6PV6LFu2DE1NTThz5gyKi4uRl5eHxMREpKenAwAyMjKwcuVKNDY24uzZszh69CgSEhKQn58PANi7dy8uXbqEYcOGQaVSQaVSYevWrXj11VehUqlgsbh+UtBqtTAYDA5/nPAhQaviNyGZSfW2fZIHWHgtKi/WQcabSsuoliDgyIogCB1uMM6OxZRo7GMJAJp4QBAryJ2kL8I7sd8ng0yj0WDYsGEoKipyWF5UVITRo0d7/KxarUaXLl2gVCrx6aefYsaMGVAoHDev0+nQuXNnmM1mfPbZZ7j++usBAJMmTcKBAwfw008/SX/Dhw/H7373O/z000+SJ43DiXUkMdJ2pBY8wT1k8uGNxzKqJQg4stPRSsuoD5ELggu1flEcNsw9ZD4/As+bNw+33347hg8fjiuuuAJvv/02iouLMWvWLAA0THjhwgVJa+z48ePYvXs3Ro4cierqaixatAgHDx7E+++/L63z+++/x4ULFzBkyBBcuHABzzzzDKxWKx555BEAQGJiIgYMGOAwj/j4eKSlpTkt53BiGZ7UH154I0PCQ5YcXzDo6G27owZZVB9vWgPQXG1T648QcVifDbJbb70VlZWVWLBgAUpKSjBgwACsXr0a3bp1AwCUlJQ4aJJZLBa8/PLLOHbsGNRqNSZMmIAdO3YgLy9PGtPS0oInnngCp06dQkJCAqZNm4YPP/wQycnJHd5BDieWsDfIPOV2eoLrkMmHLx6yqJQg4MiOoYMesph4AHCj1h91HjIAmD17NmbPnu3yveXLlzv8v6CgwKXAqz3jxo3D4cOHfZrDli1bfBrP4cQC7KZutFjR0GpGoh9q+9xDJh+Sh8xN1avJYkWN2Bkhqm+QHNnoeMgyBnIWtUn031bWz1L0kIV5P0vey5LDiSLiNCro1PS09jdsyQ0y+WgvZFkt6iIJAm2dxOG0h1w5ZFGds6gTDbKWtjlkzR3Sbws03CDjcKKMNFbZ54dB1mKywGi2AuAGmRzYGoy7/i3Y8mS9WpIz4HA8Yauy9DNkGQtCxG2S+jMN9JrYYrJ2uO1UIOEGGYcTZUhGgB+VluwirxCAeA2Xvego7cmQsN8ojWvxcbykowZZ1FdZAk5q/Tq1Utrfi2HcZJwbZBxOlNGRSkv7hH4F99h0mDS7kKWrUElMJFhzZKWjSf0xUWXZJqkfsKu0DOMm49wg43CiDG+kFtzB88fkhf0WRrMVjUZnAeuY8FZwZKWjOWSVsXDMaR1DlkBkVFpyg4zDiTJsUgu+97PkBpm8xGmUtiILFyFk7iHj+EpHDDKTxZZDFdXHXJukfiAytMi4QcbhRBmpHUjq5waZvAiCYFdk4Wwgx4QEAUdWOmKQ2Vf1JkdzVa/OMYcMiAy1fm6QcThRRkcajHNRWPnxlNMXE/k8HFnpiEHGjreUOE10V/W6CFlKWmTcIONwOMGiI0n9rGGxwQ9BWY5rPOX02SQIeJUlxzvYw1Kr2YoWk3NeoidY2DzqHwBchCyzk1kOGU/q53A4QYLpC/nTYJyHLOXHk8eSJ/VzfCVRqwLriFbX4puXLGZyFiUPmeuQZbiKw3KDjMOJMmxSCzypPxzgIUuOnCgUAhK1VCPQVy0y1sIrPZpFYQGbh6y1HrBSoWsWsmwyWlDfag7VzDzCDTIOJ8pgN/cWkxVNRt8uPNwgkx93HkurlUhJ1txDxvGFpDj/8shi5gGAJfUTK2BsAADoNUoki99buOaRcYOMw4kyErQqaJT01PY1bFnHDTLZcSdDUtNsglWMnKRE+w2SIyv+JvZXxkIfSwBQ6QCFeA1zkdgfrpWW3CDjcKIMQRD8TuznHjL5cdc+iRloBp0KaiW/FHO8x1+DLGZyFgXBpVp/tqRFFp6J/fwqwOFEIf4aZCxJmBtk8uGuyrKS97Hk+Imtn6VvKQkxk9QPuJa+CHMtMm6QcThRCGsw7qs4rE2HjDcWlwt3VZYxk8/DkR3uIfMCV9IXrH1SmDYY5wYZhxOFpPrRPslksaJJ7LfIPWTywZL6m4wWNNv1s4wpbwVHVphOoN9J/dFeZQm4VOtn7ZNK6rhBxuFwgoQ/DcbtL+6JXBhWNhK1KqiVVDjKXopECllyg4zjIwY/PGQWu6remHgIcKlFxnPIOBxOkJHCZD5UWbKLe6JOFd1tVYKMuyIL5r2MiZsjR1b8CVnWNBnB9FBTormPJcNTyJLnkHE4nGDhrrLPE7zCMnC4avjOQ5Ycf/HHIGPXgiS9OjaqeiVxWOek/voWMxrCUBw2Bn4VDif28CdkyTXIAocrj6WUYB0L+TwcWbFVWXpvkFXGUkI/YAtZ2nnIErQqqctBOIrDcoOMw4lCWGsUfzxkvLG4/LgOWbIbJJe94PiGPwZZzFX1ukjqB2xNxrlBxuFwgoI/OmTcQxY4XMmQ8JAlx1/8CVnG3PHmQocMsNciC7/Efm6QcThRCPO6NLSa0Wq2tDOawnPIAkfb9kmEEFTzkCXHT1iVZaPRApPF6tVnqhpi7HhzodQPANkG7iHjcDhBxKBXQSVWSnrrJZMMsjhukMlN2yKLumYzzGIjy5jxWHBkw6CzCTd7G7aMuapeF0n9QHhrkXGDjMOJQgRBkBpWe9tgnHvIAkfbIgumR5agVUGrUoZsXpzIRKVUIEFMTvc2bBkzjcUZLpL6AXstMm6QcTicIOGuZY87bG2TuEEmN2ltiixiLsGaIzu+5pHFVNskwE6HzDGpv1MYa5Fxg4zDiVJsXhnv2iexRsXcQyY/qW1kL2IuwZojO+zBqa7FOz2tyljLIWMeMmM9YLXl0WaLSf3hqNbPDTIOJ0pJ9TNkaZ+fwpEH5pWoF4ssYs5bwZGdJL2/IcsYOeZYUj8AtNZLL5mHrLrJhBaTdwVPwYIbZBxOlOJvyJJ7yOTHoFNL7aiqG008ZMnpML40GLfa9bGMGd07lRZQUePLPrHfoFMhTkPzNsMtbMkNMg4nSvG1fRLXIQscCoUg9Q+sbGyVvJapsRI+4siOL+KwdS0mWMSq3pT4GDq/XST2C4Jgl0cWXmFLbpBxOFFKqgsxUndYrAT1rTyHLJDYeyyZBAEPWXL8xZekfnYNSIy1ql53av1hWmnJDTIOJ0rxJWRp/5TNqywDg333hJiTIODIjmSQNbVvkEkh8ljzyLpT6zcwtX5ukHE4nCDgS/ukuhZ6UY/XKKFW8stCIJA8lg1GntTP6TBMwJmdu56QQuSxdry5U+vnHjIOhxNM0qQqy/ZlL7gGWeBxDFnG6A2SIxu+hCxj9gGgPbV+bpBxOJxgwG72dS3mdvvd8QrLwGOvCxezHguObPhSZRlzbZMYWtc5ZDnJooesjif1czicIJAcp4FAlRakRtbu4B6ywMO8E8VVTTCKBnLMiHRyZMfgR1J/zOUsulPrNzBxWO4h43A4QUDpILXgnUHGPWSBg90MT5Q1AAB0agXiNFyEl+MfPGTpBW6S+lkOWUWDEa3m8BGH5QYZhxPFeJvYzw2ywMN+i0v1TPIixrwVHFlh52p9i1nSGHNHzOYsuknqT45TQ6ui5s+lOu9aywUDbpBxOFGMlNjPDbKQ0zY8GXM3R46s2J+r9e1UWsasELGbpH5BECQvWTgl9nODjMOJYpgRUNVOpSVvLB542oaLeP4YpyNoVAro1VTklZ2/7oj5kGWbHDIAYanWzw0yDieK8TZkWccbiwcc+yILgHvIOB3HmzwyQggPWbYJWQJAdlL4JfZzg4zDiWJYIrnXIcs47iELFPZFFkAMeis4smPQ0wcoTwZZQ6vZVtUba3mLbpL6gfDUIuMGGYcTxXjbPonnkAUHew9FzEkQcGTHGw8ZO/f1aiX0mhjqYwm04yHjIUsOhxNEJDHSBm6QhQP2Bhn3kHE6ijcGWYV47sdkzqIumf5ragQsjnl2nQzh1z6JG2QcThSTZqcO7wlukAWHNAcPWQzeIDmy4o04bMwm9AOANtH22kmLLPwajHODjMOJYliZu6eQpdVKpLJ5rtQfWBxClrHoseDICnuA8tRgPGbbJgGAUg2o4+jrtmr9YsiyvKG13dZywcIvg+yNN95Afn4+dDodhg0bhu3bt3sc//rrr6OgoAB6vR59+vTBBx984PC+yWTCggUL0KNHD+h0OgwePBhr1651GLNw4UJcfvnlSExMRGZmJm644QYcO3bMn+lzODEDuwjXNJvcikc2GM1gb7H+eJzAkMZDlhwZ8SZkGbNtkxhuEvvT4jVQKwUQYhNrDjU+G2QrVqzA3Llz8fjjj2Pfvn0YO3YsCgsLUVxc7HL8kiVLMH/+fDzzzDM4dOgQnn32WcyZMwdfffWVNOaJJ57AW2+9hddeew2HDx/GrFmzcOONN2Lfvn3SmK1bt2LOnDnYtWsXioqKYDabMWXKFDQ2Nvqx2xxObMCq+ggBqptce8lqm+jFXKtSQKeOsaTfIJPKQ5YcGfGmwXhVLOeQAW4T+xUKQfKSlYZJYr/PBtmiRYtwzz334N5770VBQQEWL16M3NxcLFmyxOX4Dz/8EPfddx9uvfVWdO/eHbfddhvuuecevPDCCw5jHnvsMUybNg3du3fH/fffj6lTp+Lll1+WxqxduxYzZ85E//79MXjwYLz33nsoLi7G3r17/dhtDic2UCsV0lO0u7Alzx8LHqkJ1EuhUSqQoOWab5yOIYUsvcghi9kHADdq/QCQbQivPDKfDDKj0Yi9e/diypQpDsunTJmCHTt2uPxMa2srdDqdwzK9Xo/du3fDZDJ5HPPtt9+6nUttLY0Hp6amuh3T2tqKuro6hz8OJ9ZIa6fSso4bZEEjXfwtUuM1EOxVYjkcP/AtZBmjBpkXav3hUmnpk0FWUVEBi8WCrKwsh+VZWVkoLS11+ZmpU6di6dKl2Lt3Lwgh2LNnD5YtWwaTyYSKigppzKJFi3DixAlYrVYUFRXhyy+/RElJict1EkIwb948XHnllRgwYIDb+S5cuBBJSUnSX25uri+7y+FEBe2p9XMPWfAY2jUFo3ukYeaYvFBPhRMFMCFnXmXpAQ9aZLmpenRO1kMRJg9HfiX1t32yI4S4fdp78sknUVhYiFGjRkGtVuP666/HzJkzAQBKJc1XeeWVV9CrVy/07dsXGo0GDzzwAO666y7p/bY88MAD+Pnnn/HJJ594nOf8+fNRW1sr/Z07d87HPeVwIh+bQeY6cZVVaHGDLPDoNUp8/IdRmDWuR6inwokCeMjSCzyo9f91al989+hE3H1lfpAn5RqfDLL09HQolUonb9ilS5ecvGYMvV6PZcuWoampCWfOnEFxcTHy8vKQmJiI9PR0AEBGRgZWrlyJxsZGnD17FkePHkVCQgLy852/pD/96U9YtWoVNm/ejC5dunicr1arhcFgcPjjcGINlszrrn0S95BxOJGJTfbCDEJcV1EzDcKYa5vEYDlkLkKW4YZPBplGo8GwYcNQVFTksLyoqAijR4/2+Fm1Wo0uXbpAqVTi008/xYwZM6BQOG5ep9Ohc+fOMJvN+Oyzz3D99ddL7xFC8MADD+Dzzz/Hpk2bXBprHA7HGW9DllyDjMOJLFiVpcVK0NBqdnq/yWhGi4lqbMWs7p3OvYcs3PC5zGfevHm4/fbbMXz4cFxxxRV4++23UVxcjFmzZgGgYcILFy5IWmPHjx/H7t27MXLkSFRXV2PRokU4ePAg3n//fWmd33//PS5cuIAhQ4bgwoULeOaZZ2C1WvHII49IY+bMmYOPP/4YX375JRITEyUvXVJSEvR6fYe+BA4nmmmvwTg3yDicyESnVkCjVMBosaK22YTENjqCrJBHo1IgPtb6WDK0keMh89kgu/XWW1FZWYkFCxagpKQEAwYMwOrVq9GtWzcAQElJiYMmmcViwcsvv4xjx45BrVZjwoQJ2LFjB/Ly8qQxLS0teOKJJ3Dq1CkkJCRg2rRp+PDDD5GcnCyNYbIa48ePd5jPe++9J+WkcTgcZ6QG426qLGub6ZM1D1lyOJGFIAgw6NWoaGhFbbMJXVIc37dP6I/Zql4PSf3hhl9COLNnz8bs2bNdvrd8+XKH/xcUFDgIvLpi3LhxOHz4sMcx7uLjHA7HM2nttE/iOWQcTuSSpFdJBllbYj6hH/CY1B9u8F6WHE6UkxrPk/o5nGjFU6VlzGuQAXZJ/dwg43A4IYZVV1U3GWF10c+ynhtkHE7EYjPInJP6q6QKy1g2yLiHjMPhhAkp8bZKLKY5Zo8tqZ+38uFwIg1Pav0x31gc8KjUH25wg4zDiXK0KiUSxb6JFW0S+wkhPGTJ4UQwBk8GWaw3FgdsHjJzC2B2nbYRLnCDjMOJAVLdJPY3GS0wi2FMbpBxOJGHJw9ZzLdNAmweMiDsw5bcIONwYgB37ZPYRVytFKBXx6hOEYcTwXgXsoxhg0yhBDSJ9HWYhy25QcbhxABpbiot7cOVMatTxOFEMJ5CllJSfyyHLIGISeznBhmHEwOkuhGHreMq/RxORGPrZ+nCIGvgSf0AIiaxnxtkHE4M4K59klRhqeMGGYcTibgLWbaYLGg0WgDEeMgSiBi1fm6QcTgxQJqbBuO8wpLDiWzYw1RbYVh2rquVAgy6GJe0iRC1fm6QcTgxQCo3yDicqCQpzuYhs28xyM71lLgY7mPJiBC1fm6QcTgxAJO9aBuyrOMGGYcT0bBz12QhaDZZpOW8wtKO/9/encdVVa57AP9tNuyBWUAZEhCHBCcU6DjkXOFQXNHuPWodwyHTTpZTOYTz0RzKoZtlac7Xo56OR7Nj2aEcjoqmoagpqSiGejCUCBSEzfDcP3Av2YAKpa7N3r/v58NH9lrvXuvZy7Xg4X2f9S4W9RORtfC+z7QXTMiIaicXnRZah7IesPJ1ZLzDshwW9RORtSg/ZFl+WCO3oOz5d0zIiGonjUZT5fMss3iH5R0s6icia2F+wHhRieBG4Z0f2uwhI6r9qrrTkrP0l6MU9bOHjIhUZtRplZn4y89FxgeLE9V+VU0O+wtryO4weJb9yx4yIrIGXlXM1p/DiWGJaj3ztBblEzIW9ZdjYA0ZEVkR7yoeMM4hS6Laj0OW98F5yIjImlR1pyUTMqLa714JGXvIwKJ+IrIuFR+fVFBUAlNxKQAmZES12Z27LMsNWd7ktBcK9pARkTUx/2A23w5v/uHtoAFc9SzqJ6qtKiZkpuJSZUobb057cWem/hITUFSgbiz3wISMyE5UfHxS+YJ+u3+0ClEtVnHIMju/7BrXOmjY+w0AOlcAt3/GWXFhPxMyIjtR8S5L1o8R2YaK016Ye8HrODvBwYF/bMHBoVYMWzIhI7ITFYv6mZAR2YaKPWQs6K9CLSjsZ0JGZCeUIcub7CEjsiUVE7Ks2390MSErpxbM1s+EjMhOeJe7y1JEOCkskY24Ww8ZC/rLMRf2s4eMiNTmdfsuy8LiUuSbSpQHEbOHjKh2M/9RVVhcioKiEg5ZVqUWzNbPhIzITrjotNA5ll3yv+SZOGRJZCPc9I4w3yidW1DExyZVhUX9RGQtNBqNUtifVS4hczcwISOqzRwcNHC7PZdg7q0ipU6Uk8KWw6J+IrImXuXutGQPGZHt8HC+U0fGIcsqmGvI2ENGRNZAmYvspkmZ1ZsJGVHtV76wn3dZVkHPHjIisiLe5WbrZw8Zke0on5DxLssqsKifiKyJ+QHjTMiIbIv5Ov4lrwi/3r622UNWDov6iciaKA8YzzMht4AJGZGtMF/H6Vl5EClbVseZ17ZCmYeMPWREZAXMfzH/nFuAfFMJAMDd6KhmSET0AJjnIkvLygcAeDo7wVHLX/EKFvUTkTUxJ2QXs/KUZW6c9oKo1jNPX5N2/SYADldWwqJ+IrImPreHLC9n3wIAuBkcoXXQqBkSET0A5iFL87XtzYTMUvmifvOYrpVhQkZkR8xF/eafR6wfI7IN5mvZfG3zDssKzD1kUgIU5asby10wISOyIxWHMZiQEdmGiteyF2fpt6RzATTasu+tdNiSCRmRHXE3OMJJe2eIkgkZkW2oeC1zyLICjebOsKWVFvYzISOyIxqNBnWc7/ygZkJGZBsq9ZAxIavMygv7mZAR2ZnyP6j5YHEi2+DOhOz+rHy2fiZkRHbGu1xtiQcnjiSyCe4Gy/kEWdRfBb15LjImZERkBbzK/aDmkCWRbXDUOsBVfycpYw9ZFQwcsiQiK1K+2LfiMAcR1V7l/8Dy5l2WlVn5bP1MyIjsTPm/nNlDRmQ7yv+BVf7mHbqNRf1EZE2YkBHZJo/bz6V1MzhC58hf75XYYlH/Rx99hJCQEBgMBkRGRmLfvn33bP/hhx8iLCwMRqMRTZs2xbp16yzWFxUVYdasWWjUqBEMBgPCw8Oxc+fO371fIqrMYsjSwAeLE9kK813TnIPsLvQ2Ng/Z5s2bMWbMGMTHx+PYsWPo1KkTevXqhfT09CrbL1u2DJMnT8aMGTNw6tQpzJw5E6+99hq++OILpc2UKVPwySef4IMPPsDp06cxcuRI9O3bF8eOHfvN+yWiqrGHjMg2ma9nFvTfha0V9S9atAjDhg3Dyy+/jLCwMCxZsgSBgYFYtmxZle3Xr1+PESNGoH///mjYsCEGDBiAYcOGYf78+RZt3n77bfTu3RsNGzbEq6++ih49emDhwoW/eb8AUFhYiNzcXIsvIntnMe0FEzIim3EnIeOUF1WypaJ+k8mEpKQkREdHWyyPjo5GYmJile8pLCyEwWCwWGY0GnH48GEUFRXds83+/ft/834BYO7cufDw8FC+AgMDq/dBiWxYPXcDtA4aGJwceJclkQ3x8yj7PfqYp+E+Le2U3oZqyK5fv46SkhL4+vpaLPf19cXVq1erfE+PHj3w6aefIikpCSKC77//HqtWrUJRURGuX7+utFm0aBHOnTuH0tJSJCQk4PPPP0dGRsZv3i8ATJ48GTk5OcrXpUuXavJxiWySu8EJH74QgWV/ioSTloW/RLbij08EYupzzfDnbo3VDsU6mXvIrHTI8jdV9Go0GovXIlJpmdnUqVNx9epVtGvXDiICX19fDB48GAsWLIBWW/bk9ffffx/Dhw9HaGgoNBoNGjVqhCFDhmD16tW/eb8AoNfrodez65aoop4t/NQOgYgeMHeDE4Z1DFE7DOulFPXbQA+Zj48PtFptpV6pzMzMSr1XZkajEatWrUJ+fj4uXryI9PR0NGjQAG5ubvDx8QEA1K1bF9u2bUNeXh5++ukn/Pjjj3B1dUVISMhv3i8RERGRwlzUX3gDEFE3lirUKCHT6XSIjIxEQkKCxfKEhAR06NDhnu91cnJC/fr1odVqsWnTJjz33HNwcLDcvcFgwGOPPYbi4mJs2bIFffr0+d37JSIiIlKGLKUUMN1UN5Yq1HjIcty4cRg0aBCioqLQvn17LF++HOnp6Rg5ciSAsrqtK1euKHONnT17FocPH0bbtm2RnZ2NRYsW4YcffsDatWuVbX733Xe4cuUKWrdujStXrmDGjBkoLS3FhAkTqr1fIiIiortyNAAOTkBpUVlhv95N7Ygs1Dgh69+/P7KysjBr1ixkZGSgRYsW+PLLLxEcHAwAyMjIsJgbrKSkBAsXLsSZM2fg5OSEbt26ITExEQ0aNFDaFBQUYMqUKbhw4QJcXV3Ru3dvrF+/Hp6entXeLxEREdFdaTRlw5b5WWWF/R5qB2RJI2KFA6kPSW5uLjw8PJCTkwN3d3e1wyEiIqJH6f3WQHYaMPRrIKjdQ99dTfIO3vNORERE9sGKZ+tnQkZERET2wYpn62dCRkRERPZBma3/V1XDqAoTMiIiIrIPVjxbPxMyIiIisg/KbP1MyIiIiIjUwaJ+IiIiIpWxqJ+IiIhIZUpRv/U9YJwJGREREdkHDlkSERERqYxF/UREREQq47QXRERERCpjUT8RERGRysoPWZaWqBtLBUzIiIiIyD6Yi/oBoPCGenFUgQkZERER2QdHPaDVl31vZcOWTMiIiIjIflhpYT8TMiIiIrIfBuuc+oIJGREREdkPK52t31HtAKxNSUkJioqK1A6D6IHRarVwdHSERqNROxQiIvVZ6Wz9TMjKuXnzJi5fvgwRUTsUogfK2dkZ/v7+0Ol0aodCRKQuK52tnwnZbSUlJbh8+TKcnZ1Rt25d9iaQTRARmEwmXLt2DWlpaWjSpAkcHFipQER2TCnq55ClVSoqKoKIoG7dujAajWqHQ/TAGI1GODk54aeffoLJZILBYFA7JCIi9VjpbP38U7kC9oyRLWKvGBHRbVZa1M+f0kRERGQ/rLSonwkZERER2Q8rLepnQkZERET2gzP1ExEREanMwBoyshOcWJeIiKwWhyxrFxFBvqlYla+aTky7c+dOdOzYEZ6envD29sZzzz2H8+fPK+svX76MAQMGwMvLCy4uLoiKisJ3332nrN++fTuioqJgMBjg4+ODfv36Kes0Gg22bdtmsT9PT0+sWbMGAHDx4kVoNBr87W9/Q9euXWEwGPB///d/yMrKwsCBA1G/fn04OzujZcuW2Lhxo8V2SktLMX/+fDRu3Bh6vR5BQUGYM2cOAKB79+4YNWqURfusrCzo9Xrs2rWrRseHiIhIYaVF/ZyH7C5uFZWg2bSvVdn36Vk94Kyr/n9NXl4exo0bh5YtWyIvLw/Tpk1D3759kZycjPz8fHTp0gWPPfYYtm/fDj8/Pxw9ehSlpaUAgB07dqBfv36Ij4/H+vXrYTKZsGPHjhrHPHHiRCxcuBCrV6+GXq9HQUEBIiMjMXHiRLi7u2PHjh0YNGgQGjZsiLZt2wIAJk+ejBUrVmDx4sXo2LEjMjIy8OOPPwIAXn75ZYwaNQoLFy6EXq8HAGzYsAEBAQHo1q1bjeMjIiICAOhv15AV5QElxYDWOlIh64iCfpfnn3/e4vXKlStRr149nD59GomJibh27RqOHDkCLy8vAEDjxo2VtnPmzMGAAQMwc+ZMZVl4eHiNYxgzZoxFzxoAvPnmm8r3r7/+Onbu3InPPvsMbdu2xY0bN/D+++9j6dKliIuLAwA0atQIHTt2VD7T66+/js8//xx//OMfAQCrV6/G4MGDOVccERH9duYeMqBs2NLZS71YymFCdhdGJy1Oz+qh2r5r4vz585g6dSoOHTqE69evK71f6enpSE5ORps2bZRkrKLk5GQMHz78d8ccFRVl8bqkpATz5s3D5s2bceXKFRQWFqKwsBAuLi4AgJSUFBQWFuKpp56qcnt6vR5/+tOfsGrVKvzxj39EcnIyjh8/Xmn4lIiIqEa0ToCTM1CUX1bYz4TMumk0mhoNG6opJiYGgYGBWLFiBQICAlBaWooWLVrAZDLd9zFQ91uv0Wgq1bRVVbRvTrTMFi5ciMWLF2PJkiVo2bIlXFxcMGbMGJhMpmrtFygbtmzdujUuX76MVatW4amnnkJwcPB930dERHRPeveyhMyKCvtZ1F/LZWVlISUlBVOmTMFTTz2FsLAwZGdnK+tbtWqF5ORk/PLLL1W+v1WrVvj222/vuv26desiIyNDeX3u3Dnk5+ffN659+/ahT58++NOf/oTw8HA0bNgQ586dU9Y3adIERqPxnvtu2bIloqKisGLFCvz1r3/F0KFD77tfIiKi+7LCwn4mZLVcnTp14O3tjeXLlyM1NRW7du3CuHHjlPUDBw6En58fYmNjceDAAVy4cAFbtmzBwYMHAQDTp0/Hxo0bMX36dKSkpODkyZNYsGCB8v7u3btj6dKlOHr0KL7//nuMHDkSTk5O942rcePGSEhIQGJiIlJSUjBixAhcvXpVWW8wGDBx4kRMmDAB69atw/nz53Ho0CGsXLnSYjsvv/wy5s2bh5KSEvTt2/f3Hi4iIiKrnPqCCVkt5+DggE2bNiEpKQktWrTA2LFj8e677yrrdTod/vWvf6FevXro3bs3WrZsiXnz5kGrLatT69q1Kz777DNs374drVu3Rvfu3S2mxFi4cCECAwPRuXNnvPDCC3jzzTfh7Ox837imTp2KiIgI9OjRA127dlWSwoptxo8fj2nTpiEsLAz9+/dHZmamRZuBAwfC0dERL7zwAgwGw+84UkRERLdZ4Wz9GqnppFe1WG5uLjw8PJCTkwN3d3eLdQUFBUhLS0NISAh/8VuRS5cuoUGDBjhy5AgiIiLUDqfW4vlNRFTOZ4OBU1uBnvOBdiMf2m7ulXdUVDuq1snuFBUVISMjA5MmTUK7du2YjBER0YPDIUui6jlw4ACCg4ORlJSEjz/+WO1wiIjIlljh8yzZQ0ZWqWvXrjV+hBQREVG1mGfrZw8ZERERkUqssKifCRkRERHZFyscsmRCRkRERPaFRf1EREREKuNM/UREREQqM7Con4iIiEhdehvpIfvoo4+UGb8jIyOxb9++e7b/8MMPERYWBqPRiKZNm2LdunWV2ixZsgRNmzaF0WhEYGAgxo4di4KCAmV9cXExpkyZgpCQEBiNRjRs2BCzZs1CaWnpb/kIdBcNGjTAkiVLqt3+4sWL0Gg0SE5OfmgxlbdmzRp4eno+kn0REZGNMg9ZFt8Cik3qxnJbjech27x5M8aMGYOPPvoITz75JD755BP06tULp0+fRlBQUKX2y5Ytw+TJk7FixQo88cQTOHz4MIYPH446deogJiYGALBhwwZMmjQJq1atQocOHXD27FkMHjwYALB48WIAwPz58/Hxxx9j7dq1aN68Ob7//nsMGTIEHh4eGD169O84BFTekSNH4OLi8kC3uWbNGowZMwa//vrrA90uERHRb6Iv9xijwlzA0Ue9WG6rcUK2aNEiDBs2DC+//DKAsp6tr7/+GsuWLcPcuXMrtV+/fj1GjBiB/v37AwAaNmyIQ4cOYf78+UpCdvDgQTz55JN44YUXAJT10gwcOBCHDx9WtnPw4EH06dMHzz77rNJm48aN+P7772v6Eege6tatq3YINs1kMkGn06kdBhGRfXPQAjpXwHSzbOoLF/UTshoNWZpMJiQlJSE6OtpieXR0NBITE6t8T2FhYaWHGRuNRhw+fBhFRUUAgI4dOyIpKUlJwC5cuIAvv/xSSb7Mbb799lucPXsWAHD8+HHs378fvXv3vmu8hYWFyM3NtfiqNhHAlKfOVw1mqP/iiy/g6empDN0mJydDo9HgrbfeUtqMGDECAwcOBAAkJiaic+fOytDwG2+8gby8PKVtxSHLH3/8ER07doTBYECzZs3wzTffQKPRYNu2bRZxXLhwAd26dYOzszPCw8Nx8OBBAMCePXswZMgQ5OTkQKPRQKPRYMaMGQDKzqcJEybgscceg4uLC9q2bYs9e/ZYbHfNmjUICgqCs7Mz+vbti6ysrGofm/Pnz6NPnz7w9fWFq6srnnjiCXzzzTcWbQoLCzFhwgQEBgZCr9ejSZMmWLlypbL+1KlTePbZZ+Hu7g43Nzd06tQJ58+fB1D2NIExY8ZYbC82Nlbp3TUfz9mzZ2Pw4MHw8PDA8OHDAQATJ07E448/DmdnZzRs2BBTp05Vrgez7du3IyoqCgaDAT4+PujXrx8AYNasWWjZsmWlzxsZGYlp06ZV+/gQEdk1Kyvsr1EP2fXr11FSUgJfX1+L5b6+vrh69WqV7+nRowc+/fRTxMbGIiIiAklJSVi1ahWKiopw/fp1+Pv7Y8CAAbh27Ro6duwIEUFxcTFeffVVTJo0SdnOxIkTkZOTg9DQUGi1WpSUlGDOnDlKolGVuXPnYubMmTX5iHcU5QPvBPy29/5eb/8H0FVv2LBz5864ceMGjh07hsjISOzduxc+Pj7Yu3ev0mbPnj0YO3YsTp48iR49euAvf/kLVq5ciWvXrmHUqFEYNWoUVq9eXWnbpaWliI2NRVBQEL777jvcuHED48ePrzKO+Ph4vPfee2jSpAni4+MxcOBApKamokOHDliyZAmmTZuGM2fOAABcXV0BAEOGDMHFixexadMmBAQEYOvWrejZsydOnjyJJk2a4LvvvsPQoUPxzjvvoF+/fti5cyemT59e7cN48+ZN9O7dG7Nnz4bBYMDatWsRExODM2fOKMPrL730Eg4ePIj//d//RXh4ONLS0nD9+nUAwJUrV9C5c2d07doVu3btgru7Ow4cOIDi4uJqxwAA7777LqZOnYopU6Yoy9zc3LBmzRoEBATg5MmTGD58ONzc3DBhwgQAwI4dO9CvXz/Ex8dj/fr1MJlM2LFjBwBg6NChmDlzJo4cOYInnngCAHDixAkcO3YMn332WY1iIyKyW3p3AFesp7BfauDKlSsCQBITEy2Wz549W5o2bVrle/Lz82XIkCHi6OgoWq1WAgICZMKECQJAfv75ZxER2b17t/j6+sqKFSvkxIkT8o9//EMCAwNl1qxZynY2btwo9evXl40bN8qJEydk3bp14uXlJWvWrLlrvAUFBZKTk6N8Xbp0SQBITk5Opba3bt2S06dPy61bt8oWFN4Ume6uzlfhzZr8t0hERIS89957IiISGxsrc+bMEZ1OJ7m5uZKRkSEAJCUlRQYNGiSvvPKKxXv37dsnDg4OyucODg6WxYsXi4jIV199JY6OjpKRkaG0T0hIEACydetWERFJS0sTAPLpp58qbU6dOqXsU0Rk9erV4uHhYbHf1NRU0Wg0cuXKFYvlTz31lEyePFlERAYOHCg9e/a0WN+/f/9K26qJZs2ayQcffCAiImfOnBEAkpCQUGXbyZMnS0hIiJhMpirXd+nSRUaPHm2xrE+fPhIXF6e8Dg4OltjY2PvGtWDBAomMjFRet2/fXl588cW7tu/Vq5e8+uqryusxY8ZI165d79q+0vlNRGTvPn2m7Hfuqc8f2i5ycnLumndUVKMeMh8fH2i12kq9YZmZmZV6zcyMRiNWrVqFTz75BD///DP8/f2xfPlyuLm5wcenbMx26tSpGDRokFKX1rJlS+Tl5eGVV15BfHw8HBwc8NZbb2HSpEkYMGCA0uann37C3LlzERcXV+W+9Xo99Hp9TT7iHU7OZT1VanByrlHzrl27Ys+ePRg3bhz27duH2bNnY8uWLdi/fz9+/fVX+Pr6IjQ0FElJSUhNTcWGDRuU94oISktLkZaWhrCwMIvtnjlzBoGBgfDz81OW/eEPf6gyhlatWinf+/v7Ayg7L0JDQ6tsf/ToUYgIHn/8cYvlhYWF8Pb2BgCkpKSgb9++Fuvbt2+PnTt33u+QAADy8vIwc+ZM/POf/8R//vMfFBcX49atW0hPTwdQNryr1WrRpUuXKt+fnJyMTp06wcnJqVr7u5uoqKhKy/7+979jyZIlSE1Nxc2bN1FcXAx39ztFpsnJycrwZlWGDx+OoUOHYtGiRdBqtdiwYQMWLlz4u+IkIrIrVjZbf40SMp1Oh8jISCQkJFj8okxISECfPn3u+V4nJyfUr18fALBp0yY899xzcHAoK2HLz89XvjfTarUQEcjteqq7tXlo015oNNUeNlRb165dsXLlShw/fhwODg5o1qwZunTpgr179yI7O1tJOEpLSzFixAi88cYblbZR1R2yIgKNRlOtGMonLeb33Ov/prS0FFqtFklJSdBqtRbrzEOaUoNauqq89dZb+Prrr/Hee++hcePGMBqN+O///m+YTGW3OBuNxnu+/37rHRwcKsVYsQ4MQKW7Vg8dOoQBAwZg5syZ6NGjBzw8PLBp0yaLhOp++46JiYFer8fWrVuh1+tRWFiI559//p7vISKicqxstv4a32U5btw4DBo0CFFRUWjfvj2WL1+O9PR0jBw5EgAwefJkXLlyRZlr7OzZszh8+DDatm2L7OxsLFq0CD/88APWrl2rbDMmJgaLFi1CmzZt0LZtW6SmpmLq1Kn4r//6L+WXdUxMDObMmYOgoCA0b94cx44dw6JFizB06NAHcRxqNXMd2ZIlS9ClSxdoNBp06dIFc+fORXZ2tjItSEREBE6dOoXGjRtXa7uhoaFIT0/Hzz//rPSAHjlypMbx6XQ6lJSUWCxr06YNSkpKkJmZiU6dOlX5vmbNmuHQoUMWyyq+vpd9+/Zh8ODByh8PN2/exMWLF5X1LVu2RGlpKfbu3Yunn3660vtbtWqFtWvXoqioqMpesrp16yIjI0N5XVJSgh9++AHdunW7Z1wHDhxAcHAw4uPjlWU//fRTpX1/++23GDJkSJXbcHR0RFxcHFavXg29Xo8BAwbA2blmPatERHbNyor6a1RDZvbhhx9KcHCw6HQ6iYiIkL179yrr4uLipEuXLsrr06dPS+vWrcVoNIq7u7v06dNHfvzxR4vtFRUVyYwZM6RRo0ZiMBgkMDBQ/vznP0t2drbSJjc3V0aPHi1BQUFiMBikYcOGEh8fL4WFhdWO+15jubW9xiYiIkK0Wq0sXbpURER++eUXcXJyEgBy6tQpERE5fvy4GI1G+fOf/yzHjh2Ts2fPyueffy6jRo1StlO+hqy4uFiaNm0qPXr0kOPHj8v+/fulbdu2AkC2bdsmIndqyI4dO6ZsIzs7WwDI7t27RUTkwIEDAkC++eYbuXbtmuTl5YmIyIsvvigNGjSQLVu2yIULF+Tw4cMyb9482bFjh4iIHDx4UDQajcyfP1/OnDkjH3zwgXh6ela7hiw2NlZat24tx44dk+TkZImJiRE3NzeLuq/BgwdLYGCgbN26VS5cuCC7d++WzZs3i4jI9evXxdvbW/r16ydHjhyRs2fPyrp165Tz9+OPPxZnZ2f55z//KSkpKfLKK6+Iu7t7pRoy8/E027Ztmzg6OsrGjRslNTVV3n//ffHy8rL4XLt37xYHBweZNm2anD59Wk6cOCHz58+32M7Zs2dFq9WKVquVQ4cO3fNY1Pbzm4jogbt6SuT8bpHs9Ie2i5rUkP2mhKy2suWEbPz48QJAfvjhB2VZeHi41K1bV0pLS5Vlhw8flmeeeUZcXV3FxcVFWrVqJXPmzFHWV0wgUlJS5MknnxSdTiehoaHyxRdfCADZuXOniFQvIRMRGTlypHh7ewsAmT59uoiImEwmmTZtmjRo0ECcnJzEz89P+vbtKydOnFDet3LlSqlfv74YjUaJiYmR9957r9oJWVpamnTr1k2MRqMEBgbK0qVLKxXi37p1S8aOHSv+/v6i0+mkcePGsmrVKmX98ePHJTo6WpydncXNzU06deok58+fV+J/9dVXxcvLS+rVqydz586tsqi/YkImIvLWW2+Jt7e3uLq6Sv/+/WXx4sWVPteWLVukdevWotPpxMfHR/r161dpO506dZJmzZrd91jU9vObiKg2qklCphH5nYU6tUhubi48PDyQk5NjUUANAAUFBUhLS1MeCUVVO3DgADp27IjU1FQ0atRI7XDsmoggNDQUI0aMwLhx4+7Zluc3EdGjd6+8o6Ia15CRfdm6dStcXV3RpEkTpKamYvTo0XjyySeZjKksMzMT69evx5UrV+5aZ0ZERLUHEzK6pxs3bmDChAm4dOkSfHx88PTTT1vN9ArNmzevVAxv9sknn+DFF198xBE9Or6+vvDx8cHy5ctRp04dtcMhIqLfiQkZ3dNLL72El156Se0wqvTll19WOc0EgLvOi2cr7KjSgIjILjAho1orODhY7RCIiIgeiBo9XNwesOeBbBHPayIi68aE7DbzBLTmWdyJbEl+fj4A/O7HQBER0cPBIcvbHB0d4ezsjGvXrsHJyanSY5qIaiMRQX5+PjIzM+Hp6VnpMVVERGQdmJDdptFo4O/vj7S0tLveuUdUW3l6elo8JJ6IiKwLE7JydDodmjRpwmFLsilOTk7sGSMisnJMyCpwcHDgTOZERET0SLFQioiIiEhlTMiIiIiIVMaEjIiIiEhldlVDZp4cMzc3V+VIiIiIyNaZ843qTM5tVwnZjRs3AACBgYEqR0JERET24saNG/Dw8LhnG43Y0TNVSktL8Z///Adubm7QaDQPfPu5ubkIDAzEpUuX4O7u/sC3T3fHY68eHnv18Nirh8deHbXtuIsIbty4gYCAgPtOOG9XPWQODg6oX7/+Q9+Pu7t7rThRbBGPvXp47NXDY68eHnt11Kbjfr+eMTMW9RMRERGpjAkZERERkcqYkD1Aer0e06dPh16vVzsUu8Njrx4ee/Xw2KuHx14dtnzc7aqon4iIiMgasYeMiIiISGVMyIiIiIhUxoSMiIiISGVMyIiIiIhUxoTsAfroo48QEhICg8GAyMhI7Nu3T+2QbN6MGTOg0Wgsvvz8/NQOyyb9+9//RkxMDAICAqDRaLBt2zaL9SKCGTNmICAgAEajEV27dsWpU6fUCdaG3O+4Dx48uNI10K5dO3WCtTFz587FE088ATc3N9SrVw+xsbE4c+aMRRue9w9HdY69rZ37TMgekM2bN2PMmDGIj4/HsWPH0KlTJ/Tq1Qvp6elqh2bzmjdvjoyMDOXr5MmTaodkk/Ly8hAeHo6lS5dWuX7BggVYtGgRli5diiNHjsDPzw/PPPOM8gxZ+m3ud9wBoGfPnhbXwJdffvkII7Rde/fuxWuvvYZDhw4hISEBxcXFiI6ORl5entKG5/3DUZ1jD9jYuS/0QPzhD3+QkSNHWiwLDQ2VSZMmqRSRfZg+fbqEh4erHYbdASBbt25VXpeWloqfn5/MmzdPWVZQUCAeHh7y8ccfqxChbap43EVE4uLipE+fPqrEY28yMzMFgOzdu1dEeN4/ShWPvYjtnfvsIXsATCYTkpKSEB0dbbE8OjoaiYmJKkVlP86dO4eAgACEhIRgwIABuHDhgtoh2Z20tDRcvXrV4hrQ6/Xo0qULr4FHYM+ePahXrx4ef/xxDB8+HJmZmWqHZJNycnIAAF5eXgB43j9KFY+9mS2d+0zIHoDr16+jpKQEvr6+Fst9fX1x9epVlaKyD23btsW6devw9ddfY8WKFbh69So6dOiArKwstUOzK+bznNfAo9erVy9s2LABu3btwsKFC3HkyBF0794dhYWFaodmU0QE48aNQ8eOHdGiRQsAPO8flaqOPWB7576j2gHYEo1GY/FaRCotowerV69eyvctW7ZE+/bt0ahRI6xduxbjxo1TMTL7xGvg0evfv7/yfYsWLRAVFYXg4GDs2LED/fr1UzEy2zJq1CicOHEC+/fvr7SO5/3Ddbdjb2vnPnvIHgAfHx9otdpKfxFlZmZW+suJHi4XFxe0bNkS586dUzsUu2K+s5XXgPr8/f0RHBzMa+ABev3117F9+3bs3r0b9evXV5bzvH/47nbsq1Lbz30mZA+ATqdDZGQkEhISLJYnJCSgQ4cOKkVlnwoLC5GSkgJ/f3+1Q7ErISEh8PPzs7gGTCYT9u7dy2vgEcvKysKlS5d4DTwAIoJRo0bhH//4B3bt2oWQkBCL9TzvH577Hfuq1PZzn0OWD8i4ceMwaNAgREVFoX379li+fDnS09MxcuRItUOzaW+++SZiYmIQFBSEzMxMzJ49G7m5uYiLi1M7NJtz8+ZNpKamKq/T0tKQnJwMLy8vBAUFYcyYMXjnnXfQpEkTNGnSBO+88w6cnZ3xwgsvqBh17Xev4+7l5YUZM2bg+eefh7+/Py5evIi3334bPj4+6Nu3r4pR24bXXnsNf/3rX/H555/Dzc1N6Qnz8PCA0WiERqPhef+Q3O/Y37x50/bOfRXv8LQ5H374oQQHB4tOp5OIiAiL23Pp4ejfv7/4+/uLk5OTBAQESL9+/eTUqVNqh2WTdu/eLQAqfcXFxYlI2RQA06dPFz8/P9Hr9dK5c2c5efKkukHbgHsd9/z8fImOjpa6deuKk5OTBAUFSVxcnKSnp6sdtk2o6rgDkNWrVytteN4/HPc79rZ47mtERB5lAkhEREREllhDRkRERKQyJmREREREKmNCRkRERKQyJmREREREKmNCRkRERKQyJmREREREKmNCRkRERKQyJmREREREKmNCRkRUDRcvXoRGo0FycrLaoRCRDWJCRkRUDYGBgcjIyECLFi3UDgUzZsxA69at1Q6DiB4gPlyciOg+TCYTdDod/Pz81A6FiGwUe8iIyKqICBYsWICGDRvCaDQiPDwcf//73yEiePrpp9GzZ0+YH8H766+/IigoCPHx8QCAPXv2QKPRYMeOHQgPD4fBYEDbtm1x8uRJi30kJiaic+fOMBqNCAwMxBtvvIG8vDxlfYMGDTB79mwMHjwYHh4eGD58eKUhS/O+vv76a7Rp0wZGoxHdu3dHZmYmvvrqK4SFhcHd3R0DBw5Efn7+fT+fmXm73377LaKiouDs7IwOHTrgzJkzAIA1a9Zg5syZOH78ODQaDTQaDdasWfMw/iuI6FFS88nmREQVvf322xIaGio7d+6U8+fPy+rVq0Wv18uePXvk8uXLUqdOHVmyZImIiPTv31+ioqLEZDKJiMju3bsFgISFhcm//vUvOXHihDz33HPSoEEDpc2JEyfE1dVVFi9eLGfPnpUDBw5ImzZtZPDgwUoMwcHB4u7uLu+++66cO3dOzp07J2lpaQJAjh07ZrGvdu3ayf79++Xo0aPSuHFj6dKli0RHR8vRo0fl3//+t3h7e8u8efOq9fnKb7dt27ayZ88eOXXqlHTq1Ek6dOggIiL5+fkyfvx4ad68uWRkZEhGRobk5+c/9P8XInq4mJARkdW4efOmGAwGSUxMtFg+bNgwGThwoIiI/O1vfxO9Xi+TJ08WZ2dnOXPmjNLOnMxs2rRJWZaVlSVGo1E2b94sIiKDBg2SV155xWL7+/btEwcHB7l165aIlCVksbGxFm3ulpB98803Spu5c+cKADl//ryybMSIEdKjR49qf76qtrtjxw4BoMQ3ffp0CQ8Pv9ehJKJahjVkRGQ1Tp8+jYKCAjzzzDMWy00mE9q0aQMA+J//+R9s3boVc+fOxbJly/D4449X2k779u2V7728vNC0aVOkpKQAAJKSkpCamooNGzYobUQEpaWlSEtLQ1hYGAAgKiqqWjG3atVK+d7X1xfOzs5o2LChxbLDhw9X+/NVtV1/f38AQGZmJoKCgqoVFxHVLkzIiMhqlJaWAgB27NiBxx57zGKdXq8HAOTn5yMpKQlarRbnzp2r9rY1Go2yjxEjRuCNN96o1KZ8suPi4lKt7To5OVnso/xr8zLz56rO57vbdsu/n4hsDxMyIrIazZo1g16vR3p6Orp06VJlm/Hjx8PBwQFfffUVevfujWeffRbdu3e3aHPo0CElucrOzsbZs2cRGhoKAIiIiMCpU6fQuHHjh/thqlCdz1cdOp0OJSUlDzAyIlIbEzIishpubm548803MXbsWJSWlqJjx47Izc1FYmIiXF1d4ePjg1WrVuHgwYOIiIjApEmTEBcXhxMnTqBOnTrKdmbNmgVvb2/4+voiPj4ePj4+iI2NBQBMnDgR7dq1w2uvvYbhw4fDxcUFKSkpSEhIwAcffKDq54uLi6vWdho0aIC0tDQkJyejfv36cHNzq9TDRkS1C6e9ICKr8pe//AXTpk3D3LlzERYWhh49euCLL75AgwYNMGzYMMyYMQMREREAgOnTpyMgIAAjR4602Ma8efMwevRoREZGIiMjA9u3b4dOpwNQVpu1d+9enDt3Dp06dUKbNm0wdepUpU5Lrc8XEhJS7W08//zz6NmzJ7p164a6deti48aNDzFiInoUNCK3J/QhIqrl9uzZg27duiE7Oxuenp5qh0NEVG3sISMiIiJSGRMyIiIiIpVxyJKIiIhIZewhIyIiIlIZEzIiIiIilTEhIyIiIlIZEzIiIiIilTEhIyIiIlIZEzIiIiIilTEhIyIiIlIZEzIiIiIilf0/Yu6y2uUBIdMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Agent = TreeAgent\n",
    "exp_list = Two_Obs_Exps\n",
    "num_rounds = 40000\n",
    "replay_size = 500\n",
    "num_optimizer_steps = 10\n",
    "learning_rate = 0.01\n",
    "num_cores = 6\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run the parallelized training of the final layers and save results\n",
    "trainAndPlotTasks(num_cores = num_cores, Agent = TreeAgent, exp_list = exp_list, num_rounds = num_rounds, replay_size = replay_size, num_optimizer_steps = num_optimizer_steps, learning_rate = learning_rate )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8731464-b6ef-4c38-a025-48c111983522",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
